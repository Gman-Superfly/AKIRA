{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 032 v4.2: Full Ablation with Fixed Wormhole\n",
    "\n",
    "**Purpose**: Ablation study of SBM variants with proper wormhole attention.\n",
    "\n",
    "**v4.2 Fixes over v4.1**:\n",
    "1. **Pooled Wormhole**: Wormhole operates on global pooled features [B, D], not per-pixel [B, H, W, D]\n",
    "   - This reduces memory from O(H*W * T*H*W) to O(T) - massive savings\n",
    "   - Conceptually correct: wormhole finds globally similar states, not per-pixel matches\n",
    "2. **Lower threshold**: 0.9 instead of 0.9995\n",
    "3. **Sequence-based training**: Proper temporal history within trajectories\n",
    "\n",
    "**Models to test**:\n",
    "- SBM_2B_None, SBM_3B_None, SBM_7B_None (band count ablation)\n",
    "- SBM_3B_Temporal, SBM_3B_Neighbor, SBM_3B_Wormhole (attention type ablation)\n",
    "- SBM_3B_All (all attention types combined)\n",
    "- Flat_None, Flat_Temporal (baselines)\n",
    "\n",
    "**Run on Colab with GPU**: Runtime -> Change runtime type -> A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GRID_SIZE = 64\n",
    "EPOCHS = 50\n",
    "PREDICT_DELTA = True\n",
    "PREDICTION_HORIZON = 5\n",
    "NUM_TRAJECTORIES = 32  # Batch size = number of parallel trajectories\n",
    "TRAJECTORY_LENGTH = 100  # Steps per trajectory\n",
    "LR = 0.001\n",
    "\n",
    "# Attention config\n",
    "HISTORY_LEN = 8\n",
    "TOP_K_TEMPORAL = 4\n",
    "TEMPORAL_DECAY = 0.9\n",
    "NEIGHBOR_RANGE = 1  # 3x3 neighborhood\n",
    "\n",
    "# Wormhole - FIXED\n",
    "WORMHOLE_THRESHOLD = 0.9  # Was 0.9995\n",
    "WORMHOLE_MAX_CONN = 4\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Grid: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch: {NUM_TRAJECTORIES} trajectories x {TRAJECTORY_LENGTH} steps\")\n",
    "print(f\"Prediction: delta t+{PREDICTION_HORIZON}\")\n",
    "print(f\"\\nWormhole: threshold={WORMHOLE_THRESHOLD}, max_conn={WORMHOLE_MAX_CONN}\")\n",
    "print(f\"KEY FIX: Pooled wormhole (global features, not per-pixel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import math\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class PlasmaConfig:\n",
    "    height: int = 64\n",
    "    width: int = 64\n",
    "    diffusion: float = 0.25\n",
    "    advection: float = 0.08\n",
    "    noise_std: float = 0.02\n",
    "    disturbance_prob: float = 0.1\n",
    "    disturbance_strength: float = 0.15\n",
    "    num_vortices: int = 3\n",
    "    vortex_strength: float = 0.1\n",
    "    shear_strength: float = 0.05\n",
    "    multiscale_noise: bool = True\n",
    "    num_actuators: int = 9\n",
    "    _base_actuator_sigma: float = 5.0\n",
    "    device: str = \"cpu\"\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    \n",
    "    @property\n",
    "    def actuator_sigma(self) -> float:\n",
    "        return self._base_actuator_sigma * min(self.height, self.width) / 64.0\n",
    "    \n",
    "    @classmethod\n",
    "    def turbulent(cls, device: str = \"cpu\", size: int = 64):\n",
    "        return cls(height=size, width=size, diffusion=0.3, advection=0.12, noise_std=0.03,\n",
    "                   disturbance_prob=0.25, disturbance_strength=0.3,\n",
    "                   num_vortices=3, vortex_strength=0.15, shear_strength=0.08,\n",
    "                   multiscale_noise=True, num_actuators=9, device=device)\n",
    "\n",
    "\n",
    "class TurbulentPlasmaEnv:\n",
    "    def __init__(self, cfg: PlasmaConfig):\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.dtype = cfg.dtype\n",
    "        self._actuator_maps = self._build_actuator_maps()\n",
    "        self._vortex_flow = self._build_vortex_flow()\n",
    "        self._shear_flow = self._build_shear_flow()\n",
    "    \n",
    "    def _build_actuator_maps(self) -> torch.Tensor:\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        grid_n = int(math.ceil(math.sqrt(self.cfg.num_actuators)))\n",
    "        centers_y = torch.linspace(h * 0.2, h * 0.8, grid_n, device=self.device, dtype=self.dtype)\n",
    "        centers_x = torch.linspace(w * 0.2, w * 0.8, grid_n, device=self.device, dtype=self.dtype)\n",
    "        centers = torch.cartesian_prod(centers_y, centers_x)[:self.cfg.num_actuators]\n",
    "        sig2 = self.cfg.actuator_sigma ** 2\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "            torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        bumps = [torch.exp(-((yy - cy) ** 2 + (xx - cx) ** 2) / (2 * sig2)) for cy, cx in centers]\n",
    "        return torch.stack(bumps, dim=0)\n",
    "    \n",
    "    def _build_vortex_flow(self):\n",
    "        if self.cfg.num_vortices == 0: return None, None\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        torch.manual_seed(42)\n",
    "        centers_y = torch.rand(self.cfg.num_vortices, device=self.device) * (h * 0.6) + (h * 0.2)\n",
    "        centers_x = torch.rand(self.cfg.num_vortices, device=self.device) * (w * 0.6) + (w * 0.2)\n",
    "        yy, xx = torch.meshgrid(torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "                                torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        vy, vx = torch.zeros_like(yy), torch.zeros_like(xx)\n",
    "        scale = min(h, w) / 64.0\n",
    "        for i in range(self.cfg.num_vortices):\n",
    "            dy, dx = yy - centers_y[i], xx - centers_x[i]\n",
    "            r2 = dy**2 + dx**2 + 1e-6\n",
    "            decay = torch.exp(-r2 / (2 * (10 * scale)**2))\n",
    "            sign = 1 if i % 2 == 0 else -1\n",
    "            vy += sign * self.cfg.vortex_strength * (-dx) / torch.sqrt(r2) * decay\n",
    "            vx += sign * self.cfg.vortex_strength * dy / torch.sqrt(r2) * decay\n",
    "        return vy, vx\n",
    "    \n",
    "    def _build_shear_flow(self):\n",
    "        if self.cfg.shear_strength == 0: return None, None\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        yy, _ = torch.meshgrid(torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "                               torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        return torch.zeros_like(yy), self.cfg.shear_strength * torch.sin(2 * math.pi * yy / h)\n",
    "    \n",
    "    def _apply_flow(self, field, vy, vx):\n",
    "        B, C, H, W = field.shape\n",
    "        yy, xx = torch.meshgrid(torch.linspace(-1, 1, H, device=self.device),\n",
    "                                torch.linspace(-1, 1, W, device=self.device), indexing=\"ij\")\n",
    "        grid = torch.stack([xx - vx/(W/2), yy - vy/(H/2)], dim=-1).unsqueeze(0).expand(B, -1, -1, -1)\n",
    "        return F.grid_sample(field, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    \n",
    "    def _multiscale_noise(self, shape):\n",
    "        B, C, H, W = shape\n",
    "        noise = torch.zeros(shape, device=self.device, dtype=self.dtype)\n",
    "        for scale in [1, 2, 4, 8]:\n",
    "            hs, ws = H // scale, W // scale\n",
    "            if hs < 4: continue\n",
    "            coarse = torch.randn(B, C, hs, ws, device=self.device, dtype=self.dtype)\n",
    "            noise += F.interpolate(coarse, size=(H, W), mode='bilinear', align_corners=False) * (self.cfg.noise_std / scale)\n",
    "        return noise\n",
    "    \n",
    "    def reset(self, batch_size=1):\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        cx = torch.randint(int(w*0.3), int(w*0.7), (batch_size,), device=self.device)\n",
    "        cy = torch.randint(int(h*0.3), int(h*0.7), (batch_size,), device=self.device)\n",
    "        yy, xx = torch.meshgrid(torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "                                torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        sig2 = (self.cfg.actuator_sigma * 1.5) ** 2\n",
    "        field = torch.zeros(batch_size, 1, h, w, device=self.device, dtype=self.dtype)\n",
    "        for b in range(batch_size):\n",
    "            field[b, 0] = torch.exp(-((yy - cy[b].float())**2 + (xx - cx[b].float())**2) / (2*sig2))\n",
    "        return field\n",
    "    \n",
    "    def step(self, field, control, noise=True):\n",
    "        B = field.shape[0]\n",
    "        lap = (F.pad(field, (0,0,1,0))[:,:,:-1,:] + F.pad(field, (0,0,0,1))[:,:,1:,:] +\n",
    "               F.pad(field, (1,0,0,0))[:,:,:,:-1] + F.pad(field, (0,1,0,0))[:,:,:,1:]) - 4*field\n",
    "        diffused = field + self.cfg.diffusion * lap\n",
    "        advected = torch.roll(diffused, shifts=(1,-1), dims=(2,3)) * self.cfg.advection + diffused * (1 - self.cfg.advection)\n",
    "        if self._vortex_flow[0] is not None:\n",
    "            advected = self._apply_flow(advected, self._vortex_flow[0], self._vortex_flow[1])\n",
    "        if self._shear_flow[0] is not None:\n",
    "            advected = self._apply_flow(advected, self._shear_flow[0], self._shear_flow[1])\n",
    "        force = torch.einsum('ba,ahw->bhw', control, self._actuator_maps).unsqueeze(1)\n",
    "        next_field = advected + force\n",
    "        if noise:\n",
    "            next_field = next_field + (self._multiscale_noise(next_field.shape) if self.cfg.multiscale_noise \n",
    "                                       else torch.randn_like(next_field) * self.cfg.noise_std)\n",
    "        return torch.clamp(next_field, -1, 1)\n",
    "\n",
    "\n",
    "def generate_trajectory_batch(env, batch_size: int, traj_len: int, control_scale: float = 0.1, horizon: int = 1):\n",
    "    \"\"\"Generate batch of trajectories for sequence-based training.\n",
    "    \n",
    "    Returns:\n",
    "        fields: [T, B, 1, H, W] - time-first indexing\n",
    "        targets: [T, B, 1, H, W]\n",
    "    \"\"\"\n",
    "    all_fields = []\n",
    "    all_targets = []\n",
    "    \n",
    "    field = env.reset(batch_size)\n",
    "    trajectory = [field.clone()]\n",
    "    \n",
    "    for _ in range(traj_len + horizon):\n",
    "        ctrl = torch.clamp(torch.randn(batch_size, env.cfg.num_actuators, device=env.device) * control_scale, -1, 1)\n",
    "        field = env.step(field, ctrl)\n",
    "        trajectory.append(field.clone())\n",
    "    \n",
    "    for t in range(traj_len):\n",
    "        all_fields.append(trajectory[t])\n",
    "        all_targets.append(trajectory[t + horizon])\n",
    "    \n",
    "    return torch.stack(all_fields, dim=0), torch.stack(all_targets, dim=0)\n",
    "\n",
    "print(\"Environment defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL ATTENTION: Top-K causal attention over time\n",
    "# ============================================================================\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Per-band temporal attention with Top-K selection.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, num_heads: int = 4, max_len: int = 16,\n",
    "                 top_k: int = 4, decay: float = 0.9, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        self.top_k = top_k\n",
    "        self.decay = decay\n",
    "        \n",
    "        self.qkv = nn.Linear(feature_dim, 3 * feature_dim)\n",
    "        self.out = nn.Linear(feature_dim, feature_dim)\n",
    "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(max_len, max_len), 1).bool())\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Args: x: [B, T, D]. Returns: [B, T, D]\"\"\"\n",
    "        B, T, D = x.shape\n",
    "        qkv = self.qkv(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        scores = scores.masked_fill(self.causal_mask[:T, :T].unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        if T > self.top_k:\n",
    "            _, topk_idx = torch.topk(scores, self.top_k, dim=-1)\n",
    "            mask = torch.ones_like(scores, dtype=torch.bool)\n",
    "            mask.scatter_(-1, topk_idx, False)\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        if self.decay < 1.0:\n",
    "            time_offsets = torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "            time_diff = time_offsets.unsqueeze(0) - time_offsets.unsqueeze(1)\n",
    "            decay_weights = torch.pow(self.decay, time_diff.clamp(min=0).float())\n",
    "            decay_weights = torch.tril(decay_weights)\n",
    "            scores = scores + torch.log(decay_weights + 1e-10).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = (attn @ V).transpose(1, 2).reshape(B, T, D)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# NEIGHBOR ATTENTION: Local 3x3 spatial attention\n",
    "# ============================================================================\n",
    "\n",
    "class NeighborAttention(nn.Module):\n",
    "    \"\"\"Local spatial attention within 3x3 neighborhood.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, num_heads: int = 4, layer_range: int = 1, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        self.layer_range = layer_range\n",
    "        self.window_size = 2 * layer_range + 1\n",
    "        \n",
    "        self.qkv = nn.Linear(feature_dim, 3 * feature_dim)\n",
    "        self.out = nn.Linear(feature_dim, feature_dim)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Args: x: [B, H, W, D]. Returns: [B, H, W, D]\"\"\"\n",
    "        B, H, W, D = x.shape\n",
    "        \n",
    "        # Pad for neighborhood extraction\n",
    "        x_pad = F.pad(x.permute(0, 3, 1, 2), (self.layer_range,) * 4, mode='replicate')\n",
    "        x_pad = x_pad.permute(0, 2, 3, 1)  # [B, H+2r, W+2r, D]\n",
    "        \n",
    "        # Extract neighborhoods\n",
    "        neighbors = x_pad.unfold(1, self.window_size, 1).unfold(2, self.window_size, 1)\n",
    "        neighbors = neighbors.reshape(B, H, W, D, -1).permute(0, 1, 2, 4, 3)  # [B, H, W, K, D]\n",
    "        K_size = neighbors.shape[3]\n",
    "        \n",
    "        # QKV\n",
    "        Q = self.qkv(x)[:, :, :, :D].reshape(B, H, W, self.num_heads, self.head_dim)\n",
    "        Q = Q.unsqueeze(4)  # [B, H, W, heads, 1, head_dim]\n",
    "        \n",
    "        kv = self.qkv(neighbors)  # [B, H, W, K, 3D]\n",
    "        K = kv[:, :, :, :, D:2*D].reshape(B, H, W, K_size, self.num_heads, self.head_dim)\n",
    "        V = kv[:, :, :, :, 2*D:].reshape(B, H, W, K_size, self.num_heads, self.head_dim)\n",
    "        K = K.permute(0, 1, 2, 4, 3, 5)  # [B, H, W, heads, K, head_dim]\n",
    "        V = V.permute(0, 1, 2, 4, 3, 5)\n",
    "        \n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, H, W, heads, 1, K]\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = (attn @ V).squeeze(4)  # [B, H, W, heads, head_dim]\n",
    "        out = out.reshape(B, H, W, D)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# WORMHOLE ATTENTION: Global pooled similarity (FIXED - no per-pixel)\n",
    "# ============================================================================\n",
    "\n",
    "class WormholeAttention(nn.Module):\n",
    "    \"\"\"Sparse non-local attention via cosine similarity on POOLED features.\n",
    "    \n",
    "    v4.2 FIX: Operates on global pooled features [B, D], not per-pixel.\n",
    "    This is conceptually correct (wormhole finds globally similar states)\n",
    "    and memory efficient (O(T) instead of O(H*W * T*H*W)).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, attn_dim: int, threshold: float = 0.9,\n",
    "                 max_connections: int = 4, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.threshold = threshold\n",
    "        self.max_connections = max_connections\n",
    "        \n",
    "        self.W_q = nn.Linear(feature_dim, attn_dim, bias=False)\n",
    "        self.W_k = nn.Linear(feature_dim, attn_dim, bias=False)\n",
    "        self.W_v = nn.Linear(feature_dim, attn_dim, bias=False)\n",
    "        self.W_o = nn.Linear(attn_dim, feature_dim, bias=False)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, history: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Args:\n",
    "            query: Current pooled features [B, D]\n",
    "            history: Past pooled features [B, T, D]\n",
    "        Returns:\n",
    "            output: [B, D]\n",
    "            stats: dict with connection info\n",
    "        \"\"\"\n",
    "        B, D = query.shape\n",
    "        T = history.shape[1] if history is not None else 0\n",
    "        \n",
    "        if T == 0:\n",
    "            return torch.zeros_like(query), {'num_connections': 0, 'max_sim': 0.0}\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        Q_norm = F.normalize(query, p=2, dim=-1)  # [B, D]\n",
    "        K_norm = F.normalize(history, p=2, dim=-1)  # [B, T, D]\n",
    "        \n",
    "        # Cosine similarity: [B, T]\n",
    "        sim = torch.bmm(Q_norm.unsqueeze(1), K_norm.transpose(1, 2)).squeeze(1)\n",
    "        \n",
    "        # Top-K selection\n",
    "        K_conn = min(self.max_connections, T)\n",
    "        topk_sim, topk_idx = torch.topk(sim, K_conn, dim=-1)  # [B, K]\n",
    "        \n",
    "        # Threshold mask\n",
    "        mask = topk_sim > self.threshold  # [B, K]\n",
    "        num_valid = mask.sum().item()\n",
    "        \n",
    "        if num_valid == 0:\n",
    "            return torch.zeros_like(query), {\n",
    "                'num_connections': 0, \n",
    "                'max_sim': sim.max().item()\n",
    "            }\n",
    "        \n",
    "        # Gather selected history\n",
    "        selected_hist = torch.gather(history, 1, topk_idx.unsqueeze(-1).expand(-1, -1, D))  # [B, K, D]\n",
    "        \n",
    "        # Attention\n",
    "        Q = self.W_q(query).unsqueeze(1)  # [B, 1, attn_dim]\n",
    "        K = self.W_k(selected_hist)  # [B, K, attn_dim]\n",
    "        V = self.W_v(selected_hist)  # [B, K, attn_dim]\n",
    "        \n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)).squeeze(1) / math.sqrt(self.attn_dim)  # [B, K]\n",
    "        scores = scores.masked_fill(~mask, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = torch.where(mask, attn, torch.zeros_like(attn))\n",
    "        \n",
    "        out = torch.bmm(attn.unsqueeze(1), V).squeeze(1)  # [B, attn_dim]\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, {\n",
    "            'num_connections': num_valid,\n",
    "            'max_sim': topk_sim.max().item(),\n",
    "            'mean_sim': topk_sim[mask].mean().item() if num_valid > 0 else 0.0,\n",
    "            'conn_per_sample': mask.sum(dim=1).float().mean().item(),\n",
    "        }\n",
    "\n",
    "print(\"Attention modules defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SBM and Flat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SBMConfig:\n",
    "    height: int = 64\n",
    "    width: int = 64\n",
    "    num_bands: int = 3\n",
    "    channels: int = 16\n",
    "    # Attention flags\n",
    "    use_temporal: bool = False\n",
    "    use_neighbor: bool = False\n",
    "    use_wormhole: bool = False\n",
    "    # Attention params\n",
    "    attn_dim: int = 32\n",
    "    num_heads: int = 4\n",
    "    history_len: int = 8\n",
    "    top_k: int = 4\n",
    "    temporal_decay: float = 0.9\n",
    "    neighbor_range: int = 1\n",
    "    wormhole_threshold: float = 0.9\n",
    "    wormhole_max_conn: int = 4\n",
    "    device: str = 'cpu'\n",
    "\n",
    "\n",
    "def make_radial_masks(h, w, num_bands, device):\n",
    "    yy, xx = torch.meshgrid(\n",
    "        torch.linspace(-1.0, 1.0, h, device=device),\n",
    "        torch.linspace(-1.0, 1.0, w, device=device), indexing=\"ij\")\n",
    "    rr = torch.sqrt(yy ** 2 + xx ** 2).clamp(min=1e-6)\n",
    "    edges = torch.logspace(-3, math.log10(math.sqrt(2)), steps=num_bands + 1, device=device)\n",
    "    edges[0] = 0.0\n",
    "    masks = [torch.sigmoid((rr - edges[i]) * 20) * torch.sigmoid((edges[i+1] - rr) * 20) for i in range(num_bands)]\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    return masks / (masks.sum(dim=0, keepdim=True) + 1e-8)\n",
    "\n",
    "\n",
    "class SBMWithAttention(nn.Module):\n",
    "    \"\"\"Spectral Belief Machine with optional attention modules.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: SBMConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.register_buffer(\"window\", torch.ones(cfg.height, cfg.width, device=cfg.device))\n",
    "        self.register_buffer(\"masks\", make_radial_masks(cfg.height, cfg.width, cfg.num_bands, cfg.device))\n",
    "        \n",
    "        # Per-band processing\n",
    "        self.bands = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(2, cfg.channels, 3, padding=1), nn.GELU(),\n",
    "                nn.Conv2d(cfg.channels, cfg.channels, 3, padding=1), nn.GELU(),\n",
    "                nn.Conv2d(cfg.channels, 2, 1),\n",
    "            ) for _ in range(cfg.num_bands)\n",
    "        ])\n",
    "        \n",
    "        # Pooled feature dimension for wormhole\n",
    "        self.pool_dim = cfg.num_bands * 2 * 4  # bands * (real,imag) * 2x2_pool\n",
    "        \n",
    "        # Attention modules\n",
    "        if cfg.use_temporal:\n",
    "            self.temporal_attn = TemporalAttention(\n",
    "                feature_dim=2, num_heads=cfg.num_heads, max_len=cfg.history_len + 1,\n",
    "                top_k=cfg.top_k, decay=cfg.temporal_decay, device=cfg.device\n",
    "            )\n",
    "        \n",
    "        if cfg.use_neighbor:\n",
    "            self.neighbor_attn = NeighborAttention(\n",
    "                feature_dim=2, num_heads=cfg.num_heads,\n",
    "                layer_range=cfg.neighbor_range, device=cfg.device\n",
    "            )\n",
    "        \n",
    "        if cfg.use_wormhole:\n",
    "            self.wormhole_attn = WormholeAttention(\n",
    "                feature_dim=self.pool_dim, attn_dim=cfg.attn_dim,\n",
    "                threshold=cfg.wormhole_threshold, max_connections=cfg.wormhole_max_conn,\n",
    "                device=cfg.device\n",
    "            )\n",
    "            # Project wormhole output back to spatial\n",
    "            self.wormhole_proj = nn.Linear(self.pool_dim, cfg.num_bands * 2)\n",
    "        \n",
    "        # Fusion if using spatial attention\n",
    "        num_spatial_attn = sum([cfg.use_temporal, cfg.use_neighbor])\n",
    "        if num_spatial_attn > 0:\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(2 * (1 + num_spatial_attn), cfg.attn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(cfg.attn_dim, 2)\n",
    "            )\n",
    "        \n",
    "        self.to(cfg.device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, history: Optional[Dict] = None) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Args:\n",
    "            x: [B, 1, H, W]\n",
    "            history: Dict with 'bands': [B, T, num_bands, H, W, 2] and 'pooled': [B, T, pool_dim]\n",
    "        Returns:\n",
    "            pred: [B, 1, H, W]\n",
    "            info: Dict with features for history\n",
    "        \"\"\"\n",
    "        B, _, H, W = x.shape\n",
    "        \n",
    "        # FFT decompose\n",
    "        fft = torch.fft.fftshift(torch.fft.fft2(x.squeeze(1) * self.window))\n",
    "        \n",
    "        proc_bands = []\n",
    "        current_band_feats = []\n",
    "        \n",
    "        for i in range(self.cfg.num_bands):\n",
    "            band = fft * self.masks[i].unsqueeze(0)\n",
    "            band_feat = torch.stack([band.real, band.imag], dim=1)  # [B, 2, H, W]\n",
    "            proc = self.bands[i](band_feat)\n",
    "            processed = band_feat + proc  # Residual\n",
    "            \n",
    "            # Apply spatial attention if enabled and history available\n",
    "            if history is not None and 'bands' in history and history['bands'].shape[1] > 0:\n",
    "                band_history = history['bands'][:, :, i]  # [B, T, H, W, 2]\n",
    "                current_hw = processed.permute(0, 2, 3, 1)  # [B, H, W, 2]\n",
    "                \n",
    "                attn_outputs = [current_hw]\n",
    "                \n",
    "                if self.cfg.use_temporal:\n",
    "                    # Build temporal sequence\n",
    "                    seq = torch.cat([band_history, current_hw.unsqueeze(1)], dim=1)  # [B, T+1, H, W, 2]\n",
    "                    seq_flat = seq.reshape(B * H * W, -1, 2)  # [B*H*W, T+1, 2]\n",
    "                    temp_out = self.temporal_attn(seq_flat)[:, -1]  # [B*H*W, 2]\n",
    "                    attn_outputs.append(temp_out.reshape(B, H, W, 2))\n",
    "                \n",
    "                if self.cfg.use_neighbor:\n",
    "                    neigh_out = self.neighbor_attn(current_hw)\n",
    "                    attn_outputs.append(neigh_out)\n",
    "                \n",
    "                if len(attn_outputs) > 1:\n",
    "                    fused = self.fusion(torch.cat(attn_outputs, dim=-1))\n",
    "                    processed = processed + 0.1 * fused.permute(0, 3, 1, 2)\n",
    "            \n",
    "            proc_bands.append(processed)\n",
    "            current_band_feats.append(processed.permute(0, 2, 3, 1))  # [B, H, W, 2]\n",
    "        \n",
    "        # Pooled features for wormhole\n",
    "        pooled_bands = [F.adaptive_avg_pool2d(b, (2, 2)).flatten(1) for b in proc_bands]\n",
    "        current_pooled = torch.cat(pooled_bands, dim=1)  # [B, pool_dim]\n",
    "        \n",
    "        # Wormhole attention on pooled features\n",
    "        wormhole_stats = {'num_connections': 0, 'max_sim': 0.0}\n",
    "        if self.cfg.use_wormhole and history is not None and 'pooled' in history:\n",
    "            wh_out, wormhole_stats = self.wormhole_attn(current_pooled, history['pooled'])\n",
    "            # Project back and add to bands\n",
    "            wh_delta = self.wormhole_proj(wh_out)  # [B, num_bands * 2]\n",
    "            for i in range(self.cfg.num_bands):\n",
    "                delta_i = wh_delta[:, i*2:(i+1)*2].unsqueeze(-1).unsqueeze(-1)  # [B, 2, 1, 1]\n",
    "                proc_bands[i] = proc_bands[i] + 0.1 * delta_i\n",
    "        \n",
    "        # Reconstruct\n",
    "        fft_recon = sum(\n",
    "            torch.complex(b[:, 0], b[:, 1]) * self.masks[i].unsqueeze(0)\n",
    "            for i, b in enumerate(proc_bands)\n",
    "        )\n",
    "        pred = torch.fft.ifft2(torch.fft.ifftshift(fft_recon)).real.unsqueeze(1)\n",
    "        \n",
    "        # Pack current features for history\n",
    "        current_bands = torch.stack(current_band_feats, dim=1)  # [B, num_bands, H, W, 2]\n",
    "        \n",
    "        info = {\n",
    "            'bands': current_bands.detach(),\n",
    "            'pooled': current_pooled.detach(),\n",
    "            'wormhole_conn': wormhole_stats['num_connections'],\n",
    "            'wormhole_max_sim': wormhole_stats['max_sim'],\n",
    "        }\n",
    "        return pred, info\n",
    "\n",
    "\n",
    "class FlatBaseline(nn.Module):\n",
    "    \"\"\"Simple ConvNet baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, height, width, channels=32, use_temporal=False, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.use_temporal = use_temporal\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels, channels*2, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels*2, channels*2, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels*2, channels, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels, 1, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "        if use_temporal:\n",
    "            self.temporal = TemporalAttention(\n",
    "                feature_dim=channels, num_heads=4, max_len=16,\n",
    "                top_k=4, decay=0.9, device=device\n",
    "            )\n",
    "            self.feat_extract = nn.Conv2d(1, channels, 3, padding=1)\n",
    "            self.feat_combine = nn.Conv2d(channels * 2, channels, 1)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x, history=None):\n",
    "        if self.use_temporal and history is not None and 'feat' in history and history['feat'].shape[1] > 0:\n",
    "            feat = self.feat_extract(x)  # [B, C, H, W]\n",
    "            B, C, H, W = feat.shape\n",
    "            \n",
    "            current_hw = feat.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
    "            hist = history['feat']  # [B, T, H, W, C]\n",
    "            seq = torch.cat([hist, current_hw.unsqueeze(1)], dim=1)  # [B, T+1, H, W, C]\n",
    "            seq_flat = seq.reshape(B * H * W, -1, C)\n",
    "            temp_out = self.temporal(seq_flat)[:, -1].reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "            \n",
    "            feat = self.feat_combine(torch.cat([feat, temp_out], dim=1))\n",
    "            \n",
    "            # Continue through net (skip first conv)\n",
    "            out = feat\n",
    "            for layer in list(self.net.children())[2:]:\n",
    "                out = layer(out)\n",
    "            \n",
    "            info = {'feat': current_hw.detach()}\n",
    "        else:\n",
    "            out = self.net(x)\n",
    "            if self.use_temporal:\n",
    "                feat = self.feat_extract(x).permute(0, 2, 3, 1)  # [B, H, W, C]\n",
    "                info = {'feat': feat.detach()}\n",
    "            else:\n",
    "                info = {}\n",
    "        \n",
    "        return out, info\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Sequence-Based Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequential(model, env, num_traj, traj_len, epochs, lr, device,\n",
    "                     predict_delta=True, horizon=5, history_len=8):\n",
    "    \"\"\"Train with sequence-based batching for proper temporal history.\"\"\"\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    wh_stats = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        fields, targets = generate_trajectory_batch(env, num_traj, traj_len, 0.1, horizon)\n",
    "        fields = fields.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        T, B = fields.shape[0], fields.shape[1]\n",
    "        ep_losses = []\n",
    "        ep_wh_conn = []\n",
    "        \n",
    "        # Initialize history\n",
    "        history = None\n",
    "        \n",
    "        for t in range(T):\n",
    "            x = fields[t]\n",
    "            y = targets[t]\n",
    "            \n",
    "            pred, info = model(x, history)\n",
    "            \n",
    "            if predict_delta:\n",
    "                loss = F.mse_loss(pred - x, y - x)\n",
    "            else:\n",
    "                loss = F.mse_loss(pred, y)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            ep_losses.append(loss.item())\n",
    "            if 'wormhole_conn' in info:\n",
    "                ep_wh_conn.append(info['wormhole_conn'])\n",
    "            \n",
    "            # Update history\n",
    "            if 'bands' in info:\n",
    "                new_bands = info['bands'].unsqueeze(1)  # [B, 1, num_bands, H, W, 2]\n",
    "                new_pooled = info['pooled'].unsqueeze(1)  # [B, 1, pool_dim]\n",
    "                \n",
    "                if history is None:\n",
    "                    history = {'bands': new_bands, 'pooled': new_pooled}\n",
    "                else:\n",
    "                    history['bands'] = torch.cat([history['bands'], new_bands], dim=1)\n",
    "                    history['pooled'] = torch.cat([history['pooled'], new_pooled], dim=1)\n",
    "                    if history['bands'].shape[1] > history_len:\n",
    "                        history['bands'] = history['bands'][:, -history_len:]\n",
    "                        history['pooled'] = history['pooled'][:, -history_len:]\n",
    "            elif 'feat' in info:\n",
    "                new_feat = info['feat'].unsqueeze(1)  # [B, 1, H, W, C]\n",
    "                if history is None:\n",
    "                    history = {'feat': new_feat}\n",
    "                else:\n",
    "                    history['feat'] = torch.cat([history['feat'], new_feat], dim=1)\n",
    "                    if history['feat'].shape[1] > history_len:\n",
    "                        history['feat'] = history['feat'][:, -history_len:]\n",
    "        \n",
    "        avg_loss = sum(ep_losses) / len(ep_losses)\n",
    "        avg_wh = sum(ep_wh_conn) / len(ep_wh_conn) if ep_wh_conn else 0\n",
    "        losses.append(avg_loss)\n",
    "        wh_stats.append(avg_wh)\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        wh_str = f\", wh_conn={avg_wh:.1f}\" if ep_wh_conn else \"\"\n",
    "        print(f\"  Ep {epoch+1}/{epochs}: loss={avg_loss:.6f}{wh_str}, time={elapsed:.1f}s\")\n",
    "    \n",
    "    return {'loss': losses, 'wormhole_conn': wh_stats}\n",
    "\n",
    "print(\"Training function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "print(\"[1] Initializing environment...\")\n",
    "plasma_cfg = PlasmaConfig.turbulent(device=DEVICE, size=GRID_SIZE)\n",
    "env = TurbulentPlasmaEnv(plasma_cfg)\n",
    "print(f\"    Turbulent plasma environment ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models to test\n",
    "print(\"[2] Creating models...\")\n",
    "\n",
    "def make_sbm(num_bands, use_temporal=False, use_neighbor=False, use_wormhole=False):\n",
    "    cfg = SBMConfig(\n",
    "        height=GRID_SIZE, width=GRID_SIZE, num_bands=num_bands,\n",
    "        use_temporal=use_temporal, use_neighbor=use_neighbor, use_wormhole=use_wormhole,\n",
    "        history_len=HISTORY_LEN, top_k=TOP_K_TEMPORAL, temporal_decay=TEMPORAL_DECAY,\n",
    "        neighbor_range=NEIGHBOR_RANGE, wormhole_threshold=WORMHOLE_THRESHOLD,\n",
    "        wormhole_max_conn=WORMHOLE_MAX_CONN, device=DEVICE\n",
    "    )\n",
    "    return SBMWithAttention(cfg)\n",
    "\n",
    "models = {\n",
    "    # Band count ablation (no attention)\n",
    "    'SBM_2B_None': make_sbm(2),\n",
    "    'SBM_3B_None': make_sbm(3),\n",
    "    'SBM_7B_None': make_sbm(7),\n",
    "    # Attention type ablation (3 bands)\n",
    "    'SBM_3B_Temporal': make_sbm(3, use_temporal=True),\n",
    "    'SBM_3B_Neighbor': make_sbm(3, use_neighbor=True),\n",
    "    'SBM_3B_Wormhole': make_sbm(3, use_wormhole=True),\n",
    "    # Combined\n",
    "    'SBM_3B_All': make_sbm(3, use_temporal=True, use_neighbor=True, use_wormhole=True),\n",
    "    # Baselines\n",
    "    'Flat_None': FlatBaseline(GRID_SIZE, GRID_SIZE, 32, use_temporal=False, device=DEVICE),\n",
    "    'Flat_Temporal': FlatBaseline(GRID_SIZE, GRID_SIZE, 32, use_temporal=True, device=DEVICE),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"    {name}: {n_params:,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results[name] = train_sequential(\n",
    "        model, env, NUM_TRAJECTORIES, TRAJECTORY_LENGTH, EPOCHS, LR, DEVICE,\n",
    "        predict_delta=PREDICT_DELTA, horizon=PREDICTION_HORIZON, history_len=HISTORY_LEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Band count comparison\n",
    "ax = axes[0]\n",
    "for name in ['SBM_2B_None', 'SBM_3B_None', 'SBM_7B_None']:\n",
    "    ax.semilogy(results[name]['loss'], label=f\"{name}: {results[name]['loss'][-1]:.6f}\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Band Count Ablation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Attention type comparison\n",
    "ax = axes[1]\n",
    "for name in ['SBM_3B_None', 'SBM_3B_Temporal', 'SBM_3B_Neighbor', 'SBM_3B_Wormhole', 'SBM_3B_All']:\n",
    "    ax.semilogy(results[name]['loss'], label=f\"{name.replace('SBM_3B_', '')}: {results[name]['loss'][-1]:.6f}\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Attention Type Ablation (3 bands)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# SBM vs Flat\n",
    "ax = axes[2]\n",
    "ax.semilogy(results['SBM_3B_None']['loss'], label=f\"SBM_3B: {results['SBM_3B_None']['loss'][-1]:.6f}\")\n",
    "ax.semilogy(results['SBM_3B_All']['loss'], label=f\"SBM_3B_All: {results['SBM_3B_All']['loss'][-1]:.6f}\")\n",
    "ax.semilogy(results['Flat_None']['loss'], label=f\"Flat: {results['Flat_None']['loss'][-1]:.6f}\")\n",
    "ax.semilogy(results['Flat_Temporal']['loss'], label=f\"Flat_Temp: {results['Flat_Temporal']['loss'][-1]:.6f}\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('SBM vs Flat')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wormhole connections plot\n",
    "wh_models = [n for n in results if 'Wormhole' in n or 'All' in n]\n",
    "if wh_models:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    for name in wh_models:\n",
    "        if results[name]['wormhole_conn']:\n",
    "            ax.plot(results[name]['wormhole_conn'], label=name)\n",
    "    ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Avg Wormhole Connections')\n",
    "    ax.set_title(f'Wormhole Activity (threshold={WORMHOLE_THRESHOLD})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS (v4.2 - Pooled Wormhole)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['loss'][-1])\n",
    "\n",
    "print(f\"{'Model':<25} {'Final Loss':<15} {'vs Best':<15}\")\n",
    "print(\"-\"*55)\n",
    "best_loss = sorted_results[0][1]['loss'][-1]\n",
    "for name, res in sorted_results:\n",
    "    final = res['loss'][-1]\n",
    "    ratio = final / best_loss\n",
    "    print(f\"{name:<25} {final:<15.6f} {ratio:<15.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY COMPARISONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Band count\n",
    "print(\"\\n1. BAND COUNT (no attention):\")\n",
    "for n in ['SBM_2B_None', 'SBM_3B_None', 'SBM_7B_None']:\n",
    "    print(f\"   {n}: {results[n]['loss'][-1]:.6f}\")\n",
    "\n",
    "# Attention type\n",
    "print(\"\\n2. ATTENTION TYPE (3 bands):\")\n",
    "for n in ['SBM_3B_None', 'SBM_3B_Temporal', 'SBM_3B_Neighbor', 'SBM_3B_Wormhole', 'SBM_3B_All']:\n",
    "    print(f\"   {n}: {results[n]['loss'][-1]:.6f}\")\n",
    "\n",
    "# SBM vs Flat\n",
    "print(\"\\n3. SBM vs FLAT:\")\n",
    "sbm_best = min(results['SBM_3B_None']['loss'][-1], results['SBM_3B_All']['loss'][-1])\n",
    "flat_best = min(results['Flat_None']['loss'][-1], results['Flat_Temporal']['loss'][-1])\n",
    "print(f\"   Best SBM: {sbm_best:.6f}\")\n",
    "print(f\"   Best Flat: {flat_best:.6f}\")\n",
    "if sbm_best < flat_best:\n",
    "    print(f\"   SBM wins by {(flat_best/sbm_best - 1)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"   Flat wins by {(sbm_best/flat_best - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### v4.2 Key Fix: Pooled Wormhole\n",
    "\n",
    "The wormhole now operates on **globally pooled features** [B, D] instead of per-pixel [B, H, W, D].\n",
    "\n",
    "This is:\n",
    "1. **Memory efficient**: O(T) instead of O(H*W * T*H*W) - no more OOM\n",
    "2. **Conceptually correct**: Wormhole finds globally similar states, not pixel-to-pixel matches\n",
    "3. **What 033 actually did**: The original design pooled before wormhole\n",
    "\n",
    "### Sequence-Based Training\n",
    "\n",
    "History is now meaningful - each trajectory's history comes from its own past states,\n",
    "not random samples from different trajectories."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "standard",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
