{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 032 v3.1: Wormhole Attention Fixed\n",
    "\n",
    "**Purpose**: Fix wormhole attention to actually work.\n",
    "\n",
    "**v3.1 Fixes over v3**:\n",
    "1. **Sequence-based batching**: Process consecutive frames from same trajectories (not random samples)\n",
    "2. **Lower threshold**: 0.9 instead of 0.9995 (cosine similarity on 128-dim vectors is typically lower)\n",
    "3. **Proper history**: Each sample in batch maintains history from its own trajectory\n",
    "4. **Drop last batch**: Maintain consistent batch size for history tensor operations\n",
    "\n",
    "**The v3 Bug**: Random batching meant \"history\" was from different trajectories, making wormhole comparisons meaningless. Now we batch consecutive frames so history is temporally coherent.\n",
    "\n",
    "**Run on Colab with GPU**: Runtime -> Change runtime type -> A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GRID_SIZE = 64\n",
    "EPOCHS = 50\n",
    "DIFFICULTY = \"turbulent\"\n",
    "PREDICT_DELTA = True\n",
    "PREDICTION_HORIZON = 5\n",
    "NUM_TRAJECTORIES = 32  # Each trajectory is a sequence - this IS the batch size\n",
    "TRAJECTORY_LENGTH = 100\n",
    "LR = 0.001\n",
    "\n",
    "# Temporal Attention\n",
    "HISTORY_LEN = 8\n",
    "TOP_K_TEMPORAL = 4\n",
    "TEMPORAL_DECAY = 0.9\n",
    "\n",
    "# Wormhole Attention - FIXED threshold\n",
    "WORMHOLE_THRESHOLD = 0.9  # Was 0.9995 - way too high for 128-dim normalized vectors\n",
    "WORMHOLE_MAX_CONNECTIONS = 8\n",
    "WORMHOLE_MIN_TEMPORAL_DISTANCE = 2\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Grid: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {NUM_TRAJECTORIES} trajectories (sequence-based)\")\n",
    "print(f\"Trajectory length: {TRAJECTORY_LENGTH}\")\n",
    "print(f\"Prediction: delta t+{PREDICTION_HORIZON}\")\n",
    "print(f\"\\nWormhole: threshold={WORMHOLE_THRESHOLD}, max_conn={WORMHOLE_MAX_CONNECTIONS}\")\n",
    "print(f\"\\nKEY FIX: Sequence-based batching for proper temporal history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import math\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class PlasmaConfig:\n",
    "    height: int = 64\n",
    "    width: int = 64\n",
    "    diffusion: float = 0.25\n",
    "    advection: float = 0.08\n",
    "    noise_std: float = 0.02\n",
    "    disturbance_prob: float = 0.1\n",
    "    disturbance_strength: float = 0.15\n",
    "    num_vortices: int = 3\n",
    "    vortex_strength: float = 0.1\n",
    "    shear_strength: float = 0.05\n",
    "    multiscale_noise: bool = True\n",
    "    num_actuators: int = 9\n",
    "    _base_actuator_sigma: float = 5.0\n",
    "    device: str = \"cpu\"\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    \n",
    "    @property\n",
    "    def actuator_sigma(self) -> float:\n",
    "        return self._base_actuator_sigma * min(self.height, self.width) / 64.0\n",
    "    \n",
    "    @classmethod\n",
    "    def turbulent(cls, device: str = \"cpu\", size: int = 64):\n",
    "        return cls(height=size, width=size, diffusion=0.3, advection=0.12, noise_std=0.03,\n",
    "                   disturbance_prob=0.25, disturbance_strength=0.3,\n",
    "                   num_vortices=3, vortex_strength=0.15, shear_strength=0.08,\n",
    "                   multiscale_noise=True, num_actuators=9, device=device)\n",
    "\n",
    "\n",
    "class TurbulentPlasmaEnv:\n",
    "    def __init__(self, cfg: PlasmaConfig):\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.dtype = cfg.dtype\n",
    "        self._actuator_maps = self._build_actuator_maps()\n",
    "        self._vortex_flow = self._build_vortex_flow()\n",
    "        self._shear_flow = self._build_shear_flow()\n",
    "    \n",
    "    def _build_actuator_maps(self) -> torch.Tensor:\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        grid_n = int(math.ceil(math.sqrt(self.cfg.num_actuators)))\n",
    "        centers_y = torch.linspace(h * 0.2, h * 0.8, grid_n, device=self.device, dtype=self.dtype)\n",
    "        centers_x = torch.linspace(w * 0.2, w * 0.8, grid_n, device=self.device, dtype=self.dtype)\n",
    "        centers = torch.cartesian_prod(centers_y, centers_x)[:self.cfg.num_actuators]\n",
    "        sig2 = self.cfg.actuator_sigma ** 2\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "            torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        bumps = [torch.exp(-((yy - cy) ** 2 + (xx - cx) ** 2) / (2 * sig2)) for cy, cx in centers]\n",
    "        return torch.stack(bumps, dim=0)\n",
    "    \n",
    "    def _build_vortex_flow(self):\n",
    "        if self.cfg.num_vortices == 0: return None, None\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        torch.manual_seed(42)\n",
    "        centers_y = torch.rand(self.cfg.num_vortices, device=self.device) * (h * 0.6) + (h * 0.2)\n",
    "        centers_x = torch.rand(self.cfg.num_vortices, device=self.device) * (w * 0.6) + (w * 0.2)\n",
    "        yy, xx = torch.meshgrid(torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "                                torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        vy, vx = torch.zeros_like(yy), torch.zeros_like(xx)\n",
    "        scale = min(h, w) / 64.0\n",
    "        for i in range(self.cfg.num_vortices):\n",
    "            dy, dx = yy - centers_y[i], xx - centers_x[i]\n",
    "            r2 = dy**2 + dx**2 + 1e-6\n",
    "            decay = torch.exp(-r2 / (2 * (10 * scale)**2))\n",
    "            sign = 1 if i % 2 == 0 else -1\n",
    "            vy += sign * self.cfg.vortex_strength * (-dx) / torch.sqrt(r2) * decay\n",
    "            vx += sign * self.cfg.vortex_strength * dy / torch.sqrt(r2) * decay\n",
    "        return vy, vx\n",
    "    \n",
    "    def _build_shear_flow(self):\n",
    "        if self.cfg.shear_strength == 0: return None, None\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        yy, _ = torch.meshgrid(torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "                               torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        return torch.zeros_like(yy), self.cfg.shear_strength * torch.sin(2 * math.pi * yy / h)\n",
    "    \n",
    "    def _apply_flow(self, field, vy, vx):\n",
    "        B, C, H, W = field.shape\n",
    "        yy, xx = torch.meshgrid(torch.linspace(-1, 1, H, device=self.device),\n",
    "                                torch.linspace(-1, 1, W, device=self.device), indexing=\"ij\")\n",
    "        grid = torch.stack([xx - vx/(W/2), yy - vy/(H/2)], dim=-1).unsqueeze(0).expand(B, -1, -1, -1)\n",
    "        return F.grid_sample(field, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    \n",
    "    def _multiscale_noise(self, shape):\n",
    "        B, C, H, W = shape\n",
    "        noise = torch.zeros(shape, device=self.device, dtype=self.dtype)\n",
    "        for scale in [1, 2, 4, 8]:\n",
    "            hs, ws = H // scale, W // scale\n",
    "            if hs < 4: continue\n",
    "            coarse = torch.randn(B, C, hs, ws, device=self.device, dtype=self.dtype)\n",
    "            noise += F.interpolate(coarse, size=(H, W), mode='bilinear', align_corners=False) * (self.cfg.noise_std / scale)\n",
    "        return noise\n",
    "    \n",
    "    def reset(self, batch_size=1):\n",
    "        h, w = self.cfg.height, self.cfg.width\n",
    "        cx = torch.randint(int(w*0.3), int(w*0.7), (batch_size,), device=self.device)\n",
    "        cy = torch.randint(int(h*0.3), int(h*0.7), (batch_size,), device=self.device)\n",
    "        yy, xx = torch.meshgrid(torch.arange(h, device=self.device, dtype=self.dtype),\n",
    "                                torch.arange(w, device=self.device, dtype=self.dtype), indexing=\"ij\")\n",
    "        sig2 = (self.cfg.actuator_sigma * 1.5) ** 2\n",
    "        field = torch.zeros(batch_size, 1, h, w, device=self.device, dtype=self.dtype)\n",
    "        for b in range(batch_size):\n",
    "            field[b, 0] = torch.exp(-((yy - cy[b].float())**2 + (xx - cx[b].float())**2) / (2*sig2))\n",
    "        return field\n",
    "    \n",
    "    def step(self, field, control, noise=True):\n",
    "        B = field.shape[0]\n",
    "        lap = (F.pad(field, (0,0,1,0))[:,:,:-1,:] + F.pad(field, (0,0,0,1))[:,:,1:,:] +\n",
    "               F.pad(field, (1,0,0,0))[:,:,:,:-1] + F.pad(field, (0,1,0,0))[:,:,:,1:]) - 4*field\n",
    "        diffused = field + self.cfg.diffusion * lap\n",
    "        advected = torch.roll(diffused, shifts=(1,-1), dims=(2,3)) * self.cfg.advection + diffused * (1 - self.cfg.advection)\n",
    "        if self._vortex_flow[0] is not None:\n",
    "            advected = self._apply_flow(advected, self._vortex_flow[0], self._vortex_flow[1])\n",
    "        if self._shear_flow[0] is not None:\n",
    "            advected = self._apply_flow(advected, self._shear_flow[0], self._shear_flow[1])\n",
    "        force = torch.einsum('ba,ahw->bhw', control, self._actuator_maps).unsqueeze(1)\n",
    "        next_field = advected + force\n",
    "        if noise:\n",
    "            next_field = next_field + (self._multiscale_noise(next_field.shape) if self.cfg.multiscale_noise \n",
    "                                       else torch.randn_like(next_field) * self.cfg.noise_std)\n",
    "        return torch.clamp(next_field, -1, 1)\n",
    "\n",
    "\n",
    "def generate_trajectory_batch(env, batch_size: int, traj_len: int, control_scale: float = 0.1, horizon: int = 1):\n",
    "    \"\"\"Generate batch of trajectories - each sample in batch is from SAME timestep across different trajectories.\n",
    "    \n",
    "    Returns:\n",
    "        fields: [traj_len, batch_size, 1, H, W] - indexed by time first, then batch\n",
    "        targets: [traj_len, batch_size, 1, H, W]\n",
    "    \n",
    "    This allows sequence-based training where we iterate over time steps,\n",
    "    and each batch contains the same timestep from different trajectories.\n",
    "    History is built up naturally as we iterate through timesteps.\n",
    "    \"\"\"\n",
    "    all_fields = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Generate batch_size parallel trajectories\n",
    "    field = env.reset(batch_size)  # [B, 1, H, W]\n",
    "    trajectory = [field.clone()]\n",
    "    \n",
    "    for _ in range(traj_len + horizon):\n",
    "        ctrl = torch.clamp(torch.randn(batch_size, env.cfg.num_actuators, device=env.device) * control_scale, -1, 1)\n",
    "        field = env.step(field, ctrl)\n",
    "        trajectory.append(field.clone())\n",
    "    \n",
    "    # Extract (input, target) pairs\n",
    "    for t in range(traj_len):\n",
    "        all_fields.append(trajectory[t])  # [B, 1, H, W]\n",
    "        all_targets.append(trajectory[t + horizon])  # [B, 1, H, W]\n",
    "    \n",
    "    # Stack: [T, B, 1, H, W]\n",
    "    fields = torch.stack(all_fields, dim=0)\n",
    "    targets = torch.stack(all_targets, dim=0)\n",
    "    \n",
    "    return fields, targets\n",
    "\n",
    "print(\"Environment defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpectralConfigV3:\n",
    "    height: int = 64\n",
    "    width: int = 64\n",
    "    num_spectral_bands: int = 7\n",
    "    channels_per_band: int = 16\n",
    "    history_len: int = 8\n",
    "    num_heads: int = 4\n",
    "    top_k: int = 4\n",
    "    temporal_decay: float = 0.9\n",
    "    # Wormhole - FIXED threshold\n",
    "    wormhole_threshold: float = 0.9  # Was 0.9995 - way too high\n",
    "    wormhole_max_connections: int = 8\n",
    "    wormhole_min_temporal_distance: int = 2\n",
    "    use_windowing: bool = False\n",
    "    device: str = \"cpu\"\n",
    "\n",
    "\n",
    "def make_radial_masks(h: int, w: int, num_bands: int, device) -> torch.Tensor:\n",
    "    yy, xx = torch.meshgrid(\n",
    "        torch.linspace(-1.0, 1.0, h, device=device),\n",
    "        torch.linspace(-1.0, 1.0, w, device=device), indexing=\"ij\")\n",
    "    rr = torch.sqrt(yy ** 2 + xx ** 2).clamp(min=1e-6)\n",
    "    edges = torch.logspace(-3, math.log10(math.sqrt(2)), steps=num_bands + 1, device=device)\n",
    "    edges[0] = 0.0\n",
    "    masks = [torch.sigmoid((rr - edges[i]) * 20) * torch.sigmoid((edges[i+1] - rr) * 20) for i in range(num_bands)]\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "    return masks / (masks.sum(dim=0, keepdim=True) + 1e-8)\n",
    "\n",
    "\n",
    "class PerBandBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, channels, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels, 2, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TopKTemporalBand(nn.Module):\n",
    "    \"\"\"Temporal attention with Top-K selection.\"\"\"\n",
    "    def __init__(self, dim, num_heads, max_len, top_k=4, decay=0.9):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.top_k = top_k\n",
    "        self.decay = decay\n",
    "        self.qkv = nn.Linear(dim, 3 * dim)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(max_len, max_len), 1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        qkv = self.qkv(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        scores = scores.masked_fill(self.causal_mask[:T, :T].unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        if T > self.top_k:\n",
    "            _, topk_idx = torch.topk(scores, self.top_k, dim=-1)\n",
    "            mask = torch.ones_like(scores, dtype=torch.bool)\n",
    "            mask.scatter_(-1, topk_idx, False)\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        if self.decay < 1.0:\n",
    "            time_offsets = torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "            time_diff = time_offsets.unsqueeze(0) - time_offsets.unsqueeze(1)\n",
    "            decay_weights = torch.pow(self.decay, time_diff.clamp(min=0).float())\n",
    "            decay_weights = torch.tril(decay_weights)\n",
    "            scores = scores + torch.log(decay_weights + 1e-10).unsqueeze(0).unsqueeze(0)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        entropy = -(attn.mean(1) * torch.log(attn.mean(1) + 1e-9)).sum(-1).mean(-1)\n",
    "        out = (attn @ V).transpose(1, 2).reshape(B, T, D)\n",
    "        return self.out(out), entropy\n",
    "\n",
    "\n",
    "class WormholeAttention(nn.Module):\n",
    "    \"\"\"v3.1: Non-local attention via similarity gating - FIXED.\n",
    "    \n",
    "    Changes from v3:\n",
    "    - Threshold lowered from 0.9995 to 0.9 (configurable)\n",
    "    - Expects proper temporal history (same trajectory, consecutive frames)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, attn_dim, threshold=0.9, max_connections=8,\n",
    "                 min_temporal_distance=2, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.threshold = threshold\n",
    "        self.max_connections = max_connections\n",
    "        self.min_temporal_distance = min_temporal_distance\n",
    "        \n",
    "        self.W_q = nn.Linear(feature_dim, attn_dim, bias=False)\n",
    "        self.W_k = nn.Linear(feature_dim, attn_dim, bias=False)\n",
    "        self.W_v = nn.Linear(feature_dim, attn_dim, bias=False)\n",
    "        self.W_o = nn.Linear(attn_dim, feature_dim, bias=False)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, query_features, history_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_features: [B, D] current features\n",
    "            history_features: [B, T, D] temporal history (same trajectory!)\n",
    "        Returns:\n",
    "            output: [B, D]\n",
    "            stats: dict with connection info\n",
    "        \"\"\"\n",
    "        B, D = query_features.shape\n",
    "        T = history_features.shape[1]\n",
    "        \n",
    "        if T == 0:\n",
    "            return torch.zeros_like(query_features), {'num_connections': 0, 'mean_similarity': 0.0}\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        query_norm = F.normalize(query_features, p=2, dim=-1)  # [B, D]\n",
    "        hist_norm = F.normalize(history_features, p=2, dim=-1)  # [B, T, D]\n",
    "        \n",
    "        # Compute similarities: [B, T]\n",
    "        similarities = torch.bmm(query_norm.unsqueeze(1), hist_norm.transpose(1, 2)).squeeze(1)\n",
    "        \n",
    "        # Apply temporal distance mask (don't connect to recent frames)\n",
    "        if self.min_temporal_distance > 0 and T > self.min_temporal_distance:\n",
    "            similarities[:, -self.min_temporal_distance:] = -1.0\n",
    "        \n",
    "        # Top-K selection\n",
    "        topk_k = min(self.max_connections, T)\n",
    "        topk_vals, topk_inds = torch.topk(similarities, topk_k, dim=-1)\n",
    "        \n",
    "        # Threshold mask\n",
    "        topk_mask = topk_vals > self.threshold  # [B, K]\n",
    "        \n",
    "        num_valid = topk_mask.sum().item()\n",
    "        if num_valid == 0:\n",
    "            return torch.zeros_like(query_features), {\n",
    "                'num_connections': 0, \n",
    "                'mean_similarity': similarities.max().item(),\n",
    "                'max_similarity': similarities.max().item(),\n",
    "            }\n",
    "        \n",
    "        # Gather selected history\n",
    "        selected_hist = torch.gather(\n",
    "            history_features, 1, topk_inds.unsqueeze(-1).expand(-1, -1, D)\n",
    "        )  # [B, K, D]\n",
    "        \n",
    "        # Attention\n",
    "        Q = self.W_q(query_features).unsqueeze(1)  # [B, 1, attn_dim]\n",
    "        K = self.W_k(selected_hist)  # [B, K, attn_dim]\n",
    "        V = self.W_v(selected_hist)  # [B, K, attn_dim]\n",
    "        \n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)).squeeze(1) / math.sqrt(self.attn_dim)  # [B, K]\n",
    "        scores = scores.masked_fill(~topk_mask, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.where(topk_mask, attn_weights, torch.zeros_like(attn_weights))\n",
    "        \n",
    "        out = torch.bmm(attn_weights.unsqueeze(1), V).squeeze(1)  # [B, attn_dim]\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        stats = {\n",
    "            'num_connections': num_valid,\n",
    "            'mean_similarity': topk_vals[topk_mask].mean().item() if num_valid > 0 else 0.0,\n",
    "            'max_similarity': topk_vals.max().item(),\n",
    "            'connections_per_sample': topk_mask.sum(dim=1).float().mean().item(),\n",
    "        }\n",
    "        \n",
    "        return out, stats\n",
    "\n",
    "\n",
    "class SpectralBeliefMachineV3(nn.Module):\n",
    "    \"\"\"v3.1 SBM with Temporal + Wormhole attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: SpectralConfigV3):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.register_buffer(\"window\", torch.ones(cfg.height, cfg.width, device=cfg.device))\n",
    "        self.register_buffer(\"masks\", make_radial_masks(cfg.height, cfg.width, cfg.num_spectral_bands, cfg.device))\n",
    "        \n",
    "        self.band_blocks = nn.ModuleList([PerBandBlock(cfg.channels_per_band) for _ in range(cfg.num_spectral_bands)])\n",
    "        \n",
    "        # Feature dimension\n",
    "        self.temporal_dim = cfg.num_spectral_bands * 2 * 4  # 7 bands * 2 (real/imag) * 4 (2x2 pool)\n",
    "        self.hidden_dim = cfg.channels_per_band * 8\n",
    "        \n",
    "        self.temporal_proj_in = nn.Linear(self.temporal_dim, self.hidden_dim)\n",
    "        self.temporal_band = TopKTemporalBand(\n",
    "            self.hidden_dim, cfg.num_heads, cfg.history_len + 1,\n",
    "            top_k=cfg.top_k, decay=cfg.temporal_decay\n",
    "        )\n",
    "        self.wormhole = WormholeAttention(\n",
    "            feature_dim=self.hidden_dim,\n",
    "            attn_dim=cfg.channels_per_band * 4,\n",
    "            threshold=cfg.wormhole_threshold,\n",
    "            max_connections=cfg.wormhole_max_connections,\n",
    "            min_temporal_distance=cfg.wormhole_min_temporal_distance,\n",
    "            device=cfg.device\n",
    "        )\n",
    "        self.temporal_proj_out = nn.Linear(self.hidden_dim, self.temporal_dim)\n",
    "        self.to(cfg.device)\n",
    "    \n",
    "    def forward(self, x, history=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, 1, H, W] input field\n",
    "            history: [B, T, hidden_dim] temporal history tensor\n",
    "        Returns:\n",
    "            pred: [B, 1, H, W]\n",
    "            info: dict with 'feat' for history and stats\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # FFT decomposition\n",
    "        fft = torch.fft.fftshift(torch.fft.fft2(x.squeeze(1) * self.window))\n",
    "        \n",
    "        processed_bands = []\n",
    "        for i in range(self.cfg.num_spectral_bands):\n",
    "            band_fft = fft * self.masks[i].unsqueeze(0)\n",
    "            band_feat = torch.stack([band_fft.real, band_fft.imag], dim=1)\n",
    "            proc = self.band_blocks[i](band_feat)\n",
    "            processed_bands.append(band_feat + proc)\n",
    "        \n",
    "        # Pool band features\n",
    "        band_pooled = [F.adaptive_avg_pool2d(b, (2, 2)).flatten(1) for b in processed_bands]\n",
    "        current_feat = torch.cat(band_pooled, dim=1)  # [B, temporal_dim]\n",
    "        current_feat_proj = self.temporal_proj_in(current_feat)  # [B, hidden_dim]\n",
    "        \n",
    "        wormhole_stats = {'num_connections': 0, 'mean_similarity': 0.0, 'max_similarity': 0.0}\n",
    "        temporal_entropy = torch.zeros(B, device=x.device)\n",
    "        \n",
    "        if history is not None and history.shape[1] > 0:\n",
    "            # Build sequence for temporal attention: [B, T+1, D]\n",
    "            history_seq = torch.cat([history, current_feat_proj.unsqueeze(1)], dim=1)\n",
    "            \n",
    "            # Temporal attention\n",
    "            temporal_out, temporal_entropy = self.temporal_band(history_seq)\n",
    "            temporal_feat = temporal_out[:, -1, :]  # [B, hidden_dim]\n",
    "            \n",
    "            # Wormhole attention\n",
    "            wormhole_out, wormhole_stats = self.wormhole(current_feat_proj, history)\n",
    "            \n",
    "            # Combine\n",
    "            temporal_feat = temporal_feat + 0.1 * wormhole_out\n",
    "            \n",
    "            # Project back and apply to bands\n",
    "            temporal_delta = self.temporal_proj_out(temporal_feat)  # [B, temporal_dim]\n",
    "            chunk_size = self.temporal_dim // self.cfg.num_spectral_bands\n",
    "            for i in range(self.cfg.num_spectral_bands):\n",
    "                delta = temporal_delta[:, i*chunk_size:(i+1)*chunk_size]\n",
    "                delta_spatial = delta.view(B, 2, 2, 2)\n",
    "                delta_spatial = delta_spatial.repeat_interleave(self.cfg.height//2, 2).repeat_interleave(self.cfg.width//2, 3)\n",
    "                processed_bands[i] = processed_bands[i] + 0.1 * delta_spatial\n",
    "        \n",
    "        # Reconstruct\n",
    "        fft_recon = sum(\n",
    "            torch.complex(b[:, 0], b[:, 1]) * self.masks[i].unsqueeze(0)\n",
    "            for i, b in enumerate(processed_bands)\n",
    "        )\n",
    "        pred = torch.fft.ifft2(torch.fft.ifftshift(fft_recon)).real.unsqueeze(1)\n",
    "        \n",
    "        info = {\n",
    "            'feat': current_feat_proj.detach(),  # [B, hidden_dim]\n",
    "            'temporal_entropy': temporal_entropy,\n",
    "            'wormhole_connections': wormhole_stats['num_connections'],\n",
    "            'wormhole_mean_sim': wormhole_stats['mean_similarity'],\n",
    "            'wormhole_max_sim': wormhole_stats.get('max_similarity', 0.0),\n",
    "        }\n",
    "        return pred, info\n",
    "\n",
    "\n",
    "class FlatBaseline(nn.Module):\n",
    "    def __init__(self, height, width, channels=32, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels, channels*2, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels*2, channels*2, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels*2, channels, 3, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(channels, 1, 3, padding=1),\n",
    "        )\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x, history=None):\n",
    "        return self.net(x), {}\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with Sequence-Based Batching\n",
    "\n",
    "**Key Fix**: Instead of random batching, we iterate through timesteps sequentially.\n",
    "Each batch contains the same timestep from different trajectories.\n",
    "History builds up naturally as we iterate through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequential(model, env, num_trajectories, traj_len, epochs, lr, device,\n",
    "                     predict_delta=True, horizon=5, history_len=8):\n",
    "    \"\"\"Train with sequence-based batching for proper temporal history.\n",
    "    \n",
    "    Instead of random batching:\n",
    "    - Generate batch of parallel trajectories\n",
    "    - Iterate through timesteps sequentially\n",
    "    - History builds up naturally within each trajectory\n",
    "    \"\"\"\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    wh_connections = []\n",
    "    wh_max_sims = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Generate fresh trajectories each epoch\n",
    "        fields, targets = generate_trajectory_batch(env, num_trajectories, traj_len, 0.1, horizon)\n",
    "        # fields, targets: [T, B, 1, H, W]\n",
    "        fields = fields.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        T, B = fields.shape[0], fields.shape[1]\n",
    "        \n",
    "        ep_losses = []\n",
    "        ep_wh_conn = []\n",
    "        ep_wh_max_sim = []\n",
    "        \n",
    "        # Initialize history buffer: [B, 0, hidden_dim]\n",
    "        history = None\n",
    "        \n",
    "        # Iterate through timesteps sequentially\n",
    "        for t in range(T):\n",
    "            x = fields[t]  # [B, 1, H, W]\n",
    "            y = targets[t]  # [B, 1, H, W]\n",
    "            \n",
    "            pred, info = model(x, history)\n",
    "            \n",
    "            if predict_delta:\n",
    "                loss = F.mse_loss(pred - x, y - x)\n",
    "            else:\n",
    "                loss = F.mse_loss(pred, y)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            ep_losses.append(loss.item())\n",
    "            \n",
    "            # Update history with current features\n",
    "            if 'feat' in info:\n",
    "                new_feat = info['feat'].unsqueeze(1)  # [B, 1, D]\n",
    "                if history is None:\n",
    "                    history = new_feat\n",
    "                else:\n",
    "                    history = torch.cat([history, new_feat], dim=1)  # [B, T+1, D]\n",
    "                    if history.shape[1] > history_len:\n",
    "                        history = history[:, -history_len:]  # Keep last history_len\n",
    "            \n",
    "            # Track wormhole stats\n",
    "            if 'wormhole_connections' in info:\n",
    "                ep_wh_conn.append(info['wormhole_connections'])\n",
    "            if 'wormhole_max_sim' in info:\n",
    "                ep_wh_max_sim.append(info['wormhole_max_sim'])\n",
    "        \n",
    "        avg_loss = sum(ep_losses) / len(ep_losses)\n",
    "        avg_wh_conn = sum(ep_wh_conn) / len(ep_wh_conn) if ep_wh_conn else 0\n",
    "        avg_wh_max_sim = sum(ep_wh_max_sim) / len(ep_wh_max_sim) if ep_wh_max_sim else 0\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        wh_connections.append(avg_wh_conn)\n",
    "        wh_max_sims.append(avg_wh_max_sim)\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Ep {epoch+1}/{epochs}: loss={avg_loss:.6f}, wh_conn={avg_wh_conn:.1f}, max_sim={avg_wh_max_sim:.3f}, time={elapsed:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'loss': losses,\n",
    "        'wormhole_connections': wh_connections,\n",
    "        'wormhole_max_similarity': wh_max_sims,\n",
    "    }\n",
    "\n",
    "print(\"Training function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "print(\"[1] Initializing environment...\")\n",
    "plasma_cfg = PlasmaConfig.turbulent(device=DEVICE, size=GRID_SIZE)\n",
    "env = TurbulentPlasmaEnv(plasma_cfg)\n",
    "print(f\"    Turbulent plasma: diffusion={plasma_cfg.diffusion}, vortices={plasma_cfg.num_vortices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"[2] Initializing models...\")\n",
    "\n",
    "cfg = SpectralConfigV3(\n",
    "    height=GRID_SIZE, width=GRID_SIZE,\n",
    "    history_len=HISTORY_LEN,\n",
    "    top_k=TOP_K_TEMPORAL,\n",
    "    temporal_decay=TEMPORAL_DECAY,\n",
    "    wormhole_threshold=WORMHOLE_THRESHOLD,\n",
    "    wormhole_max_connections=WORMHOLE_MAX_CONNECTIONS,\n",
    "    wormhole_min_temporal_distance=WORMHOLE_MIN_TEMPORAL_DISTANCE,\n",
    "    device=DEVICE\n",
    ")\n",
    "sbm = SpectralBeliefMachineV3(cfg)\n",
    "flat = FlatBaseline(GRID_SIZE, GRID_SIZE, 32, DEVICE)\n",
    "\n",
    "print(f\"    SBM v3.1: {sum(p.numel() for p in sbm.parameters()):,} params\")\n",
    "print(f\"    Wormhole threshold: {cfg.wormhole_threshold} (was 0.9995 in v3)\")\n",
    "print(f\"    FlatBaseline: {sum(p.numel() for p in flat.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SBM v3.1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training SBM v3.1 (Wormhole FIXED)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sbm_metrics = train_sequential(\n",
    "    sbm, env, NUM_TRAJECTORIES, TRAJECTORY_LENGTH, EPOCHS, LR, DEVICE,\n",
    "    predict_delta=PREDICT_DELTA, horizon=PREDICTION_HORIZON, history_len=HISTORY_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Flat baseline (for comparison)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training FlatBaseline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Flat doesn't need sequential training, but we use same setup for fair comparison\n",
    "flat_metrics = train_sequential(\n",
    "    flat, env, NUM_TRAJECTORIES, TRAJECTORY_LENGTH, EPOCHS, LR, DEVICE,\n",
    "    predict_delta=PREDICT_DELTA, horizon=PREDICTION_HORIZON, history_len=HISTORY_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.semilogy(sbm_metrics['loss'], label=f'SBM v3.1 - final: {sbm_metrics[\"loss\"][-1]:.6f}')\n",
    "ax.semilogy(flat_metrics['loss'], label=f'FlatBaseline - final: {flat_metrics[\"loss\"][-1]:.6f}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Wormhole connections\n",
    "ax = axes[1]\n",
    "ax.plot(sbm_metrics['wormhole_connections'], label='Wormhole Connections')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Avg Connections per Step')\n",
    "ax.set_title(f'Wormhole Activity (threshold={WORMHOLE_THRESHOLD})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Wormhole max similarity\n",
    "ax = axes[2]\n",
    "ax.plot(sbm_metrics['wormhole_max_similarity'], label='Max Similarity')\n",
    "ax.axhline(y=WORMHOLE_THRESHOLD, color='r', linestyle='--', label=f'Threshold ({WORMHOLE_THRESHOLD})')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cosine Similarity')\n",
    "ax.set_title('Wormhole Max Similarity vs Threshold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS (v3.1 with Fixed Wormhole)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SBM v3.1: {sbm_metrics['loss'][-1]:.6f}\")\n",
    "print(f\"Flat:     {flat_metrics['loss'][-1]:.6f}\")\n",
    "print(f\"\\nWormhole Stats:\")\n",
    "print(f\"  Avg connections (final): {sbm_metrics['wormhole_connections'][-1]:.1f}\")\n",
    "print(f\"  Avg max similarity (final): {sbm_metrics['wormhole_max_similarity'][-1]:.3f}\")\n",
    "print(f\"  Threshold: {WORMHOLE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "print(\"\\nGenerating test trajectory for visualization...\")\n",
    "\n",
    "# Generate a test trajectory\n",
    "test_fields, test_targets = generate_trajectory_batch(env, 1, 20, 0.1, PREDICTION_HORIZON)\n",
    "test_fields = test_fields.to(DEVICE)\n",
    "test_targets = test_targets.to(DEVICE)\n",
    "\n",
    "# Run models\n",
    "sbm.eval()\n",
    "flat.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Warm up history\n",
    "    history = None\n",
    "    for t in range(10):\n",
    "        _, info = sbm(test_fields[t], history)\n",
    "        if 'feat' in info:\n",
    "            new_feat = info['feat'].unsqueeze(1)\n",
    "            history = new_feat if history is None else torch.cat([history, new_feat], dim=1)[:, -HISTORY_LEN:]\n",
    "    \n",
    "    # Get predictions at t=15\n",
    "    t = 15\n",
    "    x = test_fields[t]\n",
    "    y = test_targets[t]\n",
    "    \n",
    "    sbm_pred, sbm_info = sbm(x, history)\n",
    "    flat_pred, _ = flat(x, None)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "axes[0, 0].imshow(x[0, 0].cpu(), cmap='viridis')\n",
    "axes[0, 0].set_title('Input')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(y[0, 0].cpu(), cmap='viridis')\n",
    "axes[0, 1].set_title(f'Target (t+{PREDICTION_HORIZON})')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(sbm_pred[0, 0].cpu(), cmap='viridis')\n",
    "axes[0, 2].set_title('SBM v3.1 Prediction')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[0, 3].imshow(flat_pred[0, 0].cpu(), cmap='viridis')\n",
    "axes[0, 3].set_title('Flat Prediction')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "sbm_err = (sbm_pred - y).abs()[0, 0].cpu()\n",
    "flat_err = (flat_pred - y).abs()[0, 0].cpu()\n",
    "vmax = max(sbm_err.max(), flat_err.max())\n",
    "\n",
    "axes[1, 2].imshow(sbm_err, cmap='hot', vmin=0, vmax=vmax)\n",
    "axes[1, 2].set_title(f'SBM Error (MSE: {F.mse_loss(sbm_pred, y).item():.4f})')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[1, 3].imshow(flat_err, cmap='hot', vmin=0, vmax=vmax)\n",
    "axes[1, 3].set_title(f'Flat Error (MSE: {F.mse_loss(flat_pred, y).item():.4f})')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle(f'Wormhole connections at this step: {sbm_info[\"wormhole_connections\"]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### v3.1 Fixes:\n",
    "\n",
    "1. **Sequence-based batching**: Each batch processes the same timestep across different trajectories.\n",
    "   History builds up naturally as we iterate through timesteps.\n",
    "\n",
    "2. **Lower wormhole threshold**: Changed from 0.9995 to 0.9.\n",
    "   With 128-dim normalized vectors, cosine similarities are typically in the 0.8-0.95 range.\n",
    "\n",
    "3. **Proper history tensor**: History is now `[B, T, D]` where T grows over time within each epoch.\n",
    "   Each sample's history corresponds to its own trajectory.\n",
    "\n",
    "### Why v3 Had Zero Connections:\n",
    "\n",
    "- **Wrong batching**: Random samples from different trajectories were mixed.\n",
    "  The \"history\" for each sample was actually features from unrelated trajectories.\n",
    "\n",
    "- **Threshold too high**: 0.9995 cosine similarity on 128-dim vectors is nearly impossible.\n",
    "  Even identical vectors with slight floating-point noise wouldn't pass.\n",
    "\n",
    "### What Wormhole Should Do:\n",
    "\n",
    "Connect the current state to temporally distant but structurally similar past states.\n",
    "This enables \"teleportation\" of information across time when patterns repeat."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "standard",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
