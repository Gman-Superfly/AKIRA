{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 032 v2: Turbulent Plasma with Top-K Sparse Attention\n",
        "\n",
        "**Purpose**: Test 7+1 SBM on a HARDER environment with multi-scale turbulent structure.\n",
        "\n",
        "**Key Improvements over v1**:\n",
        "- **Turbulent plasma**: Adds vortices, shear, and multi-scale noise\n",
        "- **Top-K sparse temporal attention**: From Exp 033, improves efficiency and focuses on relevant history\n",
        "- **Exponential decay weighting**: Older states contribute less\n",
        "- **Harder prediction**: t+5 horizon, more disturbances\n",
        "\n",
        "**Run on Colab with GPU**: Runtime -> Change runtime type -> A100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "GRID_SIZE = 128  # 64, 128, or 256\n",
        "EPOCHS = 100\n",
        "DIFFICULTY = \"turbulent\"  # easy, medium, hard, turbulent (NEW)\n",
        "PREDICT_DELTA = True\n",
        "PREDICTION_HORIZON = 5  # Harder than v1's t+3\n",
        "NUM_TRAJECTORIES = 100\n",
        "TRAJECTORY_LENGTH = 100\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "\n",
        "# Top-K Attention Settings (from Exp 033)\n",
        "TOP_K_TEMPORAL = 4  # Number of past states to attend to\n",
        "TEMPORAL_DECAY = 0.9  # Exponential decay for older states\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Grid size: {GRID_SIZE}x{GRID_SIZE}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Difficulty: {DIFFICULTY}\")\n",
        "print(f\"Prediction mode: {'delta' if PREDICT_DELTA else 'absolute'}\")\n",
        "print(f\"Prediction horizon: t+{PREDICTION_HORIZON}\")\n",
        "print(f\"Top-K temporal: {TOP_K_TEMPORAL}, decay: {TEMPORAL_DECAY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment: Turbulent Plasma Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import math\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class PlasmaConfig:\n",
        "    \"\"\"Configuration for the turbulent plasma environment.\n",
        "    \n",
        "    v2 adds: vortices, shear flows, multi-scale noise for harder prediction.\n",
        "    All spatial parameters auto-scale with grid size.\n",
        "    \"\"\"\n",
        "    height: int = 64\n",
        "    width: int = 64\n",
        "    # Physics (scale-independent)\n",
        "    diffusion: float = 0.25\n",
        "    advection: float = 0.08\n",
        "    noise_std: float = 0.02\n",
        "    disturbance_prob: float = 0.1\n",
        "    disturbance_strength: float = 0.15\n",
        "    # v2: Turbulence parameters\n",
        "    num_vortices: int = 3  # Number of rotating vortex centers\n",
        "    vortex_strength: float = 0.1  # Rotation magnitude\n",
        "    shear_strength: float = 0.05  # Horizontal shear flow\n",
        "    multiscale_noise: bool = True  # Add noise at multiple frequencies\n",
        "    # Spatial (base values for 64x64, will be scaled)\n",
        "    # NOTE: num_actuators should be a perfect square (4, 9, 16) to fill grid evenly\n",
        "    num_actuators: int = 9  # 3x3 grid fills entire field\n",
        "    _base_actuator_sigma: float = 5.0  # Base sigma for 64x64\n",
        "    # Device\n",
        "    device: str = \"cpu\"\n",
        "    dtype: torch.dtype = torch.float32\n",
        "    \n",
        "    @property\n",
        "    def actuator_sigma(self) -> float:\n",
        "        \"\"\"Actuator sigma scaled to grid size.\"\"\"\n",
        "        scale = min(self.height, self.width) / 64.0\n",
        "        return self._base_actuator_sigma * scale\n",
        "    \n",
        "    @classmethod\n",
        "    def easy(cls, device: str = \"cpu\", size: int = 64) -> \"PlasmaConfig\":\n",
        "        return cls(height=size, width=size, diffusion=0.12, advection=0.02, noise_std=0.01,\n",
        "                   disturbance_prob=0.0, disturbance_strength=0.0, \n",
        "                   num_vortices=0, vortex_strength=0.0, shear_strength=0.0, \n",
        "                   multiscale_noise=False, num_actuators=9, device=device)\n",
        "    \n",
        "    @classmethod\n",
        "    def medium(cls, device: str = \"cpu\", size: int = 64) -> \"PlasmaConfig\":\n",
        "        return cls(height=size, width=size, num_vortices=0, vortex_strength=0.0, \n",
        "                   shear_strength=0.0, multiscale_noise=False, num_actuators=9, device=device)\n",
        "    \n",
        "    @classmethod\n",
        "    def hard(cls, device: str = \"cpu\", size: int = 64) -> \"PlasmaConfig\":\n",
        "        return cls(height=size, width=size, diffusion=0.4, advection=0.15, noise_std=0.05,\n",
        "                   disturbance_prob=0.2, disturbance_strength=0.25,\n",
        "                   num_vortices=1, vortex_strength=0.05, shear_strength=0.02,\n",
        "                   multiscale_noise=False, num_actuators=9, device=device)\n",
        "    \n",
        "    @classmethod\n",
        "    def turbulent(cls, device: str = \"cpu\", size: int = 64) -> \"PlasmaConfig\":\n",
        "        \"\"\"NEW: Turbulent mode with vortices, shear, and multi-scale noise.\"\"\"\n",
        "        return cls(height=size, width=size, diffusion=0.3, advection=0.12, noise_std=0.03,\n",
        "                   disturbance_prob=0.25, disturbance_strength=0.3,\n",
        "                   num_vortices=3, vortex_strength=0.15, shear_strength=0.08,\n",
        "                   multiscale_noise=True, num_actuators=9, device=device)\n",
        "\n",
        "\n",
        "class TurbulentPlasmaEnv:\n",
        "    \"\"\"v2 Plasma environment with turbulence, vortices, and multi-scale structure.\"\"\"\n",
        "    \n",
        "    def __init__(self, cfg: PlasmaConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(cfg.device)\n",
        "        self.dtype = cfg.dtype\n",
        "        self._actuator_maps = self._build_actuator_maps()\n",
        "        self._vortex_centers = self._init_vortex_centers()\n",
        "        self._vortex_flow = self._build_vortex_flow()\n",
        "        self._shear_flow = self._build_shear_flow()\n",
        "        \n",
        "    def _build_actuator_maps(self) -> torch.Tensor:\n",
        "        h, w = self.cfg.height, self.cfg.width\n",
        "        grid_n = int(math.ceil(math.sqrt(self.cfg.num_actuators)))\n",
        "        centers_y = torch.linspace(h * 0.2, h * 0.8, grid_n, device=self.device, dtype=self.dtype)\n",
        "        centers_x = torch.linspace(w * 0.2, w * 0.8, grid_n, device=self.device, dtype=self.dtype)\n",
        "        centers = torch.cartesian_prod(centers_y, centers_x)[:self.cfg.num_actuators]\n",
        "        sig2 = self.cfg.actuator_sigma ** 2\n",
        "        bumps = []\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.arange(h, device=self.device, dtype=self.dtype),\n",
        "            torch.arange(w, device=self.device, dtype=self.dtype),\n",
        "            indexing=\"ij\",\n",
        "        )\n",
        "        for cy, cx in centers:\n",
        "            bump = torch.exp(-((yy - cy) ** 2 + (xx - cx) ** 2) / (2 * sig2))\n",
        "            bumps.append(bump)\n",
        "        return torch.stack(bumps, dim=0)\n",
        "    \n",
        "    def _init_vortex_centers(self) -> torch.Tensor:\n",
        "        \"\"\"Initialize random vortex center positions.\"\"\"\n",
        "        if self.cfg.num_vortices == 0:\n",
        "            return None\n",
        "        h, w = self.cfg.height, self.cfg.width\n",
        "        # Random positions in central 60% of grid\n",
        "        centers_y = torch.rand(self.cfg.num_vortices, device=self.device) * (h * 0.6) + (h * 0.2)\n",
        "        centers_x = torch.rand(self.cfg.num_vortices, device=self.device) * (w * 0.6) + (w * 0.2)\n",
        "        return torch.stack([centers_y, centers_x], dim=1)\n",
        "    \n",
        "    def _build_vortex_flow(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Build velocity field from vortices.\"\"\"\n",
        "        if self.cfg.num_vortices == 0 or self._vortex_centers is None:\n",
        "            return None, None\n",
        "        h, w = self.cfg.height, self.cfg.width\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.arange(h, device=self.device, dtype=self.dtype),\n",
        "            torch.arange(w, device=self.device, dtype=self.dtype),\n",
        "            indexing=\"ij\",\n",
        "        )\n",
        "        vy = torch.zeros_like(yy)\n",
        "        vx = torch.zeros_like(xx)\n",
        "        scale = min(h, w) / 64.0  # Scale vortex size with grid\n",
        "        for i in range(self.cfg.num_vortices):\n",
        "            cy, cx = self._vortex_centers[i]\n",
        "            dy = yy - cy\n",
        "            dx = xx - cx\n",
        "            r2 = dy**2 + dx**2 + 1e-6\n",
        "            # Vortex: velocity perpendicular to radius, decays with distance\n",
        "            decay = torch.exp(-r2 / (2 * (10 * scale)**2))\n",
        "            sign = 1 if i % 2 == 0 else -1  # Alternating rotation\n",
        "            vy += sign * self.cfg.vortex_strength * (-dx) / torch.sqrt(r2) * decay\n",
        "            vx += sign * self.cfg.vortex_strength * dy / torch.sqrt(r2) * decay\n",
        "        return vy, vx\n",
        "    \n",
        "    def _build_shear_flow(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Build horizontal shear flow (varies with y).\"\"\"\n",
        "        if self.cfg.shear_strength == 0:\n",
        "            return None, None\n",
        "        h, w = self.cfg.height, self.cfg.width\n",
        "        yy, _ = torch.meshgrid(\n",
        "            torch.arange(h, device=self.device, dtype=self.dtype),\n",
        "            torch.arange(w, device=self.device, dtype=self.dtype),\n",
        "            indexing=\"ij\",\n",
        "        )\n",
        "        # Shear: vx varies sinusoidally with y\n",
        "        vx = self.cfg.shear_strength * torch.sin(2 * math.pi * yy / h)\n",
        "        vy = torch.zeros_like(vx)\n",
        "        return vy, vx\n",
        "    \n",
        "    def _apply_flow(self, field: torch.Tensor, vy: torch.Tensor, vx: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply velocity field via semi-Lagrangian advection.\"\"\"\n",
        "        B, C, H, W = field.shape\n",
        "        # Create sampling grid\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.linspace(-1, 1, H, device=self.device),\n",
        "            torch.linspace(-1, 1, W, device=self.device),\n",
        "            indexing=\"ij\",\n",
        "        )\n",
        "        # Normalized velocity (from pixel to [-1, 1] coords)\n",
        "        vy_norm = vy / (H / 2)\n",
        "        vx_norm = vx / (W / 2)\n",
        "        # Backtrack: where did this point come from?\n",
        "        sample_y = yy - vy_norm\n",
        "        sample_x = xx - vx_norm\n",
        "        # Grid for grid_sample: [B, H, W, 2] with (x, y) order\n",
        "        grid = torch.stack([sample_x, sample_y], dim=-1).unsqueeze(0).expand(B, -1, -1, -1)\n",
        "        advected = F.grid_sample(field, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
        "        return advected\n",
        "    \n",
        "    def _multiscale_noise(self, shape: Tuple[int, ...]) -> torch.Tensor:\n",
        "        \"\"\"Generate noise at multiple spatial frequencies.\"\"\"\n",
        "        B, C, H, W = shape\n",
        "        noise = torch.zeros(shape, device=self.device, dtype=self.dtype)\n",
        "        # Add noise at different scales: 1x, 2x, 4x, 8x pooling\n",
        "        for scale in [1, 2, 4, 8]:\n",
        "            h_s, w_s = H // scale, W // scale\n",
        "            if h_s < 4 or w_s < 4:\n",
        "                continue\n",
        "            coarse = torch.randn(B, C, h_s, w_s, device=self.device, dtype=self.dtype)\n",
        "            upsampled = F.interpolate(coarse, size=(H, W), mode='bilinear', align_corners=False)\n",
        "            noise += upsampled * (self.cfg.noise_std / scale)\n",
        "        return noise\n",
        "    \n",
        "    def reset(self, batch_size: int = 1) -> torch.Tensor:\n",
        "        h, w = self.cfg.height, self.cfg.width\n",
        "        cx = torch.randint(int(w * 0.3), int(w * 0.7), (batch_size,), device=self.device)\n",
        "        cy = torch.randint(int(h * 0.3), int(h * 0.7), (batch_size,), device=self.device)\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.arange(h, device=self.device, dtype=self.dtype),\n",
        "            torch.arange(w, device=self.device, dtype=self.dtype),\n",
        "            indexing=\"ij\",\n",
        "        )\n",
        "        sig2 = (self.cfg.actuator_sigma * 1.5) ** 2\n",
        "        field = torch.zeros(batch_size, 1, h, w, device=self.device, dtype=self.dtype)\n",
        "        for b in range(batch_size):\n",
        "            blob = torch.exp(-((yy - cy[b].float()) ** 2 + (xx - cx[b].float()) ** 2) / (2 * sig2))\n",
        "            field[b, 0] = blob\n",
        "        return field\n",
        "    \n",
        "    def step(self, field: torch.Tensor, control: torch.Tensor, noise: bool = True) -> torch.Tensor:\n",
        "        B = field.shape[0]\n",
        "        # Diffusion (Laplacian)\n",
        "        lap = (F.pad(field, (0, 0, 1, 0))[:, :, :-1, :]\n",
        "               + F.pad(field, (0, 0, 0, 1))[:, :, 1:, :]\n",
        "               + F.pad(field, (1, 0, 0, 0))[:, :, :, :-1]\n",
        "               + F.pad(field, (0, 1, 0, 0))[:, :, :, 1:]) - 4 * field\n",
        "        diffused = field + self.cfg.diffusion * lap\n",
        "        \n",
        "        # Standard advection\n",
        "        advected = (torch.roll(diffused, shifts=(1, -1), dims=(2, 3)) * self.cfg.advection\n",
        "                    + diffused * (1 - self.cfg.advection))\n",
        "        \n",
        "        # v2: Vortex advection\n",
        "        if self._vortex_flow[0] is not None:\n",
        "            advected = self._apply_flow(advected, self._vortex_flow[0], self._vortex_flow[1])\n",
        "        \n",
        "        # v2: Shear flow\n",
        "        if self._shear_flow[0] is not None:\n",
        "            advected = self._apply_flow(advected, self._shear_flow[0], self._shear_flow[1])\n",
        "        \n",
        "        # Actuator forces\n",
        "        bumps = self._actuator_maps.unsqueeze(0).expand(B, -1, -1, -1)\n",
        "        control_expanded = control.unsqueeze(-1).unsqueeze(-1)\n",
        "        force = torch.sum(control_expanded * bumps, dim=1, keepdim=True)\n",
        "        next_field = advected + force\n",
        "        \n",
        "        # Noise\n",
        "        if noise:\n",
        "            if self.cfg.multiscale_noise:\n",
        "                next_field = next_field + self._multiscale_noise(next_field.shape)\n",
        "            elif self.cfg.noise_std > 0:\n",
        "                next_field = next_field + torch.randn_like(next_field) * self.cfg.noise_std\n",
        "        \n",
        "        # Random disturbances\n",
        "        if self.cfg.disturbance_prob > 0 and torch.rand(1).item() < self.cfg.disturbance_prob:\n",
        "            h, w = self.cfg.height, self.cfg.width\n",
        "            cy = torch.randint(int(h * 0.2), int(h * 0.8), (1,)).item()\n",
        "            cx = torch.randint(int(w * 0.2), int(w * 0.8), (1,)).item()\n",
        "            yy, xx = torch.meshgrid(\n",
        "                torch.arange(h, device=self.device, dtype=self.dtype),\n",
        "                torch.arange(w, device=self.device, dtype=self.dtype),\n",
        "                indexing=\"ij\",\n",
        "            )\n",
        "            sig2 = (self.cfg.actuator_sigma * 1.5) ** 2\n",
        "            bump = torch.exp(-((yy - cy) ** 2 + (xx - cx) ** 2) / (2 * sig2))\n",
        "            sign = 2 * (torch.rand(1).item() > 0.5) - 1\n",
        "            disturbance = sign * self.cfg.disturbance_strength * bump.unsqueeze(0).unsqueeze(0)\n",
        "            next_field = next_field + disturbance.expand(B, -1, -1, -1)\n",
        "        \n",
        "        return torch.clamp(next_field, min=-1.0, max=1.0)\n",
        "\n",
        "\n",
        "def generate_trajectories(\n",
        "    env: TurbulentPlasmaEnv,\n",
        "    num_trajectories: int,\n",
        "    trajectory_length: int,\n",
        "    control_scale: float = 0.1,\n",
        "    prediction_horizon: int = 1,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Generate training data with multi-step prediction.\"\"\"\n",
        "    all_fields, all_next_fields, all_controls = [], [], []\n",
        "    \n",
        "    for _ in range(num_trajectories):\n",
        "        field = env.reset(batch_size=1)\n",
        "        trajectory = [field.clone()]\n",
        "        controls_traj = []\n",
        "        \n",
        "        for _ in range(trajectory_length + prediction_horizon):\n",
        "            control = torch.randn(1, env.cfg.num_actuators, device=env.device, dtype=env.dtype) * control_scale\n",
        "            control = torch.clamp(control, -1, 1)\n",
        "            next_field = env.step(field, control, noise=True)\n",
        "            trajectory.append(next_field.clone())\n",
        "            controls_traj.append(control)\n",
        "            field = next_field.detach()\n",
        "        \n",
        "        for t in range(trajectory_length):\n",
        "            all_fields.append(trajectory[t])\n",
        "            all_next_fields.append(trajectory[t + prediction_horizon])\n",
        "            all_controls.append(controls_traj[t])\n",
        "    \n",
        "    return torch.cat(all_fields), torch.cat(all_next_fields), torch.cat(all_controls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Models: SBM v2 with Top-K Sparse Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SpectralConfig:\n",
        "    \"\"\"Configuration for the Spectral Belief Machine v2.\"\"\"\n",
        "    height: int = 64\n",
        "    width: int = 64\n",
        "    num_spectral_bands: int = 7\n",
        "    channels_per_band: int = 16\n",
        "    history_len: int = 8  # Increased from 4\n",
        "    num_heads: int = 4\n",
        "    # v2: Top-K sparse attention\n",
        "    top_k: int = 4  # Number of past states to attend to\n",
        "    temporal_decay: float = 0.9  # Exponential decay for older states\n",
        "    # Windowing OFF by default\n",
        "    use_windowing: bool = False\n",
        "    band_lr_multipliers: List[float] = field(default_factory=lambda: [\n",
        "        0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 0.1\n",
        "    ])\n",
        "    device: str = \"cpu\"\n",
        "    dtype: torch.dtype = torch.float32\n",
        "\n",
        "\n",
        "def make_hamming_window(h: int, w: int, device: torch.device) -> torch.Tensor:\n",
        "    wy = torch.hamming_window(h, device=device)\n",
        "    wx = torch.hamming_window(w, device=device)\n",
        "    return wy.unsqueeze(1) * wx.unsqueeze(0)\n",
        "\n",
        "\n",
        "def make_radial_masks(h: int, w: int, num_bands: int, device: torch.device) -> torch.Tensor:\n",
        "    yy, xx = torch.meshgrid(\n",
        "        torch.linspace(-1.0, 1.0, h, device=device),\n",
        "        torch.linspace(-1.0, 1.0, w, device=device),\n",
        "        indexing=\"ij\",\n",
        "    )\n",
        "    rr = torch.sqrt(yy ** 2 + xx ** 2).clamp(min=1e-6)\n",
        "    edges = torch.logspace(-3, math.log10(math.sqrt(2)), steps=num_bands + 1, device=device)\n",
        "    edges[0] = 0.0\n",
        "    masks = []\n",
        "    for i in range(num_bands):\n",
        "        lo, hi = edges[i], edges[i + 1]\n",
        "        mask = torch.sigmoid((rr - lo) * 20) * torch.sigmoid((hi - rr) * 20)\n",
        "        masks.append(mask)\n",
        "    masks = torch.stack(masks, dim=0)\n",
        "    return masks / (masks.sum(dim=0, keepdim=True) + 1e-8)\n",
        "\n",
        "\n",
        "class PerBandBlock(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(2, channels, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(channels, 2, kernel_size=1),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TopKTemporalBand(nn.Module):\n",
        "    \"\"\"v2: Temporal attention with Top-K selection and exponential decay.\n",
        "    \n",
        "    From Experiment 033: Attends to top-K most relevant past states,\n",
        "    with exponential decay weighting for recency bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_heads: int, max_len: int, top_k: int = 4, decay: float = 0.9):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.top_k = top_k\n",
        "        self.decay = decay\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, dim)\n",
        "        self.v_proj = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(max_len, max_len), diagonal=1).bool())\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B, T, D = x.shape\n",
        "        Q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        \n",
        "        # Apply causal mask\n",
        "        scores = scores.masked_fill(self.causal_mask[:T, :T].unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "        \n",
        "        # v2: Top-K selection per query position\n",
        "        if T > self.top_k:\n",
        "            # Get top-k scores for each query (last dim is keys)\n",
        "            topk_vals, topk_inds = torch.topk(scores, min(self.top_k, T), dim=-1)\n",
        "            # Create mask for non-top-k positions\n",
        "            mask = torch.ones_like(scores, dtype=torch.bool)\n",
        "            mask.scatter_(-1, topk_inds, False)\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "        \n",
        "        # v2: Exponential decay for older positions\n",
        "        if self.decay < 1.0:\n",
        "            time_offsets = torch.arange(T, device=x.device, dtype=x.dtype)\n",
        "            # decay_weights[i, j] = decay^(i - j) for j < i\n",
        "            time_diff = time_offsets.unsqueeze(0) - time_offsets.unsqueeze(1)  # [T, T]\n",
        "            decay_weights = torch.pow(self.decay, time_diff.clamp(min=0).float())\n",
        "            decay_weights = torch.tril(decay_weights)  # Causal\n",
        "            scores = scores + torch.log(decay_weights + 1e-10).unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn_avg = attn.mean(dim=1)\n",
        "        entropy = -(attn_avg * torch.log(attn_avg + 1e-9)).sum(dim=-1).mean(dim=1)\n",
        "        \n",
        "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.out_proj(out), entropy\n",
        "\n",
        "\n",
        "class SpectralBeliefMachineV2(nn.Module):\n",
        "    \"\"\"v2 SBM with Top-K sparse temporal attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, cfg: SpectralConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        \n",
        "        # Windowing (optional)\n",
        "        if cfg.use_windowing:\n",
        "            self.register_buffer(\"window\", make_hamming_window(cfg.height, cfg.width, torch.device(cfg.device)))\n",
        "        else:\n",
        "            self.register_buffer(\"window\", torch.ones(cfg.height, cfg.width, device=torch.device(cfg.device)))\n",
        "        \n",
        "        self.register_buffer(\"masks\", make_radial_masks(cfg.height, cfg.width, cfg.num_spectral_bands, torch.device(cfg.device)))\n",
        "        self.band_blocks = nn.ModuleList([PerBandBlock(cfg.channels_per_band) for _ in range(cfg.num_spectral_bands)])\n",
        "        \n",
        "        temporal_dim = cfg.num_spectral_bands * 2 * 4\n",
        "        self.temporal_proj_in = nn.Linear(temporal_dim, cfg.channels_per_band * 8)\n",
        "        # v2: Use TopKTemporalBand instead of regular\n",
        "        # max_len = history_len + 1 because we append current frame to history\n",
        "        self.temporal_band = TopKTemporalBand(\n",
        "            cfg.channels_per_band * 8, cfg.num_heads, cfg.history_len + 1,\n",
        "            top_k=cfg.top_k, decay=cfg.temporal_decay\n",
        "        )\n",
        "        self.temporal_proj_out = nn.Linear(cfg.channels_per_band * 8, temporal_dim)\n",
        "        self.to(cfg.device)\n",
        "    \n",
        "    def _fft_decompose(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        x_windowed = x.squeeze(1) * self.window\n",
        "        fft = torch.fft.fft2(x_windowed)\n",
        "        fft_shifted = torch.fft.fftshift(fft)\n",
        "        bands = []\n",
        "        for i in range(self.cfg.num_spectral_bands):\n",
        "            mask = self.masks[i].unsqueeze(0)\n",
        "            band_fft = fft_shifted * mask\n",
        "            band_feat = torch.stack([band_fft.real, band_fft.imag], dim=1)\n",
        "            bands.append(band_feat)\n",
        "        return bands\n",
        "    \n",
        "    def _fft_reconstruct(self, bands: List[torch.Tensor]) -> torch.Tensor:\n",
        "        fft_recon = None\n",
        "        for i, band in enumerate(bands):\n",
        "            mask = self.masks[i].unsqueeze(0)\n",
        "            band_fft = torch.complex(band[:, 0], band[:, 1]) * mask\n",
        "            if fft_recon is None:\n",
        "                fft_recon = band_fft\n",
        "            else:\n",
        "                fft_recon = fft_recon + band_fft\n",
        "        fft_unshifted = torch.fft.ifftshift(fft_recon)\n",
        "        spatial = torch.fft.ifft2(fft_unshifted).real\n",
        "        return spatial.unsqueeze(1)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, history: Optional[List[torch.Tensor]] = None\n",
        "                ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        B = x.shape[0]\n",
        "        bands = self._fft_decompose(x)\n",
        "        processed_bands = []\n",
        "        band_entropies = []\n",
        "        \n",
        "        for i, band in enumerate(bands):\n",
        "            proc = self.band_blocks[i](band)\n",
        "            processed_bands.append(band + proc)\n",
        "            band_entropies.append(torch.tensor(0.0, device=x.device))\n",
        "        \n",
        "        # Temporal band features\n",
        "        band_pooled = [F.adaptive_avg_pool2d(b, (2, 2)).flatten(1) for b in processed_bands]\n",
        "        current_feat = torch.cat(band_pooled, dim=1)\n",
        "        \n",
        "        if history is not None:\n",
        "            history = [h for h in history if h.shape[0] == B and h.dim() == 2 and h.shape[1] == current_feat.shape[1]]\n",
        "        \n",
        "        if history is not None and len(history) > 0:\n",
        "            history_seq = torch.stack(history + [current_feat], dim=1)\n",
        "            history_proj = self.temporal_proj_in(history_seq)\n",
        "            temporal_out, temporal_entropy = self.temporal_band(history_proj)\n",
        "            temporal_feat = self.temporal_proj_out(temporal_out[:, -1, :])\n",
        "            \n",
        "            # Cross-band mixing\n",
        "            chunk_size = current_feat.shape[1] // self.cfg.num_spectral_bands\n",
        "            for i in range(self.cfg.num_spectral_bands):\n",
        "                delta = temporal_feat[:, i*chunk_size:(i+1)*chunk_size]\n",
        "                delta_spatial = delta.view(B, 2, 2, 2).repeat_interleave(self.cfg.height//2, dim=2).repeat_interleave(self.cfg.width//2, dim=3)\n",
        "                processed_bands[i] = processed_bands[i] + 0.1 * delta_spatial\n",
        "            band_entropies.append(temporal_entropy)\n",
        "        else:\n",
        "            band_entropies.append(torch.zeros(B, device=x.device))\n",
        "        \n",
        "        pred = self._fft_reconstruct(processed_bands)\n",
        "        belief = {\n",
        "            \"entropy_per_band\": torch.stack(band_entropies[:-1]),\n",
        "            \"temporal_entropy\": band_entropies[-1],\n",
        "            \"current_feat\": current_feat.detach(),\n",
        "        }\n",
        "        return pred, belief\n",
        "    \n",
        "    def get_lr_groups(self, base_lr: float) -> List[Dict]:\n",
        "        groups = []\n",
        "        for i, block in enumerate(self.band_blocks):\n",
        "            mult = self.cfg.band_lr_multipliers[i]\n",
        "            groups.append({\"params\": block.parameters(), \"lr\": base_lr * mult})\n",
        "        groups.append({\"params\": self.temporal_proj_in.parameters(), \"lr\": base_lr * self.cfg.band_lr_multipliers[-1]})\n",
        "        groups.append({\"params\": self.temporal_band.parameters(), \"lr\": base_lr * self.cfg.band_lr_multipliers[-1]})\n",
        "        groups.append({\"params\": self.temporal_proj_out.parameters(), \"lr\": base_lr * self.cfg.band_lr_multipliers[-1]})\n",
        "        return groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baselines (same as v1 for fair comparison)\n",
        "\n",
        "@dataclass\n",
        "class BaselineConfig:\n",
        "    height: int = 64\n",
        "    width: int = 64\n",
        "    channels: int = 32\n",
        "    device: str = \"cpu\"\n",
        "    dtype: torch.dtype = torch.float32\n",
        "\n",
        "\n",
        "class FlatBaseline(nn.Module):\n",
        "    \"\"\"Standard ConvNet without spectral decomposition.\"\"\"\n",
        "    def __init__(self, cfg: BaselineConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, cfg.channels, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(cfg.channels, cfg.channels * 2, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(cfg.channels * 2, cfg.channels * 2, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(cfg.channels * 2, cfg.channels, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(cfg.channels, 1, kernel_size=3, padding=1),\n",
        "        )\n",
        "        self.to(cfg.device)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, history: Optional[List[torch.Tensor]] = None\n",
        "                ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        pred = self.net(x)\n",
        "        # No residual connection (removed cheating)\n",
        "        return pred, {\"entropy_per_band\": torch.zeros(7, device=x.device)}\n",
        "\n",
        "\n",
        "class FourBandBaseline(nn.Module):\n",
        "    \"\"\"4-band spectral baseline.\"\"\"\n",
        "    def __init__(self, cfg: BaselineConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.num_bands = 4\n",
        "        self.register_buffer(\"masks\", self._make_masks(cfg.height, cfg.width))\n",
        "        self.band_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(2, cfg.channels, kernel_size=3, padding=1), nn.GELU(),\n",
        "                nn.Conv2d(cfg.channels, 2, kernel_size=1),\n",
        "            ) for _ in range(self.num_bands)\n",
        "        ])\n",
        "        self.to(cfg.device)\n",
        "    \n",
        "    def _make_masks(self, h: int, w: int) -> torch.Tensor:\n",
        "        yy, xx = torch.meshgrid(torch.linspace(-1, 1, h), torch.linspace(-1, 1, w), indexing=\"ij\")\n",
        "        rr = torch.sqrt(yy ** 2 + xx ** 2).clamp(min=1e-6)\n",
        "        edges = torch.logspace(-2, math.log10(math.sqrt(2)), steps=5)\n",
        "        edges[0] = 0\n",
        "        masks = [torch.sigmoid((rr - edges[i]) * 20) * torch.sigmoid((edges[i+1] - rr) * 20) for i in range(4)]\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "        return masks / (masks.sum(dim=0, keepdim=True) + 1e-8)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, history: Optional[List[torch.Tensor]] = None\n",
        "                ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        B = x.shape[0]\n",
        "        fft = torch.fft.fft2(x.squeeze(1))\n",
        "        fft_shifted = torch.fft.fftshift(fft)\n",
        "        processed_bands, band_entropies = [], []\n",
        "        for i in range(self.num_bands):\n",
        "            mask = self.masks[i].unsqueeze(0).to(x.device)\n",
        "            band_fft = fft_shifted * mask\n",
        "            band_feat = torch.stack([band_fft.real, band_fft.imag], dim=1)\n",
        "            proc = self.band_blocks[i](band_feat)\n",
        "            processed_bands.append(band_feat + proc)\n",
        "            band_entropies.append(torch.tensor(0.0, device=x.device))\n",
        "        fft_recon = torch.zeros_like(fft_shifted)\n",
        "        for i, band in enumerate(processed_bands):\n",
        "            mask = self.masks[i].unsqueeze(0).to(x.device)\n",
        "            fft_recon = fft_recon + torch.complex(band[:, 0], band[:, 1]) * mask\n",
        "        fft_unshifted = torch.fft.ifftshift(fft_recon)\n",
        "        spatial = torch.fft.ifft2(fft_unshifted).real\n",
        "        return spatial.unsqueeze(1), {\"entropy_per_band\": torch.stack(band_entropies)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_predictor(\n",
        "    model: nn.Module,\n",
        "    train_data: Tuple[torch.Tensor, torch.Tensor],\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    device: str,\n",
        "    use_differential_lr: bool = False,\n",
        "    predict_delta: bool = True,\n",
        "    batch_size: int = 32,\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"Train a predictor on next-frame prediction.\"\"\"\n",
        "    fields, next_fields = train_data\n",
        "    fields = fields.to(device)\n",
        "    next_fields = next_fields.to(device)\n",
        "    \n",
        "    if use_differential_lr and hasattr(model, 'get_lr_groups'):\n",
        "        optimizer = optim.Adam(model.get_lr_groups(lr))\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    metrics = {\"loss\": [], \"epoch_time\": []}\n",
        "    num_samples = fields.shape[0]\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_losses = []\n",
        "        history = []\n",
        "        perm = torch.randperm(num_samples)\n",
        "        \n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            idx = perm[i:i + batch_size]\n",
        "            x = fields[idx]\n",
        "            y = next_fields[idx]\n",
        "            \n",
        "            pred, belief = model(x, history if history else None)\n",
        "            \n",
        "            if predict_delta:\n",
        "                delta_target = y - x\n",
        "                delta_pred = pred - x\n",
        "                loss = F.mse_loss(delta_pred, delta_target)\n",
        "            else:\n",
        "                loss = F.mse_loss(pred, y)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_losses.append(loss.item())\n",
        "            \n",
        "            if \"current_feat\" in belief:\n",
        "                history.append(belief[\"current_feat\"])\n",
        "                if len(history) > 8:\n",
        "                    history = history[-8:]\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        metrics[\"loss\"].append(avg_loss)\n",
        "        metrics[\"epoch_time\"].append(epoch_time)\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}: loss={avg_loss:.6f}, time={epoch_time:.2f}s\")\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize environment\n",
        "print(\"[1] Initializing turbulent plasma environment...\")\n",
        "\n",
        "if DIFFICULTY == \"easy\":\n",
        "    plasma_cfg = PlasmaConfig.easy(device=DEVICE, size=GRID_SIZE)\n",
        "elif DIFFICULTY == \"hard\":\n",
        "    plasma_cfg = PlasmaConfig.hard(device=DEVICE, size=GRID_SIZE)\n",
        "elif DIFFICULTY == \"turbulent\":\n",
        "    plasma_cfg = PlasmaConfig.turbulent(device=DEVICE, size=GRID_SIZE)\n",
        "else:\n",
        "    plasma_cfg = PlasmaConfig.medium(device=DEVICE, size=GRID_SIZE)\n",
        "\n",
        "print(f\"    Diffusion: {plasma_cfg.diffusion}, Advection: {plasma_cfg.advection}\")\n",
        "print(f\"    Noise: {plasma_cfg.noise_std}, Disturbance: {plasma_cfg.disturbance_prob}@{plasma_cfg.disturbance_strength}\")\n",
        "print(f\"    Actuator sigma: {plasma_cfg.actuator_sigma:.1f} (scaled for {GRID_SIZE}x{GRID_SIZE})\")\n",
        "print(f\"    Vortices: {plasma_cfg.num_vortices}, strength: {plasma_cfg.vortex_strength}\")\n",
        "print(f\"    Shear: {plasma_cfg.shear_strength}, Multiscale noise: {plasma_cfg.multiscale_noise}\")\n",
        "\n",
        "env = TurbulentPlasmaEnv(plasma_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training data\n",
        "print(\"[2] Generating training data...\")\n",
        "fields, next_fields, controls = generate_trajectories(\n",
        "    env, NUM_TRAJECTORIES, TRAJECTORY_LENGTH,\n",
        "    control_scale=0.1,\n",
        "    prediction_horizon=PREDICTION_HORIZON\n",
        ")\n",
        "print(f\"    Generated {fields.shape[0]} samples (predicting t+{PREDICTION_HORIZON})\")\n",
        "\n",
        "train_data = (fields, next_fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "print(\"[3] Initializing models...\")\n",
        "\n",
        "spectral_cfg = SpectralConfig(\n",
        "    height=GRID_SIZE, width=GRID_SIZE, device=DEVICE,\n",
        "    top_k=TOP_K_TEMPORAL, temporal_decay=TEMPORAL_DECAY\n",
        ")\n",
        "sbm = SpectralBeliefMachineV2(spectral_cfg)\n",
        "print(f\"    SpectralBeliefMachine v2: {sum(p.numel() for p in sbm.parameters()):,} params\")\n",
        "print(f\"      Top-K: {spectral_cfg.top_k}, Decay: {spectral_cfg.temporal_decay}\")\n",
        "\n",
        "baseline_cfg = BaselineConfig(height=GRID_SIZE, width=GRID_SIZE, device=DEVICE)\n",
        "flat = FlatBaseline(baseline_cfg)\n",
        "four_band = FourBandBaseline(baseline_cfg)\n",
        "\n",
        "print(f\"    FlatBaseline: {sum(p.numel() for p in flat.parameters()):,} params\")\n",
        "print(f\"    FourBandBaseline: {sum(p.numel() for p in four_band.parameters()):,} params\")\n",
        "\n",
        "# Verify GPU usage\n",
        "print(f\"\\n[GPU CHECK]\")\n",
        "print(f\"    Target device: {DEVICE}\")\n",
        "print(f\"    SBM on: {next(sbm.parameters()).device}\")\n",
        "print(f\"    Flat on: {next(flat.parameters()).device}\")\n",
        "print(f\"    Training data on: {fields.device}\")\n",
        "if DEVICE == 'cuda' and torch.cuda.is_available():\n",
        "    print(f\"    GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SBM v2\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training SpectralBeliefMachine v2 (7+1 with Top-K)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sbm_metrics = train_predictor(\n",
        "    sbm, train_data, EPOCHS, lr=LR,\n",
        "    device=DEVICE, use_differential_lr=True,\n",
        "    predict_delta=PREDICT_DELTA, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train FlatBaseline\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training FlatBaseline...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "flat_metrics = train_predictor(\n",
        "    flat, train_data, EPOCHS, lr=LR,\n",
        "    device=DEVICE, use_differential_lr=False,\n",
        "    predict_delta=PREDICT_DELTA, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train FourBandBaseline\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training FourBandBaseline...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "four_band_metrics = train_predictor(\n",
        "    four_band, train_data, EPOCHS, lr=LR,\n",
        "    device=DEVICE, use_differential_lr=False,\n",
        "    predict_delta=PREDICT_DELTA, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax = axes[0]\n",
        "ax.semilogy(sbm_metrics['loss'], label=f'SBM v2 (7+1 Top-K) - final: {sbm_metrics[\"loss\"][-1]:.6f}')\n",
        "ax.semilogy(flat_metrics['loss'], label=f'FlatBaseline - final: {flat_metrics[\"loss\"][-1]:.6f}')\n",
        "ax.semilogy(four_band_metrics['loss'], label=f'FourBandBaseline - final: {four_band_metrics[\"loss\"][-1]:.6f}')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss (MSE)')\n",
        "ax.set_title('Training Loss Curves (Turbulent Environment)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Training speed\n",
        "ax = axes[1]\n",
        "avg_times = [\n",
        "    sum(sbm_metrics['epoch_time']) / len(sbm_metrics['epoch_time']),\n",
        "    sum(flat_metrics['epoch_time']) / len(flat_metrics['epoch_time']),\n",
        "    sum(four_band_metrics['epoch_time']) / len(four_band_metrics['epoch_time']),\n",
        "]\n",
        "bars = ax.bar(['SBM v2 (7+1)', 'FlatBaseline', 'FourBand'], avg_times)\n",
        "ax.set_ylabel('Seconds per Epoch')\n",
        "ax.set_title('Training Speed')\n",
        "for bar, t in zip(bars, avg_times):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.1f}s', \n",
        "            ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS (Turbulent Environment)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"SpectralBeliefMachine v2 (7+1 Top-K): {sbm_metrics['loss'][-1]:.6f}\")\n",
        "print(f\"FlatBaseline:                        {flat_metrics['loss'][-1]:.6f}\")\n",
        "print(f\"FourBandBaseline:                    {four_band_metrics['loss'][-1]:.6f}\")\n",
        "print(f\"\")\n",
        "print(f\"SBM improvement over epochs: {sbm_metrics['loss'][0]:.4f} -> {sbm_metrics['loss'][-1]:.4f} ({sbm_metrics['loss'][0]/sbm_metrics['loss'][-1]:.1f}x)\")\n",
        "print(f\"Flat improvement over epochs: {flat_metrics['loss'][0]:.4f} -> {flat_metrics['loss'][-1]:.4f} ({flat_metrics['loss'][0]/flat_metrics['loss'][-1]:.1f}x)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "sbm.eval()\n",
        "flat.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    idx = 500\n",
        "    x_sample = fields[idx:idx+1].to(DEVICE)\n",
        "    y_sample = next_fields[idx:idx+1].to(DEVICE)\n",
        "    \n",
        "    sbm_pred, _ = sbm(x_sample, None)\n",
        "    flat_pred, _ = flat(x_sample, None)\n",
        "    \n",
        "    sbm_error = (sbm_pred - y_sample).abs()\n",
        "    flat_error = (flat_pred - y_sample).abs()\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "axes[0, 0].imshow(x_sample[0, 0].cpu(), cmap='viridis')\n",
        "axes[0, 0].set_title('Input (t)')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(y_sample[0, 0].cpu(), cmap='viridis')\n",
        "axes[0, 1].set_title(f'Target (t+{PREDICTION_HORIZON})')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "axes[0, 2].imshow(sbm_pred[0, 0].cpu(), cmap='viridis')\n",
        "axes[0, 2].set_title('SBM v2 Prediction')\n",
        "axes[0, 2].axis('off')\n",
        "\n",
        "axes[0, 3].imshow(flat_pred[0, 0].cpu(), cmap='viridis')\n",
        "axes[0, 3].set_title('Flat Prediction')\n",
        "axes[0, 3].axis('off')\n",
        "\n",
        "axes[1, 0].axis('off')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "im1 = axes[1, 2].imshow(sbm_error[0, 0].cpu(), cmap='hot', vmin=0)\n",
        "axes[1, 2].set_title(f'SBM Error (MSE: {F.mse_loss(sbm_pred, y_sample).item():.4f})')\n",
        "axes[1, 2].axis('off')\n",
        "\n",
        "im2 = axes[1, 3].imshow(flat_error[0, 0].cpu(), cmap='hot', vmin=0)\n",
        "axes[1, 3].set_title(f'Flat Error (MSE: {F.mse_loss(flat_pred, y_sample).item():.4f})')\n",
        "axes[1, 3].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "standard",
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
