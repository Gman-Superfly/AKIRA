{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 040: Zipf-Spectral Mapping\n",
    "\n",
    "**Question:** Do token embeddings organize by Zipf rank in spectral space?\n",
    "\n",
    "**Hypothesis:** Common tokens have energy in low spectral bands; rare tokens in high bands.\n",
    "\n",
    "**Key Prediction:** Correlation r > 0.5 between log(Zipf rank) and spectral centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch scipy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "N_BANDS = 7  # Match AKIRA's spectral structure\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get embedding matrix\n",
    "embeddings = model.get_input_embeddings().weight.detach().cpu()\n",
    "vocab_size = embeddings.shape[0]\n",
    "embed_dim = embeddings.shape[1]\n",
    "\n",
    "print(f\"Loaded {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectral Decomposition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_decomposition(embedding: torch.Tensor, n_bands: int = N_BANDS) -> dict:\n",
    "    \"\"\"Decompose embedding into spectral bands via FFT.\"\"\"\n",
    "    emb_np = embedding.numpy()\n",
    "    \n",
    "    # FFT\n",
    "    fft_result = fft(emb_np)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    \n",
    "    # Split into bands\n",
    "    n_freq = len(magnitudes) // 2\n",
    "    band_size = n_freq // n_bands\n",
    "    \n",
    "    band_energies = {}\n",
    "    for band in range(n_bands):\n",
    "        start = band * band_size\n",
    "        end = (band + 1) * band_size if band < n_bands - 1 else n_freq\n",
    "        band_energies[band] = float(np.sum(magnitudes[start:end] ** 2))\n",
    "    \n",
    "    # Normalize\n",
    "    total = sum(band_energies.values())\n",
    "    if total > 0:\n",
    "        band_energies = {k: v/total for k, v in band_energies.items()}\n",
    "    \n",
    "    return band_energies\n",
    "\n",
    "def spectral_centroid(band_energies: dict) -> float:\n",
    "    \"\"\"Compute energy-weighted centroid.\"\"\"\n",
    "    total = sum(band_energies.values())\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    return sum(band * energy for band, energy in band_energies.items()) / total\n",
    "\n",
    "# Test on first token\n",
    "test_bands = spectral_decomposition(embeddings[0])\n",
    "test_centroid = spectral_centroid(test_bands)\n",
    "print(f\"Test token bands: {test_bands}\")\n",
    "print(f\"Test centroid: {test_centroid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sample of tokens (stratified by rank)\n",
    "N_SAMPLES = 2000\n",
    "\n",
    "# Stratified sampling: equal samples from each rank decile\n",
    "n_per_bucket = N_SAMPLES // 10\n",
    "bucket_size = vocab_size // 10\n",
    "\n",
    "sample_ids = []\n",
    "for i in range(10):\n",
    "    start = i * bucket_size\n",
    "    end = min((i + 1) * bucket_size, vocab_size)\n",
    "    sampled = np.random.choice(range(start, end), min(n_per_bucket, end-start), replace=False)\n",
    "    sample_ids.extend(sampled)\n",
    "\n",
    "print(f\"Analyzing {len(sample_ids)} tokens (stratified sample)\")\n",
    "\n",
    "# Analyze each token\n",
    "results = []\n",
    "for token_id in sample_ids:\n",
    "    emb = embeddings[token_id]\n",
    "    bands = spectral_decomposition(emb)\n",
    "    centroid = spectral_centroid(bands)\n",
    "    \n",
    "    # Token ID as proxy for Zipf rank (lower ID = more common)\n",
    "    zipf_rank = token_id + 1\n",
    "    log_rank = np.log10(zipf_rank)\n",
    "    \n",
    "    try:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "    except:\n",
    "        token_str = f\"[{token_id}]\"\n",
    "    \n",
    "    results.append({\n",
    "        'token_id': token_id,\n",
    "        'token_str': token_str,\n",
    "        'zipf_rank': zipf_rank,\n",
    "        'log_rank': log_rank,\n",
    "        'bands': bands,\n",
    "        'centroid': centroid\n",
    "    })\n",
    "\n",
    "print(f\"Analyzed {len(results)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "log_ranks = [r['log_rank'] for r in results]\n",
    "centroids = [r['centroid'] for r in results]\n",
    "\n",
    "# Overall correlation\n",
    "spearman_r, spearman_p = spearmanr(log_ranks, centroids)\n",
    "pearson_r, pearson_p = pearsonr(log_ranks, centroids)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"OVERALL CORRELATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Spearman r = {spearman_r:.4f} (p = {spearman_p:.2e})\")\n",
    "print(f\"Pearson r = {pearson_r:.4f} (p = {pearson_p:.2e})\")\n",
    "\n",
    "# Band-specific correlations\n",
    "print(\"\\nBAND-SPECIFIC CORRELATIONS:\")\n",
    "band_correlations = {}\n",
    "for band in range(N_BANDS):\n",
    "    band_energies = [r['bands'][band] for r in results]\n",
    "    corr, p = spearmanr(log_ranks, band_energies)\n",
    "    band_correlations[band] = corr\n",
    "    direction = \"+\" if corr > 0 else \"-\"\n",
    "    print(f\"  Band {band}: r = {corr:+.4f} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Extreme Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by rank\n",
    "sorted_results = sorted(results, key=lambda x: x['zipf_rank'])\n",
    "\n",
    "# Most common (low rank)\n",
    "common_100 = sorted_results[:100]\n",
    "common_centroid = np.mean([r['centroid'] for r in common_100])\n",
    "\n",
    "# Rarest (high rank)\n",
    "rare_100 = sorted_results[-100:]\n",
    "rare_centroid = np.mean([r['centroid'] for r in rare_100])\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"EXTREME TOKEN ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n100 Most Common tokens:\")\n",
    "print(f\"  Mean centroid: {common_centroid:.4f}\")\n",
    "print(f\"  Examples: {[r['token_str'][:10] for r in common_100[:5]]}\")\n",
    "\n",
    "print(f\"\\n100 Rarest tokens:\")\n",
    "print(f\"  Mean centroid: {rare_centroid:.4f}\")\n",
    "print(f\"  Examples: {[r['token_str'][:10] for r in rare_100[:5]]}\")\n",
    "\n",
    "print(f\"\\nCentroid Separation: {rare_centroid - common_centroid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Scatter - Log Rank vs Centroid\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(log_ranks, centroids, alpha=0.3, s=10)\n",
    "# Trend line\n",
    "z = np.polyfit(log_ranks, centroids, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(min(log_ranks), max(log_ranks), 100)\n",
    "ax1.plot(x_line, p(x_line), 'r-', linewidth=2, label=f'Trend (r={spearman_r:.3f})')\n",
    "ax1.set_xlabel('Log10(Zipf Rank)', fontsize=12)\n",
    "ax1.set_ylabel('Spectral Centroid', fontsize=12)\n",
    "ax1.set_title('Zipf Rank vs Spectral Centroid', fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Band Correlations\n",
    "ax2 = axes[0, 1]\n",
    "bands = list(band_correlations.keys())\n",
    "corrs = list(band_correlations.values())\n",
    "colors = ['red' if c < 0 else 'green' for c in corrs]\n",
    "ax2.bar(bands, corrs, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Spectral Band', fontsize=12)\n",
    "ax2.set_ylabel('Correlation with Log(Rank)', fontsize=12)\n",
    "ax2.set_title('Band-Specific Correlations', fontsize=14)\n",
    "\n",
    "# Plot 3: Heatmap by Rank Bucket\n",
    "ax3 = axes[1, 0]\n",
    "n_buckets = 10\n",
    "bucket_size = len(sorted_results) // n_buckets\n",
    "\n",
    "heatmap_data = []\n",
    "for i in range(n_buckets):\n",
    "    start = i * bucket_size\n",
    "    end = (i+1) * bucket_size if i < n_buckets - 1 else len(sorted_results)\n",
    "    bucket = sorted_results[start:end]\n",
    "    band_means = [np.mean([r['bands'][b] for r in bucket]) for b in range(N_BANDS)]\n",
    "    heatmap_data.append(band_means)\n",
    "\n",
    "im = ax3.imshow(heatmap_data, aspect='auto', cmap='viridis')\n",
    "ax3.set_xlabel('Spectral Band', fontsize=12)\n",
    "ax3.set_ylabel('Rank Bucket (Common -> Rare)', fontsize=12)\n",
    "ax3.set_title('Band Energy by Zipf Bucket', fontsize=14)\n",
    "ax3.set_xticks(range(N_BANDS))\n",
    "ax3.set_yticks(range(n_buckets))\n",
    "ax3.set_yticklabels([f'{i+1}' for i in range(n_buckets)])\n",
    "plt.colorbar(im, ax=ax3, label='Energy')\n",
    "\n",
    "# Plot 4: Centroid Distribution by Bucket\n",
    "ax4 = axes[1, 1]\n",
    "bucket_centroids = []\n",
    "for i in range(n_buckets):\n",
    "    start = i * bucket_size\n",
    "    end = (i+1) * bucket_size if i < n_buckets - 1 else len(sorted_results)\n",
    "    bucket = sorted_results[start:end]\n",
    "    bucket_centroids.append([r['centroid'] for r in bucket])\n",
    "\n",
    "ax4.boxplot(bucket_centroids, labels=[f'{i+1}' for i in range(n_buckets)])\n",
    "ax4.set_xlabel('Rank Bucket (Common -> Rare)', fontsize=12)\n",
    "ax4.set_ylabel('Spectral Centroid', fontsize=12)\n",
    "ax4.set_title('Centroid Distribution by Bucket', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 040 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CORRELATION RESULTS:\")\n",
    "print(f\"   Spearman r = {spearman_r:.4f}\")\n",
    "print(f\"   p-value = {spearman_p:.2e}\")\n",
    "\n",
    "print(\"\\n2. BAND STRUCTURE:\")\n",
    "for band, corr in band_correlations.items():\n",
    "    if corr < -0.1:\n",
    "        print(f\"   Band {band}: NEGATIVE (common tokens have more energy here)\")\n",
    "    elif corr > 0.1:\n",
    "        print(f\"   Band {band}: POSITIVE (rare tokens have more energy here)\")\n",
    "    else:\n",
    "        print(f\"   Band {band}: NEUTRAL\")\n",
    "\n",
    "print(\"\\n3. CENTROID SEPARATION:\")\n",
    "centroid_sep = rare_centroid - common_centroid\n",
    "print(f\"   Common tokens: {common_centroid:.4f}\")\n",
    "print(f\"   Rare tokens: {rare_centroid:.4f}\")\n",
    "print(f\"   Separation: {centroid_sep:.4f}\")\n",
    "\n",
    "print(\"\\n4. VERDICT:\")\n",
    "if spearman_r > 0.3 and spearman_p < 0.001 and centroid_sep > 0.3:\n",
    "    print(\"   HYPOTHESIS SUPPORTED\")\n",
    "    print(\"   - Significant positive correlation\")\n",
    "    print(\"   - Common tokens -> Low bands (DC-like)\")\n",
    "    print(\"   - Rare tokens -> High bands (detail)\")\n",
    "    print(\"   - Zipf's Law maps to spectral structure\")\n",
    "else:\n",
    "    print(\"   HYPOTHESIS NOT SUPPORTED\")\n",
    "    if spearman_r <= 0.3:\n",
    "        print(f\"   - Correlation too weak: r = {spearman_r:.3f}\")\n",
    "    if centroid_sep <= 0.3:\n",
    "        print(f\"   - Separation too small: {centroid_sep:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
