{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zipf-aware wave tokenization (word-level) quick viz\n",
        "\n",
        "This notebook demo assigns per-token wave frequencies from word Zipf ranks (with per-query mixing), builds simple wave embeddings, and visualizes frequency bands and a toy attention superposition.\n",
        "\n",
        "Steps:\n",
        "1. Word-aware frequency assignment per query\n",
        "2. Wave embedding preview (harmonic sin/cos)\n",
        "3. Band/bucket view\n",
        "4. Toy attention heatmap + entropy\n",
        "\n",
        "Outputs are also saved as PNG under `figs/` for quick inspection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Self-contained word-aware frequency assigner (no external import)\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "@dataclass\n",
        "class WordFreqConfig:\n",
        "    vocab_size_words: int\n",
        "    freq_min: float = 0.0\n",
        "    freq_max: float = 1.0\n",
        "    mix_weight: float = 0.7  # weight on global rank vs local rank\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        assert self.vocab_size_words > 0, \"vocab_size_words must be positive\"\n",
        "        assert 0.0 <= self.freq_min < self.freq_max <= 1.0, \"freq_min/freq_max must satisfy 0 <= min < max <= 1\"\n",
        "        assert 0.0 <= self.mix_weight <= 1.0, \"mix_weight must be in [0,1]\"\n",
        "        return True\n",
        "\n",
        "class WordFrequencyAssigner:\n",
        "    def __init__(self, tokenizer, word_rank_table: Dict[str, int], config: WordFreqConfig) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word_rank_table = word_rank_table\n",
        "        self.config = config\n",
        "        self.config.validate()\n",
        "\n",
        "    def _word_ranks(self, words: List[str]) -> List[int]:\n",
        "        default_rank = self.config.vocab_size_words  # rarest fallback\n",
        "        return [self.word_rank_table.get(w, default_rank) for w in words]\n",
        "\n",
        "    def _local_ranks(self, words: List[str]) -> List[int]:\n",
        "        counts = Counter(words)\n",
        "        sorted_words = sorted(counts.items(), key=lambda x: (-x[1], x[0]))\n",
        "        local_rank_map = {w: i for i, (w, _) in enumerate(sorted_words, start=1)}\n",
        "        return [local_rank_map[w] for w in words]\n",
        "\n",
        "    def _mix_ranks(self, global_rank: int, local_rank: int) -> float:\n",
        "        a = self.config.mix_weight\n",
        "        return (global_rank ** a) * (local_rank ** (1.0 - a))\n",
        "\n",
        "    def _rank_to_freq(self, rank: torch.Tensor) -> torch.Tensor:\n",
        "        log_v = torch.log(torch.tensor(float(self.config.vocab_size_words)))\n",
        "        norm = torch.log(rank) / log_v\n",
        "        return self.config.freq_min + (self.config.freq_max - self.config.freq_min) * norm\n",
        "\n",
        "    def assign(self, text: str):\n",
        "        encoded = self.tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "        token_ids = torch.tensor(encoded[\"input_ids\"], dtype=torch.long)\n",
        "        offsets = encoded[\"offset_mapping\"]\n",
        "        words: List[str] = []\n",
        "        token_to_word: List[int] = []\n",
        "        prev_end = -1\n",
        "        word_idx = -1\n",
        "        for (start, end) in offsets:\n",
        "            if start == prev_end:\n",
        "                token_to_word.append(word_idx)\n",
        "            else:\n",
        "                word_idx += 1\n",
        "                token_to_word.append(word_idx)\n",
        "                words.append(text[start:end])\n",
        "            prev_end = end\n",
        "        global_ranks = self._word_ranks(words)\n",
        "        local_ranks = self._local_ranks(words)\n",
        "        eff_ranks = [self._mix_ranks(g, l) for g, l in zip(global_ranks, local_ranks)]\n",
        "        rank_tensor = torch.tensor([eff_ranks[idx] for idx in token_to_word], dtype=torch.float32)\n",
        "        freqs = self._rank_to_freq(rank_tensor)\n",
        "        return token_ids, freqs\n",
        "\n",
        "# Figure directory\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "FIG_DIR = NOTEBOOK_DIR / \"figs\"\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Lightweight whitespace tokenizer with offsets compatible with WordFrequencyAssigner expectations\n",
        "class WhiteSpaceTokenizer:\n",
        "    def encode_plus(self, text: str, return_offsets_mapping=True, add_special_tokens=False):\n",
        "        words = text.split()\n",
        "        offsets = []\n",
        "        input_ids = []\n",
        "        pos = 0\n",
        "        for w in words:\n",
        "            start = text.find(w, pos)\n",
        "            end = start + len(w)\n",
        "            offsets.append((start, end))\n",
        "            input_ids.append(hash(w) % 10000)  # fake token id\n",
        "            pos = end\n",
        "        return {\"input_ids\": input_ids, \"offset_mapping\": offsets}\n",
        "\n",
        "# Example text and word rank table (mock Zipf ranks; 1 = most common)\n",
        "text = (\n",
        "    \"On a windy hill a small observatory kept vigil each night, listening to whispers \"\n",
        "    \"of meteors and distant storms. Mira adjusted brass gears, wound the ancient clock, \"\n",
        "    \"and brewed coffee while charts fluttered like birds. A traveler arrived with a \"\n",
        "    \"brittle map, promising a hidden tower where books hummed and time bent. Mira packed \"\n",
        "    \"tools, lantern, and a curious heart, crossed stone bridges at dawn, found the ivy \"\n",
        "    \"door ajar, and heard pages rustle like thunder.\"\n",
        ")\n",
        "\n",
        "# Mock global word ranks (short list; unseen words default to rare)\n",
        "word_ranks: Dict[str, int] = {\n",
        "    \"on\": 20,\n",
        "    \"a\": 10,\n",
        "    \"windy\": 2000,\n",
        "    \"hill\": 1800,\n",
        "    \"small\": 800,\n",
        "    \"observatory\": 4000,\n",
        "    \"kept\": 700,\n",
        "    \"vigil\": 3000,\n",
        "    \"each\": 200,\n",
        "    \"night\": 300,\n",
        "    \"listening\": 900,\n",
        "    \"to\": 15,\n",
        "    \"whispers\": 1600,\n",
        "    \"of\": 12,\n",
        "    \"meteors\": 4200,\n",
        "    \"and\": 5,\n",
        "    \"distant\": 2500,\n",
        "    \"storms\": 4200,\n",
        "    \"mira\": 7000,\n",
        "    \"adjusted\": 2600,\n",
        "    \"brass\": 3000,\n",
        "    \"gears\": 2600,\n",
        "    \"wound\": 4800,\n",
        "    \"the\": 1,\n",
        "    \"ancient\": 2000,\n",
        "    \"clock\": 1400,\n",
        "    \"brewed\": 2300,\n",
        "    \"coffee\": 1900,\n",
        "    \"while\": 150,\n",
        "    \"charts\": 2600,\n",
        "    \"fluttered\": 3600,\n",
        "    \"like\": 90,\n",
        "    \"birds\": 1200,\n",
        "    \"traveler\": 3300,\n",
        "    \"arrived\": 600,\n",
        "    \"with\": 30,\n",
        "    \"brittle\": 5200,\n",
        "    \"map\": 1750,\n",
        "    \"promising\": 2600,\n",
        "    \"hidden\": 2300,\n",
        "    \"tower\": 1500,\n",
        "    \"where\": 70,\n",
        "    \"books\": 900,\n",
        "    \"hummed\": 5205,\n",
        "    \"time\": 600,\n",
        "    \"bent\": 2600,\n",
        "    \"packed\": 2100,\n",
        "    \"tools\": 1900,\n",
        "    \"lantern\": 4000,\n",
        "    \"curious\": 1900,\n",
        "    \"heart\": 1600,\n",
        "    \"crossed\": 1900,\n",
        "    \"stone\": 1700,\n",
        "    \"bridges\": 2600,\n",
        "    \"at\": 40,\n",
        "    \"dawn\": 2000,\n",
        "    \"found\": 900,\n",
        "    \"ivy\": 5209,\n",
        "    \"door\": 1800,\n",
        "    \"ajar\": 5210,\n",
        "    \"heard\": 800,\n",
        "    \"pages\": 2000,\n",
        "    \"rustle\": 4200,\n",
        "    \"thunder\": 2500,\n",
        "}\n",
        "\n",
        "config = WordFreqConfig(vocab_size_words=60000, freq_min=0.0, freq_max=1.0, mix_weight=0.7)\n",
        "tokenizer = WhiteSpaceTokenizer()\n",
        "assigner = WordFrequencyAssigner(tokenizer, word_ranks, config)\n",
        "\n",
        "token_ids, freqs = assigner.assign(text)\n",
        "positions = torch.arange(len(token_ids), dtype=torch.float32)\n",
        "\n",
        "print(\"Tokens:\", token_ids.tolist())\n",
        "print(\"Freqs:\", freqs.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Band assignment and frequency plot\n",
        "words = text.split()\n",
        "num_bands = 7\n",
        "band_edges = torch.linspace(config.freq_min, config.freq_max, num_bands + 1)\n",
        "band_ids = torch.bucketize(freqs, band_edges) - 1  # 0-based\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.bar(range(len(freqs)), freqs.numpy(), color=\"steelblue\")\n",
        "plt.xticks(range(len(freqs)), words, rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Frequency (normalized)\")\n",
        "plt.title(\"Per-token frequencies (word-aware Zipf)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"freqs.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 2.5))\n",
        "plt.bar(range(len(band_ids)), band_ids.numpy(), color=\"darkorange\")\n",
        "plt.xticks(range(len(freqs)), words, rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Band index (0-6)\")\n",
        "plt.title(\"Band assignment\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"bands.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Histogram of frequencies and bands\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.hist(freqs.numpy(), bins=20, color=\"slateblue\", alpha=0.8)\n",
        "plt.xlabel(\"Frequency (normalized)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Frequency histogram\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"freq_hist.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 2.5))\n",
        "plt.hist(band_ids.numpy(), bins=num_bands, range=(-0.5, num_bands - 0.5), color=\"darkcyan\", alpha=0.8)\n",
        "plt.xlabel(\"Band index\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Band histogram\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"band_hist.png\", dpi=150)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wave embedding preview (simple sin/cos, 7 harmonics)\n",
        "harmonics = torch.arange(1, 8, dtype=torch.float32)\n",
        "\n",
        "# Plot range\n",
        "t_local = torch.linspace(0, 1, 400)\n",
        "normalize_waves = True\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "for i in range(min(7, len(freqs))):\n",
        "    f = freqs[i]\n",
        "    p = positions[i]\n",
        "    wave = torch.zeros_like(t_local)\n",
        "    for h in harmonics:\n",
        "        wave = wave + torch.sin(2 * math.pi * f * h * (p + t_local))\n",
        "    if normalize_waves:\n",
        "        wave = wave / (wave.abs().max() + 1e-6)\n",
        "    plt.plot(t_local.numpy(), wave.numpy(), label=f\"{words[i]} (band {band_ids[i].item()})\")\n",
        "plt.xlabel(\"Local time\")\n",
        "plt.ylabel(\"Wave amplitude (sum of 7 harmonics)\")\n",
        "plt.title(\"Token waves (first 7 tokens)\")\n",
        "plt.legend(ncol=2)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"waves.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Optional: per-harmonic view for the first token\n",
        "if len(freqs) > 0:\n",
        "    f0 = freqs[0]\n",
        "    p0 = positions[0]\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    for h in harmonics:\n",
        "        wave_h = torch.sin(2 * math.pi * f0 * h * (p0 + t_local))\n",
        "        plt.plot(t_local.numpy(), wave_h.numpy(), label=f\"h{int(h.item())}\")\n",
        "    plt.xlabel(\"Local time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(f\"Individual harmonics for token 0 ({words[0]})\")\n",
        "    plt.legend(ncol=4, fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / \"waves_harmonics_token0.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toy attention superposition and entropy\n",
        "import json\n",
        "\n",
        "seq_len = len(freqs)\n",
        "logits = torch.randn(seq_len, seq_len)\n",
        "attn = torch.softmax(logits, dim=-1)\n",
        "\n",
        "entropy = -(attn * (attn.clamp(min=1e-9).log())).sum(dim=-1)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(attn.numpy(), cmap=\"viridis\", aspect=\"auto\")\n",
        "plt.colorbar(label=\"Attention weight\")\n",
        "plt.xticks(range(seq_len), words, rotation=90)\n",
        "plt.yticks(range(seq_len), words)\n",
        "plt.title(\"Toy attention heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"attention_heatmap.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 2.5))\n",
        "plt.bar(range(seq_len), entropy.numpy(), color=\"seagreen\")\n",
        "plt.xticks(range(seq_len), words, rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Entropy (nats)\")\n",
        "plt.title(\"Per-token attention entropy (leaders vs certainty)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"attention_entropy.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Top-7 attended tokens per query position (arg top-k over rows)\n",
        "top_k = 7\n",
        "top_vals, top_idx = torch.topk(attn, k=min(top_k, seq_len), dim=-1)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(min(7, seq_len)):\n",
        "    plt.plot(range(top_vals.size(1)), top_vals[i].numpy(), marker=\"o\", label=f\"query {i}: {words[i]}\")\n",
        "plt.xlabel(\"Top-k rank (0=highest)\")\n",
        "plt.ylabel(\"Attention weight\")\n",
        "plt.title(\"Top-7 attention weights per query token\")\n",
        "plt.legend(ncol=2, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"attention_topk_weights.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Save key results to JSON\n",
        "results = {\n",
        "    \"text\": text,\n",
        "    \"token_ids\": token_ids.tolist(),\n",
        "    \"freqs\": freqs.tolist(),\n",
        "    \"band_ids\": band_ids.tolist(),\n",
        "    \"attention_entropy\": entropy.tolist(),\n",
        "    \"attention_top_vals\": top_vals.tolist(),\n",
        "    \"attention_top_idx\": top_idx.tolist(),\n",
        "}\n",
        "with open(FIG_DIR / \"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(\"Saved results to\", FIG_DIR / \"results.json\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase coherence through the attention lens\n",
        "# Use harmonic 1 phase at t=0: phi = 2Ï€ f * position\n",
        "harmonic = 1.0\n",
        "phi_all = 2 * math.pi * freqs * positions  # [T]\n",
        "\n",
        "# Compute coherence R per query over its top-k attended tokens\n",
        "R_per_query = []\n",
        "for i in range(seq_len):\n",
        "    idx = top_idx[i]  # top-k indices for query i\n",
        "    phi_top = phi_all[idx]\n",
        "    R = torch.abs(torch.mean(torch.exp(1j * phi_top)))\n",
        "    R_per_query.append(R.item())\n",
        "R_per_query = torch.tensor(R_per_query)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.bar(range(seq_len), R_per_query.numpy(), color=\"mediumpurple\")\n",
        "plt.xticks(range(seq_len), words, rotation=90)\n",
        "plt.ylabel(\"Phase coherence R (0-1)\")\n",
        "plt.title(\"Top-k attention phase coherence per query (harmonic 1)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"attention_phase_coherence.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Phase vs attention weight for a selected query (0) to see alignment of leaders\n",
        "q = 0\n",
        "phi_top_q = ((phi_all[top_idx[q]] + math.pi) % (2 * math.pi)) - math.pi\n",
        "w_top_q = top_vals[q]\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.scatter(phi_top_q.numpy(), w_top_q.numpy(), color=\"teal\")\n",
        "plt.xlabel(\"Phase (rad, centered)\")\n",
        "plt.ylabel(\"Attention weight\")\n",
        "plt.title(f\"Query {q} ({words[q]}): phase vs attention weight\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"attention_phase_scatter_q0.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Log into results\n",
        "results[\"attention_phase_coherence\"] = R_per_query.tolist()\n",
        "with open(FIG_DIR / \"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(\"Updated results with phase coherence ->\", FIG_DIR / \"results.json\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Head carrier phase traces (synthetic) to observe alignment\n",
        "# We attach simple carrier frequencies to heads and let attention drive phase updates.\n",
        "\n",
        "n_heads = 4\n",
        "freq_head = torch.linspace(0.05, 0.2, n_heads)  # arbitrary head carrier freqs\n",
        "phi_heads = torch.zeros(n_heads)\n",
        "R_heads = []\n",
        "phi_trace = []\n",
        "\n",
        "for t in range(seq_len):\n",
        "    # weighted complex sum of token phases (use harmonic 1 phase phi_all)\n",
        "    w = attn[t]  # [seq]\n",
        "    csum = torch.sum(w * torch.exp(1j * phi_all))\n",
        "    # update head phases: advance carrier + align to current weighted phase\n",
        "    phi_heads = phi_heads + 2 * math.pi * freq_head + torch.angle(csum)\n",
        "    phi_trace.append(phi_heads.clone())\n",
        "    R_heads.append(torch.abs(torch.mean(torch.exp(1j * phi_heads))).item())\n",
        "\n",
        "phi_trace = torch.stack(phi_trace)  # [T, n_heads]\n",
        "R_heads = torch.tensor(R_heads)\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for h in range(n_heads):\n",
        "    plt.plot(phi_trace[:, h].numpy(), label=f\"head {h}\")\n",
        "plt.xlabel(\"Step (query index)\")\n",
        "plt.ylabel(\"Phase (rad, unwrapped)\")\n",
        "plt.title(\"Head carrier phases over steps\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"head_phase_traces.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(R_heads.numpy(), marker=\"o\", color=\"firebrick\")\n",
        "plt.xlabel(\"Step (query index)\")\n",
        "plt.ylabel(\"Head coherence R (0-1)\")\n",
        "plt.title(\"Head phase coherence over steps\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"head_coherence.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Log head coherence\n",
        "results[\"head_phase_coherence\"] = R_heads.tolist()\n",
        "with open(FIG_DIR / \"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(\"Updated results with head phase coherence ->\", FIG_DIR / \"results.json\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
