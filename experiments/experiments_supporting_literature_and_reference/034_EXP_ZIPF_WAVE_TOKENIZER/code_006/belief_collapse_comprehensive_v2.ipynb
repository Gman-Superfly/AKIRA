{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Belief Collapse v2 (Long Prompts)\n",
        "\n",
        "This version stresses the model with **long, rich prompts** (stories, recipes, jokes, code) to see collapse dynamics on realistic sequences. We keep the valid metrics (entropy, logit lens, head alignment, convergence) and add top-k token readouts to avoid single-token myopia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "FIG_DIR = NOTEBOOK_DIR / \"figs_collapse_v2\"\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2\n",
        "model_name = \"gpt2\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.n_layer\n",
        "n_heads = model.config.n_head\n",
        "d_model = model.config.n_embd\n",
        "d_head = d_model // n_heads\n",
        "print(f\"Model: {n_layers} layers, {n_heads} heads, d_model={d_model}, d_head={d_head}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helper\n",
        "def run_inference(text: str) -> Dict:\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "    tokens = [tokenizer.decode([tid]) for tid in input_ids[0]]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_attentions=True, output_hidden_states=True)\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"logits\": outputs.logits.cpu(),\n",
        "        \"attentions\": [a.cpu() for a in outputs.attentions],\n",
        "        \"hidden_states\": [h.cpu() for h in outputs.hidden_states],\n",
        "        \"n_layers\": len(outputs.attentions),\n",
        "        \"n_heads\": outputs.attentions[0].size(1),\n",
        "        \"seq_len\": input_ids.size(1),\n",
        "    }\n",
        "\n",
        "print(\"run_inference() ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics\n",
        "\n",
        "def compute_attention_entropy(attn: torch.Tensor) -> float:\n",
        "    p = attn.clamp(min=1e-10)\n",
        "    entropy = -(p * p.log()).sum(dim=-1)\n",
        "    return entropy.mean().item()\n",
        "\n",
        "def attention_entropy_per_layer(result: Dict, query_pos: int = -1) -> List[float]:\n",
        "    ent = []\n",
        "    for attn in result[\"attentions\"]:\n",
        "        q_attn = attn[0, :, query_pos, :]\n",
        "        ent.append(compute_attention_entropy(q_attn))\n",
        "    return ent\n",
        "\n",
        "def logit_lens(result: Dict, pos: int = -1, top_k: int = 5) -> Tuple[List[float], List[List[str]]]:\n",
        "    lm_head = model.lm_head\n",
        "    ln_f = model.transformer.ln_f\n",
        "    entropies = []\n",
        "    tops = []\n",
        "    for hidden in result[\"hidden_states\"]:\n",
        "        h = hidden[0, pos, :]\n",
        "        h_norm = ln_f(h.to(device))\n",
        "        logits = lm_head(h_norm)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        p = probs.clamp(min=1e-10)\n",
        "        entropies.append((-(p * p.log()).sum()).item())\n",
        "        top_ids = torch.topk(probs, k=top_k).indices.tolist()\n",
        "        tops.append([tokenizer.decode([tid]) for tid in top_ids])\n",
        "    return entropies, tops\n",
        "\n",
        "def head_context_vectors(result: Dict, layer_idx: int, query_pos: int = -1) -> np.ndarray:\n",
        "    attn = result[\"attentions\"][layer_idx][0].numpy()\n",
        "    hidden_in = result[\"hidden_states\"][layer_idx][0].numpy()\n",
        "    ctxs = []\n",
        "    for h in range(attn.shape[0]):\n",
        "        w = attn[h, query_pos, :]\n",
        "        ctxs.append(w @ hidden_in)\n",
        "    return np.stack(ctxs, axis=0)\n",
        "\n",
        "def head_alignment(result: Dict, layer_idx: int, query_pos: int = -1) -> float:\n",
        "    ctxs = head_context_vectors(result, layer_idx, query_pos)\n",
        "    norms = np.linalg.norm(ctxs, axis=1, keepdims=True) + 1e-10\n",
        "    ctxs = ctxs / norms\n",
        "    sim = ctxs @ ctxs.T\n",
        "    upper = sim[np.triu_indices(sim.shape[0], k=1)]\n",
        "    return float(upper.mean())\n",
        "\n",
        "def head_alignment_per_layer(result: Dict, query_pos: int = -1) -> List[float]:\n",
        "    return [head_alignment(result, l, query_pos) for l in range(result[\"n_layers\"])]\n",
        "\n",
        "def activation_change_across_positions(result: Dict, layer_idx: int) -> np.ndarray:\n",
        "    hidden = result[\"hidden_states\"][layer_idx][0].numpy()\n",
        "    if hidden.shape[0] < 2:\n",
        "        return np.array([])\n",
        "    norms = np.linalg.norm(hidden, axis=1, keepdims=True) + 1e-10\n",
        "    hnorm = hidden / norms\n",
        "    diffs = []\n",
        "    for i in range(hidden.shape[0] - 1):\n",
        "        sim = np.dot(hnorm[i], hnorm[i + 1])\n",
        "        diffs.append(1 - sim)\n",
        "    return np.array(diffs)\n",
        "\n",
        "def mean_position_change_per_layer(result: Dict) -> List[float]:\n",
        "    out = []\n",
        "    for l in range(len(result[\"hidden_states\"])):\n",
        "        c = activation_change_across_positions(result, l)\n",
        "        out.append(float(c.mean()) if len(c) else 0.0)\n",
        "    return out\n",
        "\n",
        "def layer_to_layer_similarity(result: Dict, pos: int = -1) -> List[float]:\n",
        "    hs = result[\"hidden_states\"]\n",
        "    sims = []\n",
        "    for i in range(len(hs) - 1):\n",
        "        h1 = hs[i][0, pos, :].numpy()\n",
        "        h2 = hs[i + 1][0, pos, :].numpy()\n",
        "        n1 = np.linalg.norm(h1) + 1e-10\n",
        "        n2 = np.linalg.norm(h2) + 1e-10\n",
        "        sims.append(float(np.dot(h1, h2) / (n1 * n2)))\n",
        "    return sims\n",
        "\n",
        "def layer_similarity_to_final(result: Dict, pos: int = -1) -> List[float]:\n",
        "    hs = result[\"hidden_states\"]\n",
        "    final = hs[-1][0, pos, :].numpy()\n",
        "    n_final = np.linalg.norm(final) + 1e-10\n",
        "    sims = []\n",
        "    for h in hs:\n",
        "        v = h[0, pos, :].numpy()\n",
        "        n_v = np.linalg.norm(v) + 1e-10\n",
        "        sims.append(float(np.dot(v, final) / (n_v * n_final)))\n",
        "    return sims\n",
        "\n",
        "print(\"Metric functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Long, content-rich prompts\n",
        "- Story\n",
        "- Recipe\n",
        "- Joke\n",
        "- Technical code\n",
        "- Legal-ish clause\n",
        "- News/analysis\n",
        "- Speculative scenario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = {\n",
        "    \"story\": (\n",
        "        \"On the fourth night of the winter storm, the lighthouse keeper \"\n",
        "        \"noticed the beam flicker. He grabbed his tools, descended the narrow \"\n",
        "        \"stairs, and discovered a loose wire sparking near the fuel line. With \"\n",
        "        \"waves smashing the rocks and a cargo ship approaching, he had one \"\n",
        "        \"chance to repair the circuit before the coast went dark.\"\n",
        "    ),\n",
        "    \"recipe\": (\n",
        "        \"To make a crusty sourdough loaf, feed your starter the night before, \"\n",
        "        \"then mix 500g bread flour, 350g water, 100g active starter, and 10g salt. \"\n",
        "        \"Rest 30 minutes, stretch and fold four times every 30 minutes, bulk \"\n",
        "        \"ferment until doubled, shape, proof overnight in the fridge, bake at \"\n",
        "        \"250C with steam for 20 minutes, then 230C dry for 20-25 minutes until \"\n",
        "        \"deeply browned.\"\n",
        "    ),\n",
        "    \"joke\": (\n",
        "        \"A data scientist walks into a bakery and asks for a pie chart. The baker \"\n",
        "        \"hands over a blueberry tart and says, 'Careful, the confidence interval \"\n",
        "        \"is deliciously narrow today.'\"\n",
        "    ),\n",
        "    \"code\": (\n",
        "        \"Write a Python function that parses a CSV file of orders, groups them \"\n",
        "        \"by customer, computes total spend, and returns the top five customers \"\n",
        "        \"by revenue. The function should handle missing values, malformed rows, \"\n",
        "        \"and should stream the file to avoid loading it all into memory.\"\n",
        "    ),\n",
        "    \"legal\": (\n",
        "        \"This agreement indemnifies the consultant against all claims arising \"\n",
        "        \"from negligent implementation of the clientâ€™s specifications, except \"\n",
        "        \"where gross misconduct is proven by clear and convincing evidence.\"\n",
        "    ),\n",
        "    \"news\": (\n",
        "        \"Analysts expect the central bank to pause rate hikes after inflation \"\n",
        "        \"fell for the third consecutive month, but warn that energy volatility \"\n",
        "        \"could force a surprise move before year-end.\"\n",
        "    ),\n",
        "    \"speculative\": (\n",
        "        \"In the distant future, autonomous probes exchange compressed knowledge \"\n",
        "        \"packets near the heliopause, negotiating bandwidth and trust scores \"\n",
        "        \"before relaying discoveries back to their origin worlds.\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "print(f\"Loaded {len(prompts)} prompts (long-form)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run analysis\n",
        "all_results = {}\n",
        "\n",
        "for name, prompt in prompts.items():\n",
        "    print(f\"Processing: {name}...\")\n",
        "    result = run_inference(prompt)\n",
        "    attn_entropy = attention_entropy_per_layer(result)\n",
        "    logit_entropy_vals, top_tokens_by_layer = logit_lens(result, top_k=5)\n",
        "    alignment = head_alignment_per_layer(result)\n",
        "    pos_change = mean_position_change_per_layer(result)\n",
        "    layer_sim = layer_to_layer_similarity(result)\n",
        "    sim_final = layer_similarity_to_final(result)\n",
        "    \n",
        "    # Final output\n",
        "    final_logits = result[\"logits\"][0, -1, :]\n",
        "    probs = F.softmax(final_logits, dim=-1)\n",
        "    final_entropy = -(probs * probs.clamp(min=1e-10).log()).sum().item()\n",
        "    topk_final = torch.topk(probs, k=5).indices.tolist()\n",
        "    topk_tokens = [tokenizer.decode([tid]) for tid in topk_final]\n",
        "    \n",
        "    all_results[name] = {\n",
        "        \"prompt\": prompt,\n",
        "        \"tokens\": result[\"tokens\"],\n",
        "        \"top_tokens_by_layer\": top_tokens_by_layer,\n",
        "        \"topk_final\": topk_tokens,\n",
        "        \"final_entropy\": final_entropy,\n",
        "        \"attn_entropy\": attn_entropy,\n",
        "        \"logit_entropy\": logit_entropy_vals,\n",
        "        \"head_alignment\": alignment,\n",
        "        \"position_change\": pos_change,\n",
        "        \"layer_to_layer_sim\": layer_sim,\n",
        "        \"sim_to_final\": sim_final,\n",
        "    }\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "print(\"=\"*120)\n",
        "print(f\"{'Prompt':<16} {'Final top-5 tokens':<60} {'Ent':>6} {'Align(L11)':>10} {'SimFinal(L0)':>12}\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "for name, r in all_results.items():\n",
        "    prompt_short = name[:16]\n",
        "    top5 = \", \".join([t.strip() for t in r[\"topk_final\"]])[:58]\n",
        "    ent = r[\"final_entropy\"]\n",
        "    align = r[\"head_alignment\"][-1]\n",
        "    sim0 = r[\"sim_to_final\"][0]\n",
        "    print(f\"{prompt_short:<16} {top5:<60} {ent:>6.2f} {align:>10.3f} {sim0:>12.3f}\")\n",
        "\n",
        "print(\"=\"*120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: All metrics for the story prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = \"story\"\n",
        "r = all_results[target]\n",
        "\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "gs = GridSpec(3, 2, figure=fig)\n",
        "\n",
        "layers = list(range(n_layers))\n",
        "\n",
        "# Attention entropy\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.plot(layers, r[\"attn_entropy\"], 'o-', color='crimson', linewidth=2)\n",
        "ax1.set_xlabel(\"Layer\")\n",
        "ax1.set_ylabel(\"Entropy (nats)\")\n",
        "ax1.set_title(\"Attention Entropy\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Logit entropy\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.plot(range(len(r[\"logit_entropy\"])), r[\"logit_entropy\"], 's-', color='darkorange', linewidth=2)\n",
        "ax2.set_xlabel(\"Layer (0=emb)\")\n",
        "ax2.set_ylabel(\"Output Entropy\")\n",
        "ax2.set_title(\"Logit Lens: Output Sharpness\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Head alignment\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "ax3.plot(layers, r[\"head_alignment\"], '^-', color='forestgreen', linewidth=2)\n",
        "ax3.set_xlabel(\"Layer\")\n",
        "ax3.set_ylabel(\"Mean Pairwise Cosine Sim\")\n",
        "ax3.set_title(\"Head Alignment\")\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Convergence to final\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "ax4.plot(range(len(r[\"sim_to_final\"])), r[\"sim_to_final\"], 'D-', color='purple', linewidth=2)\n",
        "ax4.set_xlabel(\"Layer (0=emb)\")\n",
        "ax4.set_ylabel(\"Cosine Sim to Final\")\n",
        "ax4.set_title(\"Convergence to Final\")\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Layer-to-layer similarity\n",
        "ax5 = fig.add_subplot(gs[2, 0])\n",
        "ax5.plot(range(len(r[\"layer_to_layer_sim\"])), r[\"layer_to_layer_sim\"], 'p-', color='teal', linewidth=2)\n",
        "ax5.set_xlabel(\"Transition (L_i -> L_{i+1})\")\n",
        "ax5.set_ylabel(\"Cosine Sim\")\n",
        "ax5.set_title(\"Layer-to-Layer Stability\")\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Top tokens by layer\n",
        "ax6 = fig.add_subplot(gs[2, 1])\n",
        "ax6.axis('off')\n",
        "rows = []\n",
        "for i, toks in enumerate(r[\"top_tokens_by_layer\"]):\n",
        "    rows.append(f\"L{i:02d}: {', '.join([t.strip() for t in toks[:3]])}\")\n",
        "ax6.text(0.05, 0.95, \"Top-3 tokens per layer:\\n\" + \"\\n\".join(rows), va='top', fontsize=9, family='monospace')\n",
        "ax6.set_title(f\"Final top-5: {', '.join(r['topk_final'])}\")\n",
        "\n",
        "plt.suptitle(f\"Collapse Metrics (story): '{r['prompt'][:60]}...'\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"01_story_metrics.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Heatmaps across long prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = list(all_results.keys())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "alignment_matrix = np.array([all_results[n][\"head_alignment\"] for n in names])\n",
        "ax1 = axes[0]\n",
        "im1 = ax1.imshow(alignment_matrix, aspect='auto', cmap='viridis')\n",
        "ax1.set_yticks(range(len(names)))\n",
        "ax1.set_yticklabels([n[:12] for n in names], fontsize=8)\n",
        "ax1.set_xlabel(\"Layer\")\n",
        "ax1.set_title(\"Head Alignment (long prompts)\")\n",
        "plt.colorbar(im1, ax=ax1)\n",
        "\n",
        "logit_matrix = np.array([all_results[n][\"logit_entropy\"] for n in names])\n",
        "ax2 = axes[1]\n",
        "im2 = ax2.imshow(logit_matrix, aspect='auto', cmap='magma')\n",
        "ax2.set_yticks(range(len(names)))\n",
        "ax2.set_yticklabels([n[:12] for n in names], fontsize=8)\n",
        "ax2.set_xlabel(\"Layer (0=emb)\")\n",
        "ax2.set_title(\"Output Entropy (logit lens)\")\n",
        "plt.colorbar(im2, ax=ax2)\n",
        "\n",
        "plt.suptitle(\"Heatmaps: Alignment and Output Entropy (Long Prompts)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"02_heatmaps_long.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "summary = {\n",
        "    \"model\": model_name,\n",
        "    \"n_layers\": n_layers,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"prompts\": {\n",
        "        name: {\n",
        "            \"prompt\": r[\"prompt\"],\n",
        "            \"tokens\": r[\"tokens\"],\n",
        "            \"top_tokens_by_layer\": r[\"top_tokens_by_layer\"],\n",
        "            \"topk_final\": r[\"topk_final\"],\n",
        "            \"final_entropy\": float(r[\"final_entropy\"]),\n",
        "            \"attn_entropy\": [float(x) for x in r[\"attn_entropy\"]],\n",
        "            \"logit_entropy\": [float(x) for x in r[\"logit_entropy\"]],\n",
        "            \"head_alignment\": [float(x) for x in r[\"head_alignment\"]],\n",
        "            \"position_change\": [float(x) for x in r[\"position_change\"]],\n",
        "            \"layer_to_layer_sim\": [float(x) for x in r[\"layer_to_layer_sim\"]],\n",
        "            \"sim_to_final\": [float(x) for x in r[\"sim_to_final\"]],\n",
        "        }\n",
        "        for name, r in all_results.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(FIG_DIR / \"collapse_results_v2.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Saved to {FIG_DIR / 'collapse_results_v2.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "- Prompts are now long-form (stories, recipes, jokes, code, legal, news, speculative)\n",
        "- Added top-k tokens (per layer and final) to avoid single-token focus\n",
        "- Metrics kept: attention entropy, logit lens entropy, head alignment, position change, convergence\n",
        "- Figures:\n",
        "  - `01_story_metrics.png`: All metrics for story prompt\n",
        "  - `02_heatmaps_long.png`: Heatmaps of alignment and logit entropy across prompts\n",
        "- Results saved to `figs_collapse_v2/collapse_results_v2.json`\n",
        "\n",
        "Key expectation: longer prompts should show richer dynamics and more informative top-k continuations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
