{
  "model": "gpt2",
  "n_layers": 12,
  "n_heads": 12,
  "prompts": {
    "story": {
      "prompt": "On the fourth night of the winter storm, the lighthouse keeper noticed the beam flicker. He grabbed his tools, descended the narrow stairs, and discovered a loose wire sparking near the fuel line. With waves smashing the rocks and a cargo ship approaching, he had one chance to repair the circuit before the coast went dark.",
      "tokens": [
        "On",
        " the",
        " fourth",
        " night",
        " of",
        " the",
        " winter",
        " storm",
        ",",
        " the",
        " lighthouse",
        " keeper",
        " noticed",
        " the",
        " beam",
        " fl",
        "icker",
        ".",
        " He",
        " grabbed",
        " his",
        " tools",
        ",",
        " descended",
        " the",
        " narrow",
        " stairs",
        ",",
        " and",
        " discovered",
        " a",
        " loose",
        " wire",
        " sparking",
        " near",
        " the",
        " fuel",
        " line",
        ".",
        " With",
        " waves",
        " smashing",
        " the",
        " rocks",
        " and",
        " a",
        " cargo",
        " ship",
        " approaching",
        ",",
        " he",
        " had",
        " one",
        " chance",
        " to",
        " repair",
        " the",
        " circuit",
        " before",
        " the",
        " coast",
        " went",
        " dark",
        "."
      ],
      "top_tokens_by_layer": [
        [
          " challeng",
          " destro",
          " arrang",
          " corrid",
          " mathemat"
        ],
        [
          " And",
          " But",
          "\n",
          " It",
          " However"
        ],
        [
          " It",
          " And",
          " But",
          "\n",
          " They"
        ],
        [
          "\n",
          " It",
          " But",
          " And",
          " However"
        ],
        [
          " It",
          " Fortunately",
          " However",
          " But",
          "\n"
        ],
        [
          " Fortunately",
          " Luckily",
          " Unfortunately",
          " However",
          " It"
        ],
        [
          " Fortunately",
          " Luckily",
          " Unfortunately",
          " Thankfully",
          " It"
        ],
        [
          " Fortunately",
          " Luckily",
          " Unfortunately",
          " Instead",
          " Thankfully"
        ],
        [
          " Fortunately",
          " Luckily",
          " Unfortunately",
          " Instead",
          " Thankfully"
        ],
        [
          " Fortunately",
          " Luckily",
          " Instead",
          " Unfortunately",
          "\n"
        ],
        [
          " He",
          " Fortunately",
          " Instead",
          " Luckily",
          "\n"
        ],
        [
          " He",
          "\n",
          " But",
          " Instead",
          " Fortunately"
        ],
        [
          "\n",
          " (",
          " The",
          " \"",
          ","
        ]
      ],
      "topk_final": [
        "\n",
        " He",
        " The",
        " But",
        "\n\n"
      ],
      "final_entropy": 3.6544361114501953,
      "attn_entropy": [
        2.9526355266571045,
        3.397597551345825,
        2.2906181812286377,
        2.138765811920166,
        2.1158547401428223,
        1.9420162439346313,
        2.3416588306427,
        1.3474231958389282,
        2.029866933822632,
        1.6201351881027222,
        1.725428581237793,
        2.4051711559295654
      ],
      "logit_entropy": [
        0.0906115174293518,
        3.919095754623413,
        3.926478385925293,
        3.959705352783203,
        3.7983508110046387,
        3.3119523525238037,
        2.533212661743164,
        1.5185773372650146,
        1.5336182117462158,
        1.571620225906372,
        2.2544350624084473,
        2.0677666664123535,
        7.463138580322266
      ],
      "head_alignment": [
        0.8949952721595764,
        0.889513373374939,
        0.8356281518936157,
        0.9979743957519531,
        0.8671970963478088,
        0.9979698657989502,
        0.9854950308799744,
        0.9997434020042419,
        0.9907931685447693,
        0.989340603351593,
        0.999855637550354,
        0.907069981098175
      ],
      "position_change": [
        0.3342439830303192,
        0.2896176874637604,
        0.3098808228969574,
        0.31766846776008606,
        0.33416256308555603,
        0.3378032147884369,
        0.3466753661632538,
        0.34198498725891113,
        0.3363983631134033,
        0.3192441761493683,
        0.2910176217556,
        0.21985411643981934,
        0.025030866265296936
      ],
      "layer_to_layer_sim": [
        0.1769547015428543,
        0.9796624183654785,
        0.9844753742218018,
        0.9843042492866516,
        0.9820191860198975,
        0.9759806394577026,
        0.9485888481140137,
        0.967742383480072,
        0.9637032151222229,
        0.9655504822731018,
        0.9446129202842712,
        0.4325923025608063
      ],
      "sim_to_final": [
        0.029688887298107147,
        0.03841780126094818,
        0.037476398050785065,
        0.05354250967502594,
        0.06848588585853577,
        0.07617717981338501,
        0.09757623821496964,
        0.11614885926246643,
        0.12869949638843536,
        0.1637190878391266,
        0.20208914577960968,
        0.4325923025608063,
        1.0000001192092896
      ]
    },
    "recipe": {
      "prompt": "To make a crusty sourdough loaf, feed your starter the night before, then mix 500g bread flour, 350g water, 100g active starter, and 10g salt. Rest 30 minutes, stretch and fold four times every 30 minutes, bulk ferment until doubled, shape, proof overnight in the fridge, bake at 250C with steam for 20 minutes, then 230C dry for 20-25 minutes until deeply browned.",
      "tokens": [
        "To",
        " make",
        " a",
        " crust",
        "y",
        " sour",
        "d",
        "ough",
        " loaf",
        ",",
        " feed",
        " your",
        " starter",
        " the",
        " night",
        " before",
        ",",
        " then",
        " mix",
        " 500",
        "g",
        " bread",
        " flour",
        ",",
        " 350",
        "g",
        " water",
        ",",
        " 100",
        "g",
        " active",
        " starter",
        ",",
        " and",
        " 10",
        "g",
        " salt",
        ".",
        " Rest",
        " 30",
        " minutes",
        ",",
        " stretch",
        " and",
        " fold",
        " four",
        " times",
        " every",
        " 30",
        " minutes",
        ",",
        " bulk",
        " ferment",
        " until",
        " doubled",
        ",",
        " shape",
        ",",
        " proof",
        " overnight",
        " in",
        " the",
        " fridge",
        ",",
        " bake",
        " at",
        " 250",
        "C",
        " with",
        " steam",
        " for",
        " 20",
        " minutes",
        ",",
        " then",
        " 230",
        "C",
        " dry",
        " for",
        " 20",
        "-",
        "25",
        " minutes",
        " until",
        " deeply",
        " brown",
        "ed",
        "."
      ],
      "top_tokens_by_layer": [
        [
          " destro",
          " challeng",
          " withd",
          "theless",
          " arrang"
        ],
        [
          "\n",
          " And",
          " It",
          " But",
          " The"
        ],
        [
          "\n",
          " It",
          " And",
          " But",
          " The"
        ],
        [
          "\n",
          " It",
          " This",
          " However",
          " The"
        ],
        [
          "\n",
          " This",
          " Alternatively",
          " Additionally",
          " However"
        ],
        [
          "\n",
          "<|endoftext|>",
          " Finally",
          " Then",
          " This"
        ],
        [
          "\n",
          " Finally",
          "<|endoftext|>",
          " Then",
          " This"
        ],
        [
          "\n",
          "<|endoftext|>",
          " Finally",
          " Then",
          " Once"
        ],
        [
          " Then",
          "<|endoftext|>",
          " Once",
          "\n",
          " Finally"
        ],
        [
          " Then",
          " Alternatively",
          " Remove",
          " Once",
          " Serve"
        ],
        [
          " Remove",
          " Serve",
          " Repeat",
          " Then",
          " Alternatively"
        ],
        [
          " Serve",
          " Remove",
          " Transfer",
          " Allow",
          " Repeat"
        ],
        [
          "\n",
          ",",
          " (",
          ".",
          " the"
        ]
      ],
      "topk_final": [
        "\n",
        "<|endoftext|>",
        " Allow",
        " Remove",
        " Serve"
      ],
      "final_entropy": 4.4111504554748535,
      "attn_entropy": [
        3.126197576522827,
        3.6272895336151123,
        2.480970621109009,
        2.1156680583953857,
        1.9215420484542847,
        1.5274696350097656,
        2.320976734161377,
        1.5030559301376343,
        1.927357792854309,
        1.7198516130447388,
        1.7629495859146118,
        2.6613054275512695
      ],
      "logit_entropy": [
        0.6891677975654602,
        3.8627142906188965,
        3.954712152481079,
        3.8282077312469482,
        3.6629509925842285,
        2.9135751724243164,
        3.313934326171875,
        2.679779052734375,
        2.1428799629211426,
        2.220628261566162,
        2.0633978843688965,
        2.4789700508117676,
        7.49316930770874
      ],
      "head_alignment": [
        0.8892108798027039,
        0.8887979984283447,
        0.8160080313682556,
        0.9991946220397949,
        0.9076739549636841,
        0.9983920454978943,
        0.9942871928215027,
        0.9996538162231445,
        0.9968860745429993,
        0.9907403588294983,
        0.9974163174629211,
        0.8976675271987915
      ],
      "position_change": [
        0.3477184772491455,
        0.2878813147544861,
        0.3142590820789337,
        0.33343711495399475,
        0.353914350271225,
        0.364287793636322,
        0.37073004245758057,
        0.3713764250278473,
        0.37011656165122986,
        0.35295751690864563,
        0.330779492855072,
        0.2531094253063202,
        0.027157602831721306
      ],
      "layer_to_layer_sim": [
        0.21587541699409485,
        0.9732927083969116,
        0.9833762645721436,
        0.9743801951408386,
        0.971741795539856,
        0.970999002456665,
        0.9576124548912048,
        0.9522914290428162,
        0.9565027952194214,
        0.9449743628501892,
        0.9196476936340332,
        0.46641629934310913
      ],
      "sim_to_final": [
        0.05696127936244011,
        0.03526364266872406,
        0.03387098014354706,
        0.031404510140419006,
        0.0459643118083477,
        0.05862128734588623,
        0.07755254954099655,
        0.10163147002458572,
        0.12475232779979706,
        0.15900565683841705,
        0.2186761200428009,
        0.46641629934310913,
        0.9999999403953552
      ]
    },
    "joke": {
      "prompt": "A data scientist walks into a bakery and asks for a pie chart. The baker hands over a blueberry tart and says, 'Careful, the confidence interval is deliciously narrow today.'",
      "tokens": [
        "A",
        " data",
        " scientist",
        " walks",
        " into",
        " a",
        " bakery",
        " and",
        " asks",
        " for",
        " a",
        " pie",
        " chart",
        ".",
        " The",
        " baker",
        " hands",
        " over",
        " a",
        " blue",
        "berry",
        " tart",
        " and",
        " says",
        ",",
        " '",
        "Care",
        "ful",
        ",",
        " the",
        " confidence",
        " interval",
        " is",
        " delic",
        "iously",
        " narrow",
        " today",
        ".'"
      ],
      "top_tokens_by_layer": [
        [
          ".'",
          ".'\"",
          "!'\"",
          "!'",
          "'.\""
        ],
        [
          " It",
          "\n",
          " The",
          " When",
          " As"
        ],
        [
          "\n",
          " It",
          " The",
          " When",
          " That"
        ],
        [
          "\n",
          " It",
          " And",
          " That",
          " As"
        ],
        [
          "\n",
          " And",
          " The",
          " It",
          " That"
        ],
        [
          "\n",
          " And",
          " The",
          " It",
          " When"
        ],
        [
          "\n",
          " And",
          " That",
          " It",
          " Then"
        ],
        [
          "\n",
          " Then",
          " That",
          " It",
          " Meanwhile"
        ],
        [
          "\n",
          " Then",
          " That",
          " Meanwhile",
          " It"
        ],
        [
          " Then",
          "\n",
          " Meanwhile",
          " That",
          " Suddenly"
        ],
        [
          " Then",
          " Suddenly",
          " He",
          " The",
          "\n"
        ],
        [
          " The",
          "\n",
          " Then",
          " He",
          " She"
        ],
        [
          "\n",
          ",",
          " (",
          " \"",
          "."
        ]
      ],
      "topk_final": [
        "\n",
        " The",
        " He",
        " She",
        " I"
      ],
      "final_entropy": 4.156117916107178,
      "attn_entropy": [
        2.10732102394104,
        2.7706339359283447,
        2.221790075302124,
        2.016361713409424,
        2.025557518005371,
        1.4957493543624878,
        1.8947795629501343,
        1.1482471227645874,
        1.7184494733810425,
        1.5666488409042358,
        1.9022568464279175,
        2.1654562950134277
      ],
      "logit_entropy": [
        0.010295051150023937,
        4.177029609680176,
        4.299417972564697,
        3.9886083602905273,
        3.9530787467956543,
        3.4111874103546143,
        2.8331265449523926,
        2.5808565616607666,
        1.8242038488388062,
        1.8470667600631714,
        2.0767815113067627,
        2.766416549682617,
        7.239365577697754
      ],
      "head_alignment": [
        0.883674144744873,
        0.8662989139556885,
        0.8760700821876526,
        0.992579460144043,
        0.9038649797439575,
        0.9995560050010681,
        0.9861020445823669,
        0.9991073608398438,
        0.9995127320289612,
        0.9987011551856995,
        0.998814582824707,
        0.916678249835968
      ],
      "position_change": [
        0.33486443758010864,
        0.2820624113082886,
        0.3258029520511627,
        0.33290785551071167,
        0.345120906829834,
        0.3455890119075775,
        0.36060741543769836,
        0.3507683575153351,
        0.35032063722610474,
        0.32342469692230225,
        0.28754863142967224,
        0.20848628878593445,
        0.0199050884693861
      ],
      "layer_to_layer_sim": [
        0.08140356093645096,
        0.9646429419517517,
        0.9774823188781738,
        0.9773019552230835,
        0.9773105382919312,
        0.9795756340026855,
        0.9587708115577698,
        0.9695200324058533,
        0.9617865085601807,
        0.9564720392227173,
        0.9398850798606873,
        0.41663816571235657
      ],
      "sim_to_final": [
        -0.08828144520521164,
        0.00032852080767042935,
        0.004491272382438183,
        0.01384348887950182,
        0.020824801176786423,
        0.036794018000364304,
        0.053344570100307465,
        0.08124684542417526,
        0.08094505965709686,
        0.11890902370214462,
        0.17801237106323242,
        0.41663816571235657,
        1.0000001192092896
      ]
    },
    "code": {
      "prompt": "Write a Python function that parses a CSV file of orders, groups them by customer, computes total spend, and returns the top five customers by revenue. The function should handle missing values, malformed rows, and should stream the file to avoid loading it all into memory.",
      "tokens": [
        "Write",
        " a",
        " Python",
        " function",
        " that",
        " pars",
        "es",
        " a",
        " CSV",
        " file",
        " of",
        " orders",
        ",",
        " groups",
        " them",
        " by",
        " customer",
        ",",
        " comp",
        "utes",
        " total",
        " spend",
        ",",
        " and",
        " returns",
        " the",
        " top",
        " five",
        " customers",
        " by",
        " revenue",
        ".",
        " The",
        " function",
        " should",
        " handle",
        " missing",
        " values",
        ",",
        " mal",
        "formed",
        " rows",
        ",",
        " and",
        " should",
        " stream",
        " the",
        " file",
        " to",
        " avoid",
        " loading",
        " it",
        " all",
        " into",
        " memory",
        "."
      ],
      "top_tokens_by_layer": [
        [
          " challeng",
          " arrang",
          " destro",
          " mathemat",
          " corrid"
        ],
        [
          "\n",
          " It",
          " If",
          " But",
          " However"
        ],
        [
          "\n",
          " However",
          " If",
          " It",
          " The"
        ],
        [
          "\n",
          " However",
          " If",
          " This",
          " It"
        ],
        [
          "\n",
          " However",
          " If",
          " This",
          " It"
        ],
        [
          "\n",
          " However",
          " Additionally",
          " Moreover",
          " Otherwise"
        ],
        [
          " Additionally",
          "\n",
          " However",
          " Moreover",
          " Also"
        ],
        [
          "\n",
          " Additionally",
          " However",
          " Finally",
          " Also"
        ],
        [
          " Alternatively",
          "\n",
          " Otherwise",
          " Additionally",
          " If"
        ],
        [
          "\n",
          " If",
          " Alternatively",
          " This",
          " Additionally"
        ],
        [
          "\n",
          " If",
          " This",
          " Alternatively",
          " It"
        ],
        [
          "\n",
          " It",
          " The",
          " This",
          " If"
        ],
        [
          "\n",
          ",",
          " (",
          " the",
          "."
        ]
      ],
      "topk_final": [
        "\n",
        " The",
        " It",
        " This",
        " If"
      ],
      "final_entropy": 3.3714747428894043,
      "attn_entropy": [
        2.8865315914154053,
        3.3211669921875,
        2.3235924243927,
        2.16589617729187,
        2.037111282348633,
        1.47372567653656,
        2.059746026992798,
        1.1989613771438599,
        1.7794941663742065,
        1.6219242811203003,
        1.9772974252700806,
        2.7313835620880127
      ],
      "logit_entropy": [
        0.15219472348690033,
        3.966665744781494,
        3.8910000324249268,
        3.657205104827881,
        3.555806875228882,
        2.924220085144043,
        3.122696876525879,
        2.659766435623169,
        2.453543186187744,
        1.5465753078460693,
        1.949629306793213,
        1.607156753540039,
        7.696527481079102
      ],
      "head_alignment": [
        0.9019758701324463,
        0.8907579183578491,
        0.8422833681106567,
        0.9972968697547913,
        0.8786725997924805,
        0.9997797012329102,
        0.9946441650390625,
        0.9998449683189392,
        0.9981299638748169,
        0.9994039535522461,
        0.9996488094329834,
        0.9288343191146851
      ],
      "position_change": [
        0.3260657787322998,
        0.2779366970062256,
        0.31523042917251587,
        0.3272855281829834,
        0.3377090096473694,
        0.3351949453353882,
        0.34289461374282837,
        0.3275701701641083,
        0.32001668214797974,
        0.29762011766433716,
        0.2586610019207001,
        0.1875854879617691,
        0.019583463668823242
      ],
      "layer_to_layer_sim": [
        0.17169375717639923,
        0.9802113175392151,
        0.9884387254714966,
        0.9853597283363342,
        0.9804186224937439,
        0.9832605123519897,
        0.9700455069541931,
        0.9728280305862427,
        0.9650148153305054,
        0.9686883091926575,
        0.9538437724113464,
        0.3973803222179413
      ],
      "sim_to_final": [
        -0.006581851281225681,
        0.02903682552278042,
        0.03315778449177742,
        0.04028914123773575,
        0.04178817197680473,
        0.05332716926932335,
        0.07036703079938889,
        0.08788003027439117,
        0.11965944617986679,
        0.1439620703458786,
        0.17629174888134003,
        0.3973803222179413,
        1.0000001192092896
      ]
    },
    "legal": {
      "prompt": "This agreement indemnifies the consultant against all claims arising from negligent implementation of the client\u2019s specifications, except where gross misconduct is proven by clear and convincing evidence.",
      "tokens": [
        "This",
        " agreement",
        " indemn",
        "ifies",
        " the",
        " consultant",
        " against",
        " all",
        " claims",
        " arising",
        " from",
        " negligent",
        " implementation",
        " of",
        " the",
        " client",
        "\ufffd",
        "\ufffd",
        "s",
        " specifications",
        ",",
        " except",
        " where",
        " gross",
        " misconduct",
        " is",
        " proven",
        " by",
        " clear",
        " and",
        " convincing",
        " evidence",
        "."
      ],
      "top_tokens_by_layer": [
        [
          " challeng",
          " arrang",
          " mathemat",
          " corrid",
          " destro"
        ],
        [
          "\n",
          " It",
          " But",
          " And",
          " However"
        ],
        [
          "\n",
          " However",
          " It",
          " The",
          " In"
        ],
        [
          "\n",
          " However",
          " Therefore",
          " If",
          " In"
        ],
        [
          " However",
          " Therefore",
          "\n",
          " If",
          " Additionally"
        ],
        [
          " However",
          " Therefore",
          " Additionally",
          "\n",
          " Furthermore"
        ],
        [
          " However",
          " Therefore",
          " Additionally",
          "\n",
          " Accordingly"
        ],
        [
          " However",
          " Additionally",
          "\n",
          " Therefore",
          " Accordingly"
        ],
        [
          " Additionally",
          " However",
          "\n",
          " Therefore",
          " Accordingly"
        ],
        [
          " Additionally",
          " Accordingly",
          " Furthermore",
          " However",
          " Therefore"
        ],
        [
          "\n",
          " Accordingly",
          " The",
          " This",
          " Such"
        ],
        [
          "\ufffd",
          "\n",
          " The",
          " In",
          " It"
        ],
        [
          "\n",
          " (",
          ",",
          " The",
          "."
        ]
      ],
      "topk_final": [
        "\n",
        " The",
        "\ufffd",
        " In",
        " This"
      ],
      "final_entropy": 4.35002326965332,
      "attn_entropy": [
        2.5183959007263184,
        2.916832208633423,
        2.2071759700775146,
        1.7961195707321167,
        1.9492770433425903,
        1.2956769466400146,
        1.619388222694397,
        1.062691330909729,
        1.6319327354431152,
        1.5335041284561157,
        1.7609976530075073,
        2.2288458347320557
      ],
      "logit_entropy": [
        0.7367522716522217,
        4.003175735473633,
        3.836862087249756,
        3.4044342041015625,
        3.067577838897705,
        2.7921230792999268,
        2.26155948638916,
        2.378464698791504,
        2.1652579307556152,
        2.387195587158203,
        3.277574300765991,
        3.3199408054351807,
        7.452247619628906
      ],
      "head_alignment": [
        0.902955174446106,
        0.8802556991577148,
        0.8862280249595642,
        0.9989426136016846,
        0.8837896585464478,
        0.9998006820678711,
        0.9978493452072144,
        0.9999303221702576,
        0.9984558820724487,
        0.9997180700302124,
        0.996983528137207,
        0.9200400114059448
      ],
      "position_change": [
        0.3416016101837158,
        0.29011306166648865,
        0.3369762897491455,
        0.33624768257141113,
        0.3471548557281494,
        0.34958070516586304,
        0.3546913266181946,
        0.35834020376205444,
        0.3426375687122345,
        0.33056432008743286,
        0.30235302448272705,
        0.2258242666721344,
        0.020395658910274506
      ],
      "layer_to_layer_sim": [
        0.16653522849082947,
        0.9860941767692566,
        0.983425498008728,
        0.9799835681915283,
        0.9796890616416931,
        0.9825355410575867,
        0.9509260058403015,
        0.9645543694496155,
        0.9675883650779724,
        0.9657623171806335,
        0.9467110633850098,
        0.40340733528137207
      ],
      "sim_to_final": [
        -0.1070622131228447,
        0.019782328978180885,
        0.03368796408176422,
        0.050407156348228455,
        0.04900433495640755,
        0.061568520963191986,
        0.08511461317539215,
        0.10521692037582397,
        0.1259145885705948,
        0.15525054931640625,
        0.20107753574848175,
        0.40340733528137207,
        1.0
      ]
    },
    "news": {
      "prompt": "Analysts expect the central bank to pause rate hikes after inflation fell for the third consecutive month, but warn that energy volatility could force a surprise move before year-end.",
      "tokens": [
        "Analy",
        "sts",
        " expect",
        " the",
        " central",
        " bank",
        " to",
        " pause",
        " rate",
        " hikes",
        " after",
        " inflation",
        " fell",
        " for",
        " the",
        " third",
        " consecutive",
        " month",
        ",",
        " but",
        " warn",
        " that",
        " energy",
        " volatility",
        " could",
        " force",
        " a",
        " surprise",
        " move",
        " before",
        " year",
        "-",
        "end",
        "."
      ],
      "top_tokens_by_layer": [
        [
          " challeng",
          " mathemat",
          " arrang",
          " corrid",
          " destro"
        ],
        [
          "\n",
          " And",
          " But",
          " It",
          " The"
        ],
        [
          "\n",
          " It",
          " But",
          " The",
          " And"
        ],
        [
          "\n",
          " But",
          " It",
          " And",
          " However"
        ],
        [
          "\n",
          " But",
          " However",
          " It",
          " And"
        ],
        [
          "\n",
          " However",
          " But",
          " The",
          "<|endoftext|>"
        ],
        [
          "\n",
          " However",
          " Meanwhile",
          " The",
          " But"
        ],
        [
          "\n",
          " Meanwhile",
          " However",
          " The",
          " Moreover"
        ],
        [
          "\n",
          " Meanwhile",
          " However",
          " According",
          "<|endoftext|>"
        ],
        [
          "\n",
          " Meanwhile",
          "<|endoftext|>",
          " REUTERS",
          " According"
        ],
        [
          "\n",
          "<|endoftext|>",
          " Meanwhile",
          " REUTERS",
          " Credit"
        ],
        [
          "\n",
          "<|endoftext|>",
          "\n\n",
          " The",
          " Credit"
        ],
        [
          "\n",
          " (",
          " The",
          " \"",
          ","
        ]
      ],
      "topk_final": [
        "\n",
        "<|endoftext|>",
        " The",
        "\n\n",
        " \""
      ],
      "final_entropy": 1.891270637512207,
      "attn_entropy": [
        2.5616095066070557,
        3.009678602218628,
        2.274481773376465,
        1.9714421033859253,
        1.925416350364685,
        1.2888563871383667,
        1.7935924530029297,
        1.2327029705047607,
        1.4914346933364868,
        1.2061855792999268,
        1.2253963947296143,
        1.8282123804092407
      ],
      "logit_entropy": [
        0.6516130566596985,
        3.726853370666504,
        3.6792807579040527,
        3.4812121391296387,
        3.552543878555298,
        2.831597089767456,
        2.8519201278686523,
        2.4816248416900635,
        1.768811821937561,
        1.3441920280456543,
        0.6168854236602783,
        0.5341562628746033,
        6.95626163482666
      ],
      "head_alignment": [
        0.9032843708992004,
        0.8913999795913696,
        0.8836595416069031,
        0.9991406798362732,
        0.8682012557983398,
        0.9995574951171875,
        0.9973567724227905,
        0.9997243285179138,
        0.9994065165519714,
        0.9998617172241211,
        0.9998183846473694,
        0.9194644689559937
      ],
      "position_change": [
        0.3201504349708557,
        0.2759611904621124,
        0.3274824023246765,
        0.33179330825805664,
        0.3560687303543091,
        0.3585909903049469,
        0.36001741886138916,
        0.3467956781387329,
        0.3373567461967468,
        0.32193097472190857,
        0.30355533957481384,
        0.2241746485233307,
        0.0187321025878191
      ],
      "layer_to_layer_sim": [
        0.1692410111427307,
        0.9830916523933411,
        0.9849936366081238,
        0.9844117164611816,
        0.9737758636474609,
        0.9782376885414124,
        0.960618257522583,
        0.9676987528800964,
        0.9537742733955383,
        0.9650117754936218,
        0.9524447917938232,
        0.2873994708061218
      ],
      "sim_to_final": [
        -0.10909189283847809,
        0.021367061883211136,
        0.03807402774691582,
        0.0522942841053009,
        0.058163780719041824,
        0.07506044954061508,
        0.08061393350362778,
        0.10603714734315872,
        0.11340565979480743,
        0.12899930775165558,
        0.15305200219154358,
        0.2873994708061218,
        1.0
      ]
    },
    "speculative": {
      "prompt": "In the distant future, autonomous probes exchange compressed knowledge packets near the heliopause, negotiating bandwidth and trust scores before relaying discoveries back to their origin worlds.",
      "tokens": [
        "In",
        " the",
        " distant",
        " future",
        ",",
        " autonomous",
        " probes",
        " exchange",
        " compressed",
        " knowledge",
        " packets",
        " near",
        " the",
        " hel",
        "iop",
        "ause",
        ",",
        " negotiating",
        " bandwidth",
        " and",
        " trust",
        " scores",
        " before",
        " rel",
        "aying",
        " discoveries",
        " back",
        " to",
        " their",
        " origin",
        " worlds",
        "."
      ],
      "top_tokens_by_layer": [
        [
          " challeng",
          " arrang",
          " mathemat",
          " corrid",
          " destro"
        ],
        [
          "\n",
          " And",
          " It",
          " The",
          " But"
        ],
        [
          "\n",
          " It",
          " And",
          " The",
          " They"
        ],
        [
          "\n",
          " It",
          " The",
          " They",
          " But"
        ],
        [
          "\n",
          " They",
          " It",
          " But",
          " The"
        ],
        [
          "\n",
          " They",
          " And",
          " But",
          " It"
        ],
        [
          "\n",
          " They",
          " These",
          " Such",
          " It"
        ],
        [
          " Such",
          " These",
          "\n",
          " However",
          " They"
        ],
        [
          "\n",
          " Such",
          " These",
          " Eventually",
          " However"
        ],
        [
          " These",
          " Such",
          "\n",
          " This",
          " In"
        ],
        [
          " Such",
          " These",
          " This",
          "\n",
          " But"
        ],
        [
          "\n",
          " These",
          " This",
          " Such",
          " In"
        ],
        [
          "\n",
          " (",
          ",",
          " The",
          " \""
        ]
      ],
      "topk_final": [
        "\n",
        " The",
        " This",
        " In",
        " These"
      ],
      "final_entropy": 4.116214752197266,
      "attn_entropy": [
        2.484628200531006,
        2.915522336959839,
        2.101285934448242,
        1.8376035690307617,
        1.8834370374679565,
        1.499550461769104,
        1.8166770935058594,
        1.2206162214279175,
        1.4946070909500122,
        1.5291091203689575,
        1.4857310056686401,
        1.967859148979187
      ],
      "logit_entropy": [
        0.7038348913192749,
        3.6635520458221436,
        3.5234971046447754,
        3.338916540145874,
        3.4300057888031006,
        3.2074055671691895,
        3.390488386154175,
        3.424070358276367,
        3.4338574409484863,
        2.709136724472046,
        2.2655036449432373,
        2.7844491004943848,
        7.31743049621582
      ],
      "head_alignment": [
        0.8912648558616638,
        0.8925895690917969,
        0.8903311491012573,
        0.9982194900512695,
        0.9003701210021973,
        0.9993875622749329,
        0.988327145576477,
        0.9998655319213867,
        0.9990959167480469,
        0.9996271133422852,
        0.9997264742851257,
        0.9189940690994263
      ],
      "position_change": [
        0.3627704381942749,
        0.31798630952835083,
        0.3596976697444916,
        0.35841479897499084,
        0.3713604807853699,
        0.3695943355560303,
        0.37386786937713623,
        0.36743003129959106,
        0.3556767404079437,
        0.32791581749916077,
        0.295235812664032,
        0.20728908479213715,
        0.029394138604402542
      ],
      "layer_to_layer_sim": [
        0.16169549524784088,
        0.9838283658027649,
        0.9853402972221375,
        0.9867194294929504,
        0.984162449836731,
        0.9849119782447815,
        0.9693055748939514,
        0.9651178121566772,
        0.9663158655166626,
        0.975206196308136,
        0.9497400522232056,
        0.4087740182876587
      ],
      "sim_to_final": [
        -0.126255065202713,
        0.011281304061412811,
        0.03274756669998169,
        0.045467834919691086,
        0.05736523121595383,
        0.0681854784488678,
        0.0843689888715744,
        0.09645592421293259,
        0.11674074083566666,
        0.1312834769487381,
        0.18581701815128326,
        0.4087740182876587,
        1.0
      ]
    }
  }
}