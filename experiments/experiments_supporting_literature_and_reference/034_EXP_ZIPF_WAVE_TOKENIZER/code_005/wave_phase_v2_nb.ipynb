{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wave Phase Dynamics v2: Fixed Coherence Measures\n",
        "\n",
        "## Key fixes from v1:\n",
        "\n",
        "1. **INVERTED frequency mapping**: Common words = HIGH frequency (more attended)\n",
        "2. **Multiple coherence measures** that respond to attention changes\n",
        "3. **Spectral concentration** metric for measuring wave interference\n",
        "\n",
        "## The insight\n",
        "\n",
        "LLMs attend more to common/structural tokens (\"the\", \"is\", \"a\").\n",
        "These should be HIGH frequency carriers that attention amplifies.\n",
        "Rare words carry specific information but are attended less.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib import cm\n",
        "from scipy.signal import hilbert\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    HAS_DATASETS = True\n",
        "except ImportError:\n",
        "    HAS_DATASETS = False\n",
        "    print(\"datasets not available, using fallback word frequencies\")\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "FIG_DIR = NOTEBOOK_DIR / \"figs_wave_v2\"\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2\n",
        "model_name = \"gpt2\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.n_layer\n",
        "n_heads = model.config.n_head\n",
        "d_model = model.config.n_embd\n",
        "print(f\"Model: {n_layers} layers, {n_heads} heads, d_model={d_model}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build word frequency table\n",
        "def build_word_frequency_table(tokenizer, max_words: int = 50000) -> Dict[str, int]:\n",
        "    if HAS_DATASETS:\n",
        "        print(\"Loading wikitext-2 for frequency estimation...\")\n",
        "        try:\n",
        "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "            word_counts = Counter()\n",
        "            for example in dataset:\n",
        "                text = example[\"text\"]\n",
        "                if text.strip():\n",
        "                    words = text.lower().split()\n",
        "                    word_counts.update(words)\n",
        "            ranked = word_counts.most_common(max_words)\n",
        "            word_rank = {word: rank + 1 for rank, (word, _) in enumerate(ranked)}\n",
        "            print(f\"Built frequency table with {len(word_rank)} words from wikitext-2\")\n",
        "            return word_rank\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load wikitext-2: {e}\")\n",
        "    \n",
        "    print(\"Using GPT-2 vocab order as frequency proxy...\")\n",
        "    word_rank = {}\n",
        "    for token_id in range(tokenizer.vocab_size):\n",
        "        token = tokenizer.decode([token_id]).strip().lower()\n",
        "        if token and token not in word_rank:\n",
        "            word_rank[token] = len(word_rank) + 1\n",
        "    print(f\"Built frequency table with {len(word_rank)} tokens\")\n",
        "    return word_rank\n",
        "\n",
        "word_freq_table = build_word_frequency_table(tokenizer)\n",
        "\n",
        "common = sorted(word_freq_table.items(), key=lambda x: x[1])[:10]\n",
        "rare = sorted(word_freq_table.items(), key=lambda x: x[1])[-10:]\n",
        "print(f\"\\nMost common: {common}\")\n",
        "print(f\"Most rare: {rare}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wave encoder with INVERTED frequency mapping\n",
        "# Common words = HIGH frequency (more attended)\n",
        "# Rare words = LOW frequency (less attended)\n",
        "\n",
        "@dataclass\n",
        "class WaveConfig:\n",
        "    freq_min: float = 0.5      # Minimum frequency (RARE words)\n",
        "    freq_max: float = 10.0     # Maximum frequency (COMMON words)\n",
        "    n_harmonics: int = 4\n",
        "    sample_rate: int = 100\n",
        "    duration: float = 2.0\n",
        "\n",
        "class ZipfWaveEncoder:\n",
        "    \"\"\"INVERTED Zipf frequency mapping.\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer, word_freq_table: Dict[str, int], config: WaveConfig):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word_freq_table = word_freq_table\n",
        "        self.config = config\n",
        "        self.max_rank = max(word_freq_table.values()) if word_freq_table else 50000\n",
        "        n_samples = int(config.sample_rate * config.duration)\n",
        "        self.t = np.linspace(0, config.duration, n_samples)\n",
        "    \n",
        "    def token_to_frequency(self, token: str) -> float:\n",
        "        \"\"\"INVERTED: Common words -> HIGH freq, Rare words -> LOW freq.\"\"\"\n",
        "        token_clean = token.strip().lower()\n",
        "        rank = self.word_freq_table.get(token_clean, self.max_rank)\n",
        "        \n",
        "        log_rank = np.log(rank + 1)\n",
        "        log_max = np.log(self.max_rank + 1)\n",
        "        normalized = 1.0 - (log_rank / log_max)  # INVERTED\n",
        "        \n",
        "        freq = self.config.freq_min + (self.config.freq_max - self.config.freq_min) * normalized\n",
        "        return freq\n",
        "    \n",
        "    def token_to_wave(self, token: str, phase_offset: float = 0.0) -> np.ndarray:\n",
        "        freq = self.token_to_frequency(token)\n",
        "        wave = np.zeros_like(self.t, dtype=np.complex128)\n",
        "        for h in range(1, self.config.n_harmonics + 1):\n",
        "            amplitude = 1.0 / h\n",
        "            harmonic_freq = freq * h\n",
        "            wave += amplitude * np.exp(1j * 2 * np.pi * harmonic_freq * self.t + 1j * phase_offset * h)\n",
        "        return wave\n",
        "    \n",
        "    def encode_sequence(self, text: str) -> Tuple[List[str], np.ndarray, np.ndarray]:\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "        frequencies = np.array([self.token_to_frequency(t) for t in tokens])\n",
        "        waves = np.zeros((len(tokens), len(self.t)), dtype=np.complex128)\n",
        "        for i, token in enumerate(tokens):\n",
        "            phase_offset = 2 * np.pi * i / len(tokens)\n",
        "            waves[i] = self.token_to_wave(token, phase_offset)\n",
        "        return tokens, frequencies, waves\n",
        "\n",
        "wave_config = WaveConfig()\n",
        "wave_encoder = ZipfWaveEncoder(tokenizer, word_freq_table, wave_config)\n",
        "\n",
        "print(f\"Wave encoder (INVERTED): freq range [{wave_config.freq_min}, {wave_config.freq_max}] Hz\")\n",
        "print(f\"  Common words -> {wave_config.freq_max} Hz (HIGH)\")\n",
        "print(f\"  Rare words   -> {wave_config.freq_min} Hz (LOW)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED coherence measures that respond to attention\n",
        "\n",
        "def compute_wave_coherence_v1(waves: np.ndarray) -> float:\n",
        "    \"\"\"Original coherence (BROKEN - doesn't respond to attention).\"\"\"\n",
        "    amplitudes = np.abs(waves)\n",
        "    amplitudes[amplitudes < 1e-10] = 1e-10\n",
        "    normalized = waves / amplitudes\n",
        "    mean_phasor = normalized.mean(axis=0)\n",
        "    return np.abs(mean_phasor).mean()\n",
        "\n",
        "def compute_wave_coherence_weighted(waves: np.ndarray, weights: np.ndarray) -> float:\n",
        "    \"\"\"FIX 1: Weighted coherence that preserves attention influence.\"\"\"\n",
        "    weighted_sum = (weights[:, None] * waves).sum(axis=0)\n",
        "    max_magnitude = (weights[:, None] * np.abs(waves)).sum(axis=0)\n",
        "    coherence = np.abs(weighted_sum) / (max_magnitude + 1e-10)\n",
        "    return coherence.mean()\n",
        "\n",
        "def compute_spectral_concentration(waves: np.ndarray, weights: np.ndarray, sample_rate: int = 100) -> float:\n",
        "    \"\"\"FIX 2: Spectral concentration of superposed wave.\"\"\"\n",
        "    superposed = (weights[:, None] * waves).sum(axis=0)\n",
        "    fft_vals = np.abs(fft(superposed.real))\n",
        "    freqs = fftfreq(len(superposed), d=1/sample_rate)\n",
        "    pos_mask = freqs > 0\n",
        "    power = fft_vals[pos_mask] ** 2\n",
        "    power = power / (power.sum() + 1e-10)\n",
        "    entropy = -np.sum(power * np.log(power + 1e-10))\n",
        "    max_entropy = np.log(len(power))\n",
        "    return 1.0 - (entropy / max_entropy)\n",
        "\n",
        "def compute_interference_strength(waves: np.ndarray, weights: np.ndarray) -> float:\n",
        "    \"\"\"FIX 3: Interference strength (>1 = constructive).\"\"\"\n",
        "    superposed = (weights[:, None] * waves).sum(axis=0)\n",
        "    superposed_power = np.abs(superposed) ** 2\n",
        "    individual_powers = (weights[:, None] ** 2) * (np.abs(waves) ** 2)\n",
        "    sum_of_powers = individual_powers.sum(axis=0)\n",
        "    return superposed_power.mean() / (sum_of_powers.mean() + 1e-10)\n",
        "\n",
        "def run_inference(text: str) -> Dict:\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "    tokens = [tokenizer.decode([tid]) for tid in input_ids[0]]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_attentions=True, output_hidden_states=True)\n",
        "    return {\n",
        "        \"input_ids\": input_ids, \"tokens\": tokens, \"logits\": outputs.logits.cpu(),\n",
        "        \"attentions\": [a.cpu() for a in outputs.attentions],\n",
        "        \"hidden_states\": [h.cpu() for h in outputs.hidden_states],\n",
        "        \"n_layers\": len(outputs.attentions), \"n_heads\": outputs.attentions[0].size(1),\n",
        "        \"seq_len\": input_ids.size(1)\n",
        "    }\n",
        "\n",
        "print(\"Coherence functions defined:\")\n",
        "print(\"  - compute_wave_coherence_v1() [BROKEN]\")\n",
        "print(\"  - compute_wave_coherence_weighted() [FIX 1]\")\n",
        "print(\"  - compute_spectral_concentration() [FIX 2]\")\n",
        "print(\"  - compute_interference_strength() [FIX 3]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Verify inverted frequency mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize inverted frequency mapping\n",
        "test_text = \"The quantum mechanical wave function describes probability\"\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(test_text)\n",
        "\n",
        "print(f\"Text: '{test_text}'\")\n",
        "print(f\"\\nToken frequencies (INVERTED):\")\n",
        "for token, freq in sorted(zip(tokens, frequencies), key=lambda x: -x[1]):\n",
        "    rank = word_freq_table.get(token.strip().lower(), wave_encoder.max_rank)\n",
        "    print(f\"  '{token:15s}' rank={rank:6d} -> {freq:.2f} Hz\")\n",
        "\n",
        "fig, axes = plt.subplots(len(tokens), 1, figsize=(14, 2 * len(tokens)), sharex=True)\n",
        "for i, (ax, token, freq, wave) in enumerate(zip(axes, tokens, frequencies, waves)):\n",
        "    color = cm.viridis(freq / wave_config.freq_max)\n",
        "    ax.plot(wave_encoder.t, wave.real, color=color, linewidth=1)\n",
        "    ax.set_ylabel(f\"{token.strip()}\\n{freq:.1f}Hz\", fontsize=9, rotation=0, ha='right', va='center')\n",
        "    ax.set_ylim(-2, 2)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(0, 0.5)\n",
        "axes[-1].set_xlabel(\"Time (s)\")\n",
        "plt.suptitle(\"INVERTED: Common words = HIGH freq (bright), Rare = LOW freq (dark)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"01_inverted_token_waves.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Compare coherence measures through layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all coherence measures through layers\n",
        "prompt = \"The ancient library contained books about quantum mechanics and philosophy\"\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "print(f\"Tokens: {len(tokens)}\")\n",
        "\n",
        "coherence_v1, coherence_weighted, spectral_conc, interference = [], [], [], []\n",
        "\n",
        "for layer_idx in range(result['n_layers']):\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    \n",
        "    coherence_v1.append(compute_wave_coherence_v1(waves * query_attn[:, None]))\n",
        "    coherence_weighted.append(compute_wave_coherence_weighted(waves, query_attn))\n",
        "    spectral_conc.append(compute_spectral_concentration(waves, query_attn, wave_config.sample_rate))\n",
        "    interference.append(compute_interference_strength(waves, query_attn))\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "axes[0, 0].plot(range(n_layers), coherence_v1, 'o--', color='gray', linewidth=2, label='v1 (BROKEN)', alpha=0.5)\n",
        "axes[0, 0].set_xlabel(\"Layer\"); axes[0, 0].set_ylabel(\"Coherence\")\n",
        "axes[0, 0].set_title(\"Original Coherence (doesn't change)\"); axes[0, 0].grid(True, alpha=0.3); axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].plot(range(n_layers), coherence_weighted, 'o-', color='teal', linewidth=2, label='Weighted')\n",
        "axes[0, 1].set_xlabel(\"Layer\"); axes[0, 1].set_ylabel(\"Coherence\")\n",
        "axes[0, 1].set_title(\"FIX 1: Weighted Coherence\"); axes[0, 1].grid(True, alpha=0.3); axes[0, 1].legend()\n",
        "\n",
        "axes[1, 0].plot(range(n_layers), spectral_conc, 's-', color='purple', linewidth=2, label='Spectral')\n",
        "axes[1, 0].set_xlabel(\"Layer\"); axes[1, 0].set_ylabel(\"Concentration\")\n",
        "axes[1, 0].set_title(\"FIX 2: Spectral Concentration\"); axes[1, 0].grid(True, alpha=0.3); axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].plot(range(n_layers), interference, '^-', color='coral', linewidth=2, label='Interference')\n",
        "axes[1, 1].axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='No interference')\n",
        "axes[1, 1].set_xlabel(\"Layer\"); axes[1, 1].set_ylabel(\"Interference Ratio\")\n",
        "axes[1, 1].set_title(\"FIX 3: Interference (>1=constructive)\"); axes[1, 1].grid(True, alpha=0.3); axes[1, 1].legend()\n",
        "\n",
        "plt.suptitle(f\"Coherence Measures Comparison\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"02_coherence_comparison.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nv1 (BROKEN):  {coherence_v1[0]:.4f} -> {coherence_v1[-1]:.4f} (change: {coherence_v1[-1]-coherence_v1[0]:.4f})\")\n",
        "print(f\"Weighted:     {coherence_weighted[0]:.4f} -> {coherence_weighted[-1]:.4f} (change: {coherence_weighted[-1]-coherence_weighted[0]:.4f})\")\n",
        "print(f\"Spectral:     {spectral_conc[0]:.4f} -> {spectral_conc[-1]:.4f} (change: {spectral_conc[-1]-spectral_conc[0]:.4f})\")\n",
        "print(f\"Interference: {interference[0]:.4f} -> {interference[-1]:.4f} (change: {interference[-1]-interference[0]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: Multi-prompt analysis with fixed measures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test prompts\n",
        "test_prompts = {\n",
        "    \"factual_1\": \"The capital of France is\",\n",
        "    \"factual_2\": \"Water boils at one hundred degrees\",\n",
        "    \"factual_3\": \"The chemical symbol for gold is\",\n",
        "    \"factual_4\": \"The Earth orbits around the\",\n",
        "    \"factual_5\": \"The speed of light in vacuum is approximately\",\n",
        "    \n",
        "    \"narrative_1\": \"She opened the door and saw\",\n",
        "    \"narrative_2\": \"The old man walked slowly towards the\",\n",
        "    \"narrative_3\": \"In the darkness of the forest, something moved\",\n",
        "    \"narrative_4\": \"After years of searching, he finally found the\",\n",
        "    \n",
        "    \"technical_1\": \"The quantum mechanical wave function describes probability amplitudes\",\n",
        "    \"technical_2\": \"In machine learning, gradient descent optimizes the loss function by\",\n",
        "    \"technical_3\": \"The transformer architecture uses self-attention to process sequences\",\n",
        "    \"technical_4\": \"Photosynthesis converts carbon dioxide and water into glucose using\",\n",
        "    \n",
        "    \"philosophical_1\": \"The meaning of existence is\",\n",
        "    \"philosophical_2\": \"When considering the nature of consciousness,\",\n",
        "    \"philosophical_3\": \"The relationship between mind and matter suggests that\",\n",
        "}\n",
        "print(f\"Testing {len(test_prompts)} prompts...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run analysis on all prompts\n",
        "all_results = {}\n",
        "\n",
        "for name, prompt in test_prompts.items():\n",
        "    print(f\"Processing: {name}...\")\n",
        "    \n",
        "    result = run_inference(prompt)\n",
        "    tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "    \n",
        "    coh_v1, coh_weighted, spec_conc, interf = [], [], [], []\n",
        "    for layer_idx in range(result['n_layers']):\n",
        "        attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "        query_attn = attn[-1, :]\n",
        "        coh_v1.append(compute_wave_coherence_v1(waves * query_attn[:, None]))\n",
        "        coh_weighted.append(compute_wave_coherence_weighted(waves, query_attn))\n",
        "        spec_conc.append(compute_spectral_concentration(waves, query_attn, wave_config.sample_rate))\n",
        "        interf.append(compute_interference_strength(waves, query_attn))\n",
        "    \n",
        "    logits = result[\"logits\"][0, -1, :]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    entropy = -(probs * probs.clamp(min=1e-10).log()).sum().item()\n",
        "    top_token = tokenizer.decode([probs.argmax().item()])\n",
        "    \n",
        "    all_results[name] = {\n",
        "        \"prompt\": prompt, \"tokens\": tokens, \"frequencies\": frequencies,\n",
        "        \"mean_freq\": float(np.mean(frequencies)),\n",
        "        \"coherence_v1\": coh_v1, \"coherence_weighted\": coh_weighted,\n",
        "        \"spectral_concentration\": spec_conc, \"interference\": interf,\n",
        "        \"output_entropy\": entropy, \"top_token\": top_token, \"n_tokens\": len(tokens)\n",
        "    }\n",
        "\n",
        "print(\"\\nAll prompts processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*120)\n",
        "print(f\"{'Prompt':<55} {'Tok':>4} {'MeanF':>6} {'Top':>10} {'Ent':>6} {'Coh_w':>7} {'Spec':>7} {'Inter':>7}\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "for name, r in all_results.items():\n",
        "    final_coh_w = r[\"coherence_weighted\"][-1]\n",
        "    final_spec = r[\"spectral_concentration\"][-1]\n",
        "    final_inter = r[\"interference\"][-1]\n",
        "    print(f\"{r['prompt'][:53]:<55} {r['n_tokens']:>4} {r['mean_freq']:>6.2f} {r['top_token']:>10} {r['output_entropy']:>6.2f} {final_coh_w:>7.4f} {final_spec:>7.4f} {final_inter:>7.4f}\")\n",
        "\n",
        "print(\"=\"*120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare prompt types with fixed measures\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "groups = {\n",
        "    \"Factual\": [k for k in all_results.keys() if k.startswith(\"factual\")],\n",
        "    \"Narrative\": [k for k in all_results.keys() if k.startswith(\"narrative\")],\n",
        "    \"Technical\": [k for k in all_results.keys() if k.startswith(\"technical\")],\n",
        "    \"Philosophical\": [k for k in all_results.keys() if k.startswith(\"philosophical\")]\n",
        "}\n",
        "colors = {\"Factual\": \"blue\", \"Narrative\": \"green\", \"Technical\": \"purple\", \"Philosophical\": \"red\"}\n",
        "\n",
        "# Weighted coherence by group\n",
        "ax1 = axes[0, 0]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    for pname in prompt_names:\n",
        "        ax1.plot(range(n_layers), all_results[pname][\"coherence_weighted\"], \n",
        "                 color=colors[group_name], alpha=0.3, linewidth=1)\n",
        "    mean_coh = np.mean([all_results[pname][\"coherence_weighted\"] for pname in prompt_names], axis=0)\n",
        "    ax1.plot(range(n_layers), mean_coh, color=colors[group_name], linewidth=3, label=group_name)\n",
        "ax1.set_xlabel(\"Layer\"); ax1.set_ylabel(\"Weighted Coherence\")\n",
        "ax1.set_title(\"Weighted Coherence by Prompt Type\"); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Spectral concentration by group\n",
        "ax2 = axes[0, 1]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    mean_spec = np.mean([all_results[pname][\"spectral_concentration\"] for pname in prompt_names], axis=0)\n",
        "    ax2.plot(range(n_layers), mean_spec, 'o-', color=colors[group_name], linewidth=2, label=group_name)\n",
        "ax2.set_xlabel(\"Layer\"); ax2.set_ylabel(\"Spectral Concentration\")\n",
        "ax2.set_title(\"Spectral Concentration by Type\"); ax2.legend(); ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Mean frequency vs final coherence\n",
        "ax3 = axes[1, 0]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    mean_freqs = [all_results[pname][\"mean_freq\"] for pname in prompt_names]\n",
        "    final_cohs = [all_results[pname][\"coherence_weighted\"][-1] for pname in prompt_names]\n",
        "    ax3.scatter(mean_freqs, final_cohs, s=100, c=colors[group_name], alpha=0.7, label=group_name, edgecolors='black')\n",
        "corr = np.corrcoef([r[\"mean_freq\"] for r in all_results.values()],\n",
        "                   [r[\"coherence_weighted\"][-1] for r in all_results.values()])[0, 1]\n",
        "ax3.annotate(f\"r = {corr:.3f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\", fontsize=11)\n",
        "ax3.set_xlabel(\"Mean Zipf-Wave Frequency (Hz)\"); ax3.set_ylabel(\"Final Weighted Coherence\")\n",
        "ax3.set_title(\"Token Frequency vs Coherence\"); ax3.legend(); ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Entropy vs interference\n",
        "ax4 = axes[1, 1]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    entropies = [all_results[pname][\"output_entropy\"] for pname in prompt_names]\n",
        "    interferences = [all_results[pname][\"interference\"][-1] for pname in prompt_names]\n",
        "    ax4.scatter(entropies, interferences, s=100, c=colors[group_name], alpha=0.7, label=group_name, edgecolors='black')\n",
        "ax4.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "ax4.set_xlabel(\"Output Entropy\"); ax4.set_ylabel(\"Interference Strength\")\n",
        "ax4.set_title(\"Entropy vs Wave Interference\"); ax4.legend(); ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Phase Dynamics with Fixed Measures (INVERTED Zipf)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"03_prompt_comparison_v2.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4: Detailed wave interference visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed wave interference for technical prompt\n",
        "prompt = \"The quantum mechanical wave function describes probability amplitudes\"\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"\\nToken frequencies (INVERTED):\")\n",
        "for token, freq in zip(tokens, frequencies):\n",
        "    print(f\"  '{token.strip():15s}' -> {freq:.2f} Hz\")\n",
        "\n",
        "fig = plt.figure(figsize=(18, 16))\n",
        "gs = GridSpec(5, 3, figure=fig)\n",
        "\n",
        "# Token waves\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "t_show = wave_encoder.t[:50]\n",
        "for i, (token, wave, freq) in enumerate(zip(tokens, waves, frequencies)):\n",
        "    offset = i * 0.3\n",
        "    color = cm.plasma(freq / wave_config.freq_max)\n",
        "    ax1.plot(t_show, wave.real[:50] * 0.1 + offset, color=color, linewidth=1)\n",
        "    ax1.text(-0.02, offset, f\"{token.strip()[:10]} ({freq:.1f})\", ha='right', fontsize=8, va='center')\n",
        "ax1.set_xlabel(\"Time (s)\")\n",
        "ax1.set_title(\"Token Waves (INVERTED: common=bright/fast, rare=dark/slow)\")\n",
        "ax1.set_xlim(-0.15, t_show[-1])\n",
        "\n",
        "# Attention patterns\n",
        "for col, layer_idx in enumerate([0, 5, 11]):\n",
        "    ax = fig.add_subplot(gs[1, col])\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    im = ax.imshow(attn, cmap='Blues', aspect='auto')\n",
        "    ax.set_title(f\"Layer {layer_idx} Attention\")\n",
        "    ax.set_xlabel(\"Key\"); ax.set_ylabel(\"Query\")\n",
        "    plt.colorbar(im, ax=ax, shrink=0.7)\n",
        "\n",
        "# Superposed waves\n",
        "for col, layer_idx in enumerate([0, 5, 11]):\n",
        "    ax = fig.add_subplot(gs[2, col])\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    coh_w = compute_wave_coherence_weighted(waves, query_attn)\n",
        "    ax.plot(t_show, superposed.real[:50], 'b-', linewidth=1.5)\n",
        "    ax.fill_between(t_show, -np.abs(superposed[:50]), np.abs(superposed[:50]), alpha=0.2, color='blue')\n",
        "    ax.set_title(f\"Layer {layer_idx} Superposition (Coh={coh_w:.3f})\")\n",
        "    ax.set_xlabel(\"Time (s)\"); ax.grid(True, alpha=0.3)\n",
        "\n",
        "# FFT spectra\n",
        "for col, layer_idx in enumerate([0, 5, 11]):\n",
        "    ax = fig.add_subplot(gs[3, col])\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    fft_vals = np.abs(fft(superposed.real))\n",
        "    freqs = fftfreq(len(superposed), d=1/wave_config.sample_rate)\n",
        "    pos_mask = freqs > 0\n",
        "    spec_conc = compute_spectral_concentration(waves, query_attn, wave_config.sample_rate)\n",
        "    ax.plot(freqs[pos_mask][:50], fft_vals[pos_mask][:50], 'g-', linewidth=1.5)\n",
        "    ax.set_xlabel(\"Frequency (Hz)\"); ax.set_ylabel(\"Magnitude\")\n",
        "    ax.set_title(f\"Layer {layer_idx} FFT (Conc={spec_conc:.3f})\"); ax.grid(True, alpha=0.3)\n",
        "\n",
        "# All measures through layers\n",
        "ax = fig.add_subplot(gs[4, :])\n",
        "coh_w_all, spec_all, inter_all = [], [], []\n",
        "for layer_idx in range(n_layers):\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    coh_w_all.append(compute_wave_coherence_weighted(waves, query_attn))\n",
        "    spec_all.append(compute_spectral_concentration(waves, query_attn, wave_config.sample_rate))\n",
        "    inter_all.append(compute_interference_strength(waves, query_attn))\n",
        "\n",
        "ax.plot(range(n_layers), coh_w_all, 'o-', color='teal', linewidth=2, label='Weighted Coherence')\n",
        "ax.plot(range(n_layers), spec_all, 's-', color='purple', linewidth=2, label='Spectral Conc.')\n",
        "ax.plot(range(n_layers), inter_all, '^-', color='coral', linewidth=2, label='Interference')\n",
        "ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Value\")\n",
        "ax.set_title(\"All Measures Through Layers\"); ax.legend(); ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f\"Wave Interference Analysis: '{prompt[:50]}...'\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"04_wave_interference_detailed_v2.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 5: Per-head wave interference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-head wave interference analysis\n",
        "prompt = \"The ancient library contained books about quantum mechanics and philosophy\"\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "\n",
        "print(f\"Analyzing per-head interference for: '{prompt[:50]}...'\")\n",
        "print(f\"Model has {n_heads} heads per layer\")\n",
        "\n",
        "# Compute coherence per head per layer\n",
        "head_coherences = np.zeros((n_layers, n_heads))\n",
        "head_interferences = np.zeros((n_layers, n_heads))\n",
        "\n",
        "for layer_idx in range(n_layers):\n",
        "    attn = result[\"attentions\"][layer_idx][0].numpy()  # [n_heads, seq, seq]\n",
        "    for head_idx in range(n_heads):\n",
        "        head_attn = attn[head_idx, -1, :]  # Last token's attention for this head\n",
        "        head_coherences[layer_idx, head_idx] = compute_wave_coherence_weighted(waves, head_attn)\n",
        "        head_interferences[layer_idx, head_idx] = compute_interference_strength(waves, head_attn)\n",
        "\n",
        "# Plot heatmaps\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "im1 = axes[0].imshow(head_coherences, aspect='auto', cmap='viridis')\n",
        "axes[0].set_xlabel(\"Head\"); axes[0].set_ylabel(\"Layer\")\n",
        "axes[0].set_title(\"Weighted Coherence per Head\"); plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "im2 = axes[1].imshow(head_interferences, aspect='auto', cmap='coolwarm', vmin=0.5, vmax=1.5)\n",
        "axes[1].set_xlabel(\"Head\"); axes[1].set_ylabel(\"Layer\")\n",
        "axes[1].set_title(\"Interference Strength per Head (1.0 = no interference)\"); plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "plt.suptitle(\"Per-Head Wave Dynamics\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"05_per_head_coherence.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Find most/least coherent heads\n",
        "max_idx = np.unravel_index(head_coherences.argmax(), head_coherences.shape)\n",
        "min_idx = np.unravel_index(head_coherences.argmin(), head_coherences.shape)\n",
        "print(f\"\\nMost coherent: Layer {max_idx[0]}, Head {max_idx[1]} (coh={head_coherences[max_idx]:.4f})\")\n",
        "print(f\"Least coherent: Layer {min_idx[0]}, Head {min_idx[1]} (coh={head_coherences[min_idx]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 6: Wave evolution through all 12 layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wave evolution through all 12 layers\n",
        "prompt = \"The capital of France is\"  # Factual prompt\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Top prediction: {tokenizer.decode([result['logits'][0, -1].argmax().item()])}\")\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
        "t_show = wave_encoder.t[:100]\n",
        "\n",
        "for layer_idx in range(12):\n",
        "    row, col = layer_idx // 4, layer_idx % 4\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    \n",
        "    coh_w = compute_wave_coherence_weighted(waves, query_attn)\n",
        "    inter = compute_interference_strength(waves, query_attn)\n",
        "    \n",
        "    ax.plot(t_show, superposed.real[:100], 'b-', linewidth=1.2, alpha=0.8)\n",
        "    ax.fill_between(t_show, -np.abs(superposed[:100]), np.abs(superposed[:100]), alpha=0.15, color='blue')\n",
        "    ax.set_title(f\"Layer {layer_idx}: Coh={coh_w:.3f}, Int={inter:.2f}\", fontsize=10)\n",
        "    ax.set_xlim(0, t_show[-1])\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    if row == 2:\n",
        "        ax.set_xlabel(\"Time (s)\")\n",
        "    if col == 0:\n",
        "        ax.set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.suptitle(f\"Wave Evolution: '{prompt}' -> Top: {tokenizer.decode([result['logits'][0, -1].argmax().item()])}\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"06_wave_evolution_factual.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same for open-ended prompt\n",
        "prompt = \"The meaning of existence is\"  # Philosophical prompt\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Top prediction: {tokenizer.decode([result['logits'][0, -1].argmax().item()])}\")\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
        "t_show = wave_encoder.t[:100]\n",
        "\n",
        "for layer_idx in range(12):\n",
        "    row, col = layer_idx // 4, layer_idx % 4\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    \n",
        "    coh_w = compute_wave_coherence_weighted(waves, query_attn)\n",
        "    inter = compute_interference_strength(waves, query_attn)\n",
        "    \n",
        "    ax.plot(t_show, superposed.real[:100], 'r-', linewidth=1.2, alpha=0.8)\n",
        "    ax.fill_between(t_show, -np.abs(superposed[:100]), np.abs(superposed[:100]), alpha=0.15, color='red')\n",
        "    ax.set_title(f\"Layer {layer_idx}: Coh={coh_w:.3f}, Int={inter:.2f}\", fontsize=10)\n",
        "    ax.set_xlim(0, t_show[-1])\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    if row == 2:\n",
        "        ax.set_xlabel(\"Time (s)\")\n",
        "    if col == 0:\n",
        "        ax.set_ylabel(\"Amplitude\")\n",
        "\n",
        "plt.suptitle(f\"Wave Evolution: '{prompt}' -> Top: {tokenizer.decode([result['logits'][0, -1].argmax().item()])}\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"07_wave_evolution_philosophical.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "summary = {\n",
        "    \"model\": model_name,\n",
        "    \"n_layers\": n_layers,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"wave_config\": {\n",
        "        \"freq_min\": wave_config.freq_min,\n",
        "        \"freq_max\": wave_config.freq_max,\n",
        "        \"n_harmonics\": wave_config.n_harmonics,\n",
        "        \"mapping\": \"INVERTED: common words = HIGH freq, rare words = LOW freq\"\n",
        "    },\n",
        "    \"results\": []\n",
        "}\n",
        "\n",
        "for name, r in all_results.items():\n",
        "    summary[\"results\"].append({\n",
        "        \"name\": name,\n",
        "        \"prompt\": r[\"prompt\"],\n",
        "        \"n_tokens\": r[\"n_tokens\"],\n",
        "        \"mean_frequency\": r[\"mean_freq\"],\n",
        "        \"output_entropy\": r[\"output_entropy\"],\n",
        "        \"top_token\": r[\"top_token\"],\n",
        "        \"coherence_weighted_layer_0\": r[\"coherence_weighted\"][0],\n",
        "        \"coherence_weighted_layer_final\": r[\"coherence_weighted\"][-1],\n",
        "        \"spectral_concentration_layer_0\": r[\"spectral_concentration\"][0],\n",
        "        \"spectral_concentration_layer_final\": r[\"spectral_concentration\"][-1],\n",
        "        \"interference_layer_0\": r[\"interference\"][0],\n",
        "        \"interference_layer_final\": r[\"interference\"][-1],\n",
        "        \"coherence_weighted_all\": r[\"coherence_weighted\"],\n",
        "        \"spectral_concentration_all\": r[\"spectral_concentration\"],\n",
        "        \"interference_all\": r[\"interference\"]\n",
        "    })\n",
        "\n",
        "with open(FIG_DIR / \"wave_phase_results_v2.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {FIG_DIR / 'wave_phase_results_v2.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key fixes in v2\n",
        "\n",
        "1. **INVERTED frequency mapping**:\n",
        "   - Common words (\"the\", \"is\") -> HIGH frequency (10 Hz)\n",
        "   - Rare words (\"quantum\", \"crystallization\") -> LOW frequency (0.5 Hz)\n",
        "\n",
        "2. **Fixed coherence measures**:\n",
        "   - `compute_wave_coherence_weighted()`: Preserves attention influence\n",
        "   - `compute_spectral_concentration()`: Measures FFT spectrum concentration\n",
        "   - `compute_interference_strength()`: Measures constructive/destructive interference\n",
        "\n",
        "3. **Why the original was broken**:\n",
        "   - Attention scaled amplitude, but we normalized amplitude away\n",
        "   - Phase relationships were fixed at encoding, not affected by attention\n",
        "   - Result: coherence was constant across all layers (no signal)\n",
        "\n",
        "### Figures generated\n",
        "\n",
        "1. `01_inverted_token_waves.png` - Token wave visualization\n",
        "2. `02_coherence_comparison.png` - Compare v1 (broken) vs fixed measures\n",
        "3. `03_prompt_comparison_v2.png` - Multi-prompt analysis\n",
        "4. `04_wave_interference_detailed_v2.png` - Detailed interference visualization\n",
        "5. `05_per_head_coherence.png` - Per-head coherence heatmaps\n",
        "6. `06_wave_evolution_factual.png` - 12-layer evolution (factual prompt)\n",
        "7. `07_wave_evolution_philosophical.png` - 12-layer evolution (philosophical prompt)\n",
        "\n",
        "### Connection to AKIRA theory\n",
        "\n",
        "- **RADAR_ARRAY.md**: Spectral decomposition of signals, frequency bands\n",
        "- **HARMONY_AND_COHERENCE.md**: Phase locking, belief collapse as phase transition\n",
        "- **ACTION_QUANTA.md**: Minimum actionable patterns, crystallization\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
