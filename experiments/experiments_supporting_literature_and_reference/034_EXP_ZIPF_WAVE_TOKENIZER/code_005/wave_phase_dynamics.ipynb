{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wave Phase Dynamics in Transformer Attention\n",
        "\n",
        "This experiment improves on code_004's phase measurement by:\n",
        "\n",
        "1. **True wave representation**: Tokens mapped to oscillating waves via Zipf frequency\n",
        "2. **FFT-based phase extraction**: Phase from actual frequency components\n",
        "3. **Hilbert transform**: Instantaneous phase of activation envelopes\n",
        "4. **Wave superposition visualization**: How waves combine through attention\n",
        "\n",
        "## The phase problem in code_004\n",
        "\n",
        "Code_004 used position-to-angle mapping:\n",
        "```\n",
        "phase = atan2(Σ attn × sin(2π × pos/N), Σ attn × cos(2π × pos/N))\n",
        "```\n",
        "\n",
        "This measures WHERE attention points, not actual oscillatory phase.\n",
        "\n",
        "## Better approach\n",
        "\n",
        "If we treat tokens as waves with frequencies from Zipf rank:\n",
        "- Common words (the, is, a) → LOW frequency (slow oscillation)\n",
        "- Rare words (quantum, crystallization) → HIGH frequency (fast oscillation)\n",
        "\n",
        "Then phase emerges naturally from wave interference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# !pip install transformers torch matplotlib numpy scipy datasets\n",
        "\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib import cm\n",
        "from scipy.signal import hilbert\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "# Transformer model\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# For wikitext vocabulary\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    HAS_DATASETS = True\n",
        "except ImportError:\n",
        "    HAS_DATASETS = False\n",
        "    print(\"datasets not available, using fallback word frequencies\")\n",
        "\n",
        "# Figure directory\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "FIG_DIR = NOTEBOOK_DIR / \"figs_wave\"\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2\n",
        "model_name = \"gpt2\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.n_layer\n",
        "n_heads = model.config.n_head\n",
        "d_model = model.config.n_embd\n",
        "print(f\"Model: {n_layers} layers, {n_heads} heads, d_model={d_model}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build word frequency table from wikitext-2 or fallback\n",
        "\n",
        "def build_word_frequency_table(tokenizer, max_words: int = 50000) -> Dict[str, int]:\n",
        "    \"\"\"Build word frequency ranking from wikitext-2 or GPT-2 vocab.\"\"\"\n",
        "    \n",
        "    if HAS_DATASETS:\n",
        "        print(\"Loading wikitext-2 for frequency estimation...\")\n",
        "        try:\n",
        "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "            \n",
        "            # Count words\n",
        "            word_counts = Counter()\n",
        "            for example in dataset:\n",
        "                text = example[\"text\"]\n",
        "                if text.strip():\n",
        "                    words = text.lower().split()\n",
        "                    word_counts.update(words)\n",
        "            \n",
        "            # Rank by frequency\n",
        "            ranked = word_counts.most_common(max_words)\n",
        "            word_rank = {word: rank + 1 for rank, (word, _) in enumerate(ranked)}\n",
        "            print(f\"Built frequency table with {len(word_rank)} words from wikitext-2\")\n",
        "            return word_rank\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load wikitext-2: {e}\")\n",
        "    \n",
        "    # Fallback: use GPT-2 vocabulary order as proxy for frequency\n",
        "    print(\"Using GPT-2 vocab order as frequency proxy...\")\n",
        "    word_rank = {}\n",
        "    for token_id in range(tokenizer.vocab_size):\n",
        "        token = tokenizer.decode([token_id]).strip().lower()\n",
        "        if token and token not in word_rank:\n",
        "            word_rank[token] = len(word_rank) + 1\n",
        "    \n",
        "    print(f\"Built frequency table with {len(word_rank)} tokens\")\n",
        "    return word_rank\n",
        "\n",
        "word_freq_table = build_word_frequency_table(tokenizer)\n",
        "\n",
        "# Show some examples\n",
        "common = sorted(word_freq_table.items(), key=lambda x: x[1])[:10]\n",
        "rare = sorted(word_freq_table.items(), key=lambda x: x[1])[-10:]\n",
        "print(f\"\\nMost common: {common}\")\n",
        "print(f\"Most rare: {rare}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wave frequency assignment based on Zipf rank\n",
        "\n",
        "@dataclass\n",
        "class WaveConfig:\n",
        "    \"\"\"Configuration for wave-based token representation.\"\"\"\n",
        "    freq_min: float = 0.1      # Minimum wave frequency (common words)\n",
        "    freq_max: float = 10.0     # Maximum wave frequency (rare words)\n",
        "    n_harmonics: int = 4       # Number of harmonics per token\n",
        "    sample_rate: int = 100     # Samples per unit time\n",
        "    duration: float = 2.0      # Wave duration in time units\n",
        "\n",
        "class ZipfWaveEncoder:\n",
        "    \"\"\"Encode tokens as waves with frequency determined by Zipf rank.\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer, word_freq_table: Dict[str, int], config: WaveConfig):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word_freq_table = word_freq_table\n",
        "        self.config = config\n",
        "        self.max_rank = max(word_freq_table.values()) if word_freq_table else 50000\n",
        "        \n",
        "        # Time axis\n",
        "        n_samples = int(config.sample_rate * config.duration)\n",
        "        self.t = np.linspace(0, config.duration, n_samples)\n",
        "    \n",
        "    def token_to_frequency(self, token: str) -> float:\n",
        "        \"\"\"Map token to wave frequency via Zipf rank.\n",
        "        \n",
        "        Common words -> low frequency (slow wave)\n",
        "        Rare words -> high frequency (fast wave)\n",
        "        \"\"\"\n",
        "        token_clean = token.strip().lower()\n",
        "        rank = self.word_freq_table.get(token_clean, self.max_rank)\n",
        "        \n",
        "        # Log-scale mapping: rank 1 -> freq_min, rank max -> freq_max\n",
        "        log_rank = np.log(rank + 1)\n",
        "        log_max = np.log(self.max_rank + 1)\n",
        "        normalized = log_rank / log_max\n",
        "        \n",
        "        freq = self.config.freq_min + (self.config.freq_max - self.config.freq_min) * normalized\n",
        "        return freq\n",
        "    \n",
        "    def token_to_wave(self, token: str, phase_offset: float = 0.0) -> np.ndarray:\n",
        "        \"\"\"Generate wave signal for a token. Returns complex wave with harmonics.\"\"\"\n",
        "        freq = self.token_to_frequency(token)\n",
        "        \n",
        "        # Sum of harmonics with decreasing amplitude\n",
        "        wave = np.zeros_like(self.t, dtype=np.complex128)\n",
        "        for h in range(1, self.config.n_harmonics + 1):\n",
        "            amplitude = 1.0 / h  # Harmonic series decay\n",
        "            harmonic_freq = freq * h\n",
        "            wave += amplitude * np.exp(1j * 2 * np.pi * harmonic_freq * self.t + 1j * phase_offset * h)\n",
        "        \n",
        "        return wave\n",
        "    \n",
        "    def encode_sequence(self, text: str) -> Tuple[List[str], np.ndarray, np.ndarray]:\n",
        "        \"\"\"Encode text as sequence of waves.\"\"\"\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "        \n",
        "        frequencies = np.array([self.token_to_frequency(t) for t in tokens])\n",
        "        \n",
        "        # Generate waves with position-dependent phase offset\n",
        "        waves = np.zeros((len(tokens), len(self.t)), dtype=np.complex128)\n",
        "        for i, token in enumerate(tokens):\n",
        "            phase_offset = 2 * np.pi * i / len(tokens)  # Spread phases\n",
        "            waves[i] = self.token_to_wave(token, phase_offset)\n",
        "        \n",
        "        return tokens, frequencies, waves\n",
        "\n",
        "# Create encoder\n",
        "wave_config = WaveConfig()\n",
        "wave_encoder = ZipfWaveEncoder(tokenizer, word_freq_table, wave_config)\n",
        "\n",
        "print(f\"Wave encoder ready: freq range [{wave_config.freq_min}, {wave_config.freq_max}] Hz\")\n",
        "print(f\"Time samples: {len(wave_encoder.t)} over {wave_config.duration}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase extraction functions\n",
        "\n",
        "def extract_fft_phase(signal: np.ndarray, n_components: int = 5) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Extract phase from dominant FFT components.\"\"\"\n",
        "    n = len(signal)\n",
        "    fft_vals = fft(signal)\n",
        "    freqs_all = fftfreq(n, d=1.0)\n",
        "    \n",
        "    pos_mask = freqs_all > 0\n",
        "    freqs_pos = freqs_all[pos_mask]\n",
        "    fft_pos = fft_vals[pos_mask]\n",
        "    \n",
        "    magnitudes_all = np.abs(fft_pos)\n",
        "    top_indices = np.argsort(magnitudes_all)[-n_components:]\n",
        "    \n",
        "    freqs = freqs_pos[top_indices]\n",
        "    magnitudes = magnitudes_all[top_indices]\n",
        "    phases = np.angle(fft_pos[top_indices])\n",
        "    \n",
        "    return freqs, magnitudes, phases\n",
        "\n",
        "\n",
        "def extract_hilbert_phase(signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Extract instantaneous phase via Hilbert transform.\"\"\"\n",
        "    analytic = hilbert(signal)\n",
        "    inst_amplitude = np.abs(analytic)\n",
        "    inst_phase = np.unwrap(np.angle(analytic))\n",
        "    return inst_phase, inst_amplitude\n",
        "\n",
        "\n",
        "def compute_wave_coherence(waves: np.ndarray) -> float:\n",
        "    \"\"\"Compute phase coherence across waves. R = |mean(exp(i * phase))|.\"\"\"\n",
        "    amplitudes = np.abs(waves)\n",
        "    amplitudes[amplitudes < 1e-10] = 1e-10\n",
        "    normalized = waves / amplitudes\n",
        "    mean_phasor = normalized.mean(axis=0)\n",
        "    coherence = np.abs(mean_phasor).mean()\n",
        "    return coherence\n",
        "\n",
        "\n",
        "def run_inference(text: str) -> Dict:\n",
        "    \"\"\"Run GPT-2 inference and extract all internals.\"\"\"\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "    tokens = [tokenizer.decode([tid]) for tid in input_ids[0]]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_attentions=True, output_hidden_states=True)\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"tokens\": tokens,\n",
        "        \"logits\": outputs.logits.cpu(),\n",
        "        \"attentions\": [a.cpu() for a in outputs.attentions],\n",
        "        \"hidden_states\": [h.cpu() for h in outputs.hidden_states],\n",
        "        \"n_layers\": len(outputs.attentions),\n",
        "        \"n_heads\": outputs.attentions[0].size(1),\n",
        "        \"seq_len\": input_ids.size(1)\n",
        "    }\n",
        "\n",
        "print(\"Phase extraction and inference functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Token Wave Visualization\n",
        "\n",
        "Visualize individual token waves based on their Zipf frequency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize wave representation for a sample sentence\n",
        "test_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(test_text)\n",
        "\n",
        "print(f\"Text: '{test_text}'\")\n",
        "print(f\"\\nToken frequencies (Zipf-based):\")\n",
        "for token, freq in zip(tokens, frequencies):\n",
        "    print(f\"  '{token:15s}' -> {freq:.2f} Hz\")\n",
        "\n",
        "# Plot waves\n",
        "fig, axes = plt.subplots(len(tokens), 1, figsize=(14, 2 * len(tokens)), sharex=True)\n",
        "\n",
        "for i, (ax, token, freq, wave) in enumerate(zip(axes, tokens, frequencies, waves)):\n",
        "    ax.plot(wave_encoder.t, wave.real, color=cm.viridis(freq / wave_config.freq_max), linewidth=1)\n",
        "    ax.set_ylabel(f\"{token.strip()}\\n{freq:.1f}Hz\", fontsize=9, rotation=0, ha='right', va='center')\n",
        "    ax.set_ylim(-2, 2)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(0, 0.5)  # Show first 0.5s\n",
        "\n",
        "axes[-1].set_xlabel(\"Time (s)\")\n",
        "plt.suptitle(f\"Zipf-Wave Token Representation: '{test_text}'\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"01_token_waves.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Wave Superposition Through Attention Layers\n",
        "\n",
        "See how waves combine via attention at each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Longer prompt for better analysis\n",
        "prompt = \"The ancient library contained books about quantum mechanics and philosophy\"\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "\n",
        "print(f\"Tokens: {len(tokens)}\")\n",
        "print(f\"Layers: {result['n_layers']}, Heads: {result['n_heads']}\")\n",
        "\n",
        "# Visualize wave superposition at selected layers\n",
        "layers_to_show = [0, 3, 6, 9, 11]\n",
        "query_pos = -1  # Last token\n",
        "\n",
        "fig, axes = plt.subplots(len(layers_to_show), 2, figsize=(16, 3 * len(layers_to_show)))\n",
        "\n",
        "coherences_by_layer = []\n",
        "\n",
        "for row, layer_idx in enumerate(layers_to_show):\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[query_pos, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    coherence = compute_wave_coherence(waves * query_attn[:, None])\n",
        "    coherences_by_layer.append(coherence)\n",
        "    \n",
        "    # Attention pattern\n",
        "    ax1 = axes[row, 0]\n",
        "    ax1.bar(range(len(tokens)), query_attn, color='steelblue', alpha=0.7)\n",
        "    ax1.set_xticks(range(len(tokens)))\n",
        "    ax1.set_xticklabels([t.strip()[:8] for t in tokens], rotation=45, ha='right', fontsize=8)\n",
        "    ax1.set_ylabel(\"Attention\")\n",
        "    ax1.set_title(f\"Layer {layer_idx}: Attention (query=last)\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Superposed wave\n",
        "    ax2 = axes[row, 1]\n",
        "    t_show = wave_encoder.t[:100]\n",
        "    ax2.plot(t_show, superposed.real[:100], 'b-', linewidth=1.5, label='Real')\n",
        "    ax2.plot(t_show, np.abs(superposed[:100]), 'r--', linewidth=1, alpha=0.7, label='Envelope')\n",
        "    ax2.set_ylabel(\"Amplitude\")\n",
        "    ax2.set_title(f\"Layer {layer_idx}: Superposed Wave (R={coherence:.3f})\")\n",
        "    ax2.legend(loc='upper right', fontsize=8)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "axes[-1, 0].set_xlabel(\"Token\")\n",
        "axes[-1, 1].set_xlabel(\"Time (s)\")\n",
        "\n",
        "plt.suptitle(f\"Wave Superposition Through Layers\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"02_wave_superposition_layers.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nWave coherence by layer: {dict(zip(layers_to_show, coherences_by_layer))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot coherence through ALL layers\n",
        "all_coherences = []\n",
        "for layer_idx in range(result['n_layers']):\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    coherence = compute_wave_coherence(waves * query_attn[:, None])\n",
        "    all_coherences.append(coherence)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(result['n_layers']), all_coherences, 'o-', color='purple', linewidth=2, markersize=8)\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"Wave Coherence R\")\n",
        "plt.title(f\"Wave Coherence Through Layers\\n'{prompt[:50]}...'\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(range(result['n_layers']))\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"03_wave_coherence_all_layers.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: Multi-Prompt Comparison\n",
        "\n",
        "Compare phase dynamics across many prompts of different types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test prompts with varying types and lengths\n",
        "test_prompts = {\n",
        "    # Factual (constrained)\n",
        "    \"factual_1\": \"The capital of France is\",\n",
        "    \"factual_2\": \"Water boils at one hundred degrees\",\n",
        "    \"factual_3\": \"The chemical symbol for gold is\",\n",
        "    \"factual_4\": \"The Earth orbits around the\",\n",
        "    \"factual_5\": \"The speed of light in vacuum is approximately\",\n",
        "    \n",
        "    # Narrative (open-ended)\n",
        "    \"narrative_1\": \"She opened the door and saw\",\n",
        "    \"narrative_2\": \"The old man walked slowly towards the\",\n",
        "    \"narrative_3\": \"In the darkness of the forest, something moved\",\n",
        "    \"narrative_4\": \"After years of searching, he finally found the\",\n",
        "    \n",
        "    # Technical/Scientific (specialized vocabulary)\n",
        "    \"technical_1\": \"The quantum mechanical wave function describes probability amplitudes\",\n",
        "    \"technical_2\": \"In machine learning, gradient descent optimizes the loss function by\",\n",
        "    \"technical_3\": \"The transformer architecture uses self-attention to process sequences\",\n",
        "    \"technical_4\": \"Photosynthesis converts carbon dioxide and water into glucose using\",\n",
        "    \n",
        "    # Philosophical (abstract)\n",
        "    \"philosophical_1\": \"The meaning of existence is\",\n",
        "    \"philosophical_2\": \"When considering the nature of consciousness,\",\n",
        "    \"philosophical_3\": \"The relationship between mind and matter suggests that\",\n",
        "}\n",
        "\n",
        "print(f\"Testing {len(test_prompts)} prompts...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run analysis on all prompts\n",
        "all_results = {}\n",
        "\n",
        "for name, prompt in test_prompts.items():\n",
        "    print(f\"Processing: {name}...\")\n",
        "    \n",
        "    result = run_inference(prompt)\n",
        "    tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "    \n",
        "    # Wave coherence through layers\n",
        "    wave_coherences = []\n",
        "    for layer_idx in range(result['n_layers']):\n",
        "        attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "        query_attn = attn[-1, :]\n",
        "        coherence = compute_wave_coherence(waves * query_attn[:, None])\n",
        "        wave_coherences.append(coherence)\n",
        "    \n",
        "    # Prediction\n",
        "    logits = result[\"logits\"][0, -1, :]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    entropy = -(probs * probs.clamp(min=1e-10).log()).sum().item()\n",
        "    top_token = tokenizer.decode([probs.argmax().item()])\n",
        "    \n",
        "    all_results[name] = {\n",
        "        \"prompt\": prompt,\n",
        "        \"tokens\": tokens,\n",
        "        \"frequencies\": frequencies,\n",
        "        \"wave_coherences\": wave_coherences,\n",
        "        \"output_entropy\": entropy,\n",
        "        \"top_token\": top_token,\n",
        "        \"n_tokens\": len(tokens)\n",
        "    }\n",
        "\n",
        "print(\"\\nAll prompts processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(f\"{'Prompt':<55} {'Tok':>4} {'Top Pred':>12} {'Ent':>6} {'R(0)':>6} {'R(11)':>6}\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for name, r in all_results.items():\n",
        "    coh_0 = r[\"wave_coherences\"][0]\n",
        "    coh_11 = r[\"wave_coherences\"][-1]\n",
        "    print(f\"{r['prompt'][:53]:<55} {r['n_tokens']:>4} {r['top_token']:>12} {r['output_entropy']:>6.2f} {coh_0:>6.3f} {coh_11:>6.3f}\")\n",
        "\n",
        "print(\"=\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare wave coherence profiles by prompt type\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Group prompts\n",
        "groups = {\n",
        "    \"Factual\": [k for k in all_results.keys() if k.startswith(\"factual\")],\n",
        "    \"Narrative\": [k for k in all_results.keys() if k.startswith(\"narrative\")],\n",
        "    \"Technical\": [k for k in all_results.keys() if k.startswith(\"technical\")],\n",
        "    \"Philosophical\": [k for k in all_results.keys() if k.startswith(\"philosophical\")]\n",
        "}\n",
        "\n",
        "colors = {\"Factual\": \"blue\", \"Narrative\": \"green\", \"Technical\": \"purple\", \"Philosophical\": \"red\"}\n",
        "\n",
        "# Wave coherence by group\n",
        "ax1 = axes[0, 0]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    for pname in prompt_names:\n",
        "        ax1.plot(range(n_layers), all_results[pname][\"wave_coherences\"], \n",
        "                 color=colors[group_name], alpha=0.3, linewidth=1)\n",
        "    # Group mean\n",
        "    mean_coh = np.mean([all_results[pname][\"wave_coherences\"] for pname in prompt_names], axis=0)\n",
        "    ax1.plot(range(n_layers), mean_coh, color=colors[group_name], linewidth=3, label=group_name)\n",
        "\n",
        "ax1.set_xlabel(\"Layer\")\n",
        "ax1.set_ylabel(\"Wave Coherence R\")\n",
        "ax1.set_title(\"Wave Coherence by Prompt Type\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Entropy vs final coherence\n",
        "ax2 = axes[0, 1]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    entropies = [all_results[pname][\"output_entropy\"] for pname in prompt_names]\n",
        "    final_cohs = [all_results[pname][\"wave_coherences\"][-1] for pname in prompt_names]\n",
        "    ax2.scatter(entropies, final_cohs, s=100, c=colors[group_name], alpha=0.7, label=group_name, edgecolors='black')\n",
        "\n",
        "ax2.set_xlabel(\"Output Entropy\")\n",
        "ax2.set_ylabel(\"Final Layer Wave Coherence\")\n",
        "ax2.set_title(\"Entropy vs Coherence\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Token count vs coherence\n",
        "ax3 = axes[1, 0]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    n_toks = [all_results[pname][\"n_tokens\"] for pname in prompt_names]\n",
        "    final_cohs = [all_results[pname][\"wave_coherences\"][-1] for pname in prompt_names]\n",
        "    ax3.scatter(n_toks, final_cohs, s=100, c=colors[group_name], alpha=0.7, label=group_name, edgecolors='black')\n",
        "\n",
        "ax3.set_xlabel(\"Number of Tokens\")\n",
        "ax3.set_ylabel(\"Final Layer Wave Coherence\")\n",
        "ax3.set_title(\"Sequence Length vs Coherence\")\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Mean Zipf frequency vs coherence\n",
        "ax4 = axes[1, 1]\n",
        "for group_name, prompt_names in groups.items():\n",
        "    mean_freqs = [np.mean(all_results[pname][\"frequencies\"]) for pname in prompt_names]\n",
        "    final_cohs = [all_results[pname][\"wave_coherences\"][-1] for pname in prompt_names]\n",
        "    ax4.scatter(mean_freqs, final_cohs, s=100, c=colors[group_name], alpha=0.7, label=group_name, edgecolors='black')\n",
        "\n",
        "ax4.set_xlabel(\"Mean Zipf-Wave Frequency (Hz)\")\n",
        "ax4.set_ylabel(\"Final Layer Wave Coherence\")\n",
        "ax4.set_title(\"Token Rarity vs Coherence\")\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Phase Dynamics Comparison Across Prompt Types\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"04_prompt_type_comparison.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4: Detailed Wave Interference\n",
        "\n",
        "Visualize the FFT spectrum of superposed waves at each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed wave interference for technical prompt (has rare words)\n",
        "prompt = \"The quantum mechanical wave function describes probability amplitudes\"\n",
        "tokens, frequencies, waves = wave_encoder.encode_sequence(prompt)\n",
        "result = run_inference(prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Token frequencies: {dict(zip([t.strip() for t in tokens], frequencies.round(2)))}\")\n",
        "\n",
        "# Create comprehensive figure\n",
        "fig = plt.figure(figsize=(18, 14))\n",
        "gs = GridSpec(4, 3, figure=fig)\n",
        "\n",
        "# Token waves (top row, full width)\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "t_show = wave_encoder.t[:50]\n",
        "for i, (token, wave, freq) in enumerate(zip(tokens, waves, frequencies)):\n",
        "    offset = i * 0.3\n",
        "    color = cm.plasma(freq / wave_config.freq_max)\n",
        "    ax1.plot(t_show, wave.real[:50] * 0.1 + offset, color=color, linewidth=1)\n",
        "    ax1.text(-0.02, offset, f\"{token.strip()[:10]} ({freq:.1f})\", ha='right', fontsize=8, va='center')\n",
        "\n",
        "ax1.set_xlabel(\"Time (s)\")\n",
        "ax1.set_title(\"Individual Token Waves (colored by frequency: low=dark, high=bright)\")\n",
        "ax1.set_xlim(-0.15, t_show[-1])\n",
        "\n",
        "# Attention patterns at different layers\n",
        "for col, layer_idx in enumerate([0, 5, 11]):\n",
        "    ax = fig.add_subplot(gs[1, col])\n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    im = ax.imshow(attn, cmap='Blues', aspect='auto')\n",
        "    ax.set_title(f\"Layer {layer_idx} Attention\")\n",
        "    ax.set_xlabel(\"Key\")\n",
        "    ax.set_ylabel(\"Query\")\n",
        "    plt.colorbar(im, ax=ax, shrink=0.7)\n",
        "\n",
        "# Superposed waves at different layers\n",
        "for col, layer_idx in enumerate([0, 5, 11]):\n",
        "    ax = fig.add_subplot(gs[2, col])\n",
        "    \n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    \n",
        "    ax.plot(t_show, superposed.real[:50], 'b-', linewidth=1.5)\n",
        "    ax.fill_between(t_show, -np.abs(superposed[:50]), np.abs(superposed[:50]), alpha=0.2, color='blue')\n",
        "    \n",
        "    coherence = compute_wave_coherence(waves * query_attn[:, None])\n",
        "    ax.set_title(f\"Layer {layer_idx} Superposition (R={coherence:.3f})\")\n",
        "    ax.set_xlabel(\"Time (s)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# FFT of superposed waves\n",
        "for col, layer_idx in enumerate([0, 5, 11]):\n",
        "    ax = fig.add_subplot(gs[3, col])\n",
        "    \n",
        "    attn = result[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    \n",
        "    fft_vals = np.abs(fft(superposed.real))\n",
        "    freqs = fftfreq(len(superposed), d=1/wave_config.sample_rate)\n",
        "    pos_mask = freqs > 0\n",
        "    \n",
        "    ax.plot(freqs[pos_mask][:50], fft_vals[pos_mask][:50], 'g-', linewidth=1.5)\n",
        "    ax.set_xlabel(\"Frequency (Hz)\")\n",
        "    ax.set_ylabel(\"Magnitude\")\n",
        "    ax.set_title(f\"Layer {layer_idx} FFT Spectrum\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f\"Wave Interference Analysis: '{prompt[:60]}...'\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"05_wave_interference_detailed.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary results\n",
        "summary = {\n",
        "    \"model\": model_name,\n",
        "    \"n_layers\": n_layers,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"wave_config\": {\n",
        "        \"freq_min\": wave_config.freq_min,\n",
        "        \"freq_max\": wave_config.freq_max,\n",
        "        \"n_harmonics\": wave_config.n_harmonics\n",
        "    },\n",
        "    \"results\": []\n",
        "}\n",
        "\n",
        "for name, r in all_results.items():\n",
        "    summary[\"results\"].append({\n",
        "        \"name\": name,\n",
        "        \"prompt\": r[\"prompt\"],\n",
        "        \"n_tokens\": r[\"n_tokens\"],\n",
        "        \"mean_frequency\": float(np.mean(r[\"frequencies\"])),\n",
        "        \"output_entropy\": r[\"output_entropy\"],\n",
        "        \"top_token\": r[\"top_token\"],\n",
        "        \"wave_coherence_layer_0\": r[\"wave_coherences\"][0],\n",
        "        \"wave_coherence_layer_final\": r[\"wave_coherences\"][-1],\n",
        "        \"wave_coherences\": r[\"wave_coherences\"]\n",
        "    })\n",
        "\n",
        "with open(FIG_DIR / \"wave_phase_results.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {FIG_DIR / 'wave_phase_results.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Phase measurement approaches compared\n",
        "\n",
        "| Approach | What it measures | Reliability | Used in |\n",
        "|----------|------------------|-------------|---------|\n",
        "| **Position centroid** | Where attention points | Proxy only | code_004 |\n",
        "| **FFT phase** | Dominant frequency phases in activations | Good for oscillatory | This notebook |\n",
        "| **Hilbert phase** | Instantaneous phase of envelope | Good for modulation | This notebook |\n",
        "| **Wave coherence** | Phase alignment of Zipf-encoded waves | Intuitive | This notebook |\n",
        "\n",
        "### Key improvements over code_004\n",
        "\n",
        "1. **True wave representation**: Tokens are actual oscillating waves with Zipf-based frequencies\n",
        "2. **Multiple phase measures**: FFT, Hilbert, and wave coherence\n",
        "3. **Visual intuition**: Wave superposition shows how attention combines information\n",
        "4. **More prompts**: 16+ prompts across factual, narrative, technical, philosophical categories\n",
        "5. **Frequency analysis**: How rare vs common words affect phase dynamics\n",
        "\n",
        "### Connection to AKIRA theory\n",
        "\n",
        "The Zipf-wave representation connects to:\n",
        "- **RADAR_ARRAY.md**: Spectral decomposition of signals\n",
        "- **HARMONY_AND_COHERENCE.md**: Phase locking and belief collapse\n",
        "- **ACTION_QUANTA.md**: Minimum actionable patterns\n",
        "\n",
        "Common words (low frequency) form the \"carrier wave\" that rare words (high frequency) modulate.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
