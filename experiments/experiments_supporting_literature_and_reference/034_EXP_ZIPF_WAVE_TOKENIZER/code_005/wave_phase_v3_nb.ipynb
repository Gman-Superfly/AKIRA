{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wave Phase Dynamics v3: Spectra from Activations\n",
        "\n",
        "Whatâ€™s new:\n",
        "- Extract FFT + Hilbert spectra **from model activations** per layer/head\n",
        "- Keep inverted Zipf prior for comparison, but focus on hidden-state spectra\n",
        "- Use more interesting prompts (technical, legal, riddle) for visuals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib import cm\n",
        "from scipy.signal import hilbert\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    HAS_DATASETS = True\n",
        "except ImportError:\n",
        "    HAS_DATASETS = False\n",
        "    print(\"datasets not available, using fallback word frequencies\")\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "FIG_DIR = NOTEBOOK_DIR / \"figs_wave_v3\"\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2\n",
        "model_name = \"gpt2\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.n_layer\n",
        "n_heads = model.config.n_head\n",
        "d_model = model.config.n_embd\n",
        "print(f\"Model: {n_layers} layers, {n_heads} heads, d_model={d_model}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inverted Zipf wave encoder (kept for comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build word frequency table\n",
        "def build_word_frequency_table(tokenizer, max_words: int = 50000) -> Dict[str, int]:\n",
        "    if HAS_DATASETS:\n",
        "        print(\"Loading wikitext-2 for frequency estimation...\")\n",
        "        try:\n",
        "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "            word_counts = Counter()\n",
        "            for example in dataset:\n",
        "                text = example[\"text\"]\n",
        "                if text.strip():\n",
        "                    word_counts.update(text.lower().split())\n",
        "            ranked = word_counts.most_common(max_words)\n",
        "            word_rank = {word: rank + 1 for rank, (word, _) in enumerate(ranked)}\n",
        "            print(f\"Built frequency table with {len(word_rank)} words from wikitext-2\")\n",
        "            return word_rank\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load wikitext-2: {e}\")\n",
        "    \n",
        "    print(\"Using GPT-2 vocab order as frequency proxy...\")\n",
        "    word_rank = {}\n",
        "    for token_id in range(tokenizer.vocab_size):\n",
        "        token = tokenizer.decode([token_id]).strip().lower()\n",
        "        if token and token not in word_rank:\n",
        "            word_rank[token] = len(word_rank) + 1\n",
        "    print(f\"Built frequency table with {len(word_rank)} tokens\")\n",
        "    return word_rank\n",
        "\n",
        "word_freq_table = build_word_frequency_table(tokenizer)\n",
        "common = sorted(word_freq_table.items(), key=lambda x: x[1])[:5]\n",
        "rare = sorted(word_freq_table.items(), key=lambda x: x[1])[-5:]\n",
        "print(f\"Common examples: {common}\")\n",
        "print(f\"Rare examples: {rare}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inverted Zipf wave encoder (for comparison)\n",
        "@dataclass\n",
        "class WaveConfig:\n",
        "    freq_min: float = 0.5\n",
        "    freq_max: float = 10.0\n",
        "    n_harmonics: int = 4\n",
        "    sample_rate: int = 100\n",
        "    duration: float = 2.0\n",
        "\n",
        "class ZipfWaveEncoder:\n",
        "    def __init__(self, tokenizer, word_freq_table: Dict[str, int], config: WaveConfig):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word_freq_table = word_freq_table\n",
        "        self.config = config\n",
        "        self.max_rank = max(word_freq_table.values()) if word_freq_table else 50000\n",
        "        n_samples = int(config.sample_rate * config.duration)\n",
        "        self.t = np.linspace(0, config.duration, n_samples)\n",
        "    \n",
        "    def token_to_frequency(self, token: str) -> float:\n",
        "        rank = self.word_freq_table.get(token.strip().lower(), self.max_rank)\n",
        "        log_rank = np.log(rank + 1)\n",
        "        log_max = np.log(self.max_rank + 1)\n",
        "        normalized = 1.0 - (log_rank / log_max)  # inverted: common -> high freq\n",
        "        return self.config.freq_min + (self.config.freq_max - self.config.freq_min) * normalized\n",
        "    \n",
        "    def token_to_wave(self, token: str, phase_offset: float = 0.0) -> np.ndarray:\n",
        "        freq = self.token_to_frequency(token)\n",
        "        wave = np.zeros_like(self.t, dtype=np.complex128)\n",
        "        for h in range(1, self.config.n_harmonics + 1):\n",
        "            amplitude = 1.0 / h\n",
        "            wave += amplitude * np.exp(1j * 2 * np.pi * freq * h * self.t + 1j * phase_offset * h)\n",
        "        return wave\n",
        "    \n",
        "    def encode_sequence(self, text: str) -> Tuple[List[str], np.ndarray, np.ndarray]:\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "        freqs = np.array([self.token_to_frequency(t) for t in tokens])\n",
        "        waves = np.zeros((len(tokens), len(self.t)), dtype=np.complex128)\n",
        "        for i, token in enumerate(tokens):\n",
        "            phase_offset = 2 * np.pi * i / len(tokens)\n",
        "            waves[i] = self.token_to_wave(token, phase_offset)\n",
        "        return tokens, freqs, waves\n",
        "\n",
        "wave_config = WaveConfig()\n",
        "wave_encoder = ZipfWaveEncoder(tokenizer, word_freq_table, wave_config)\n",
        "print(f\"Wave encoder (INVERTED): common -> {wave_config.freq_max} Hz, rare -> {wave_config.freq_min} Hz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helper\n",
        "def run_inference(text: str) -> Dict:\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "    tokens = [tokenizer.decode([tid]) for tid in input_ids[0]]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_attentions=True, output_hidden_states=True)\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"logits\": outputs.logits.cpu(),\n",
        "        \"attentions\": [a.cpu() for a in outputs.attentions],  # list len n_layers\n",
        "        \"hidden_states\": [h.cpu() for h in outputs.hidden_states],  # len n_layers+1 (emb + layers)\n",
        "        \"n_layers\": len(outputs.attentions),\n",
        "        \"n_heads\": outputs.attentions[0].size(1),\n",
        "    }\n",
        "\n",
        "print(\"run_inference ready (returns attentions + hidden_states)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spectral analysis on activations (per head)\n",
        "\n",
        "def head_context_from_hidden(result: Dict, layer_idx: int, head_idx: int, query_pos: int = -1) -> np.ndarray:\n",
        "    \"\"\"Approximate head context: attention weights over input hidden states to this layer.\n",
        "    Uses hidden_states[layer_idx] as the input to layer `layer_idx`.\n",
        "    This ignores the value projection but preserves attention weighting structure.\n",
        "    \"\"\"\n",
        "    attn = result[\"attentions\"][layer_idx][0, head_idx].numpy()  # [seq, seq]\n",
        "    weights = attn[query_pos]  # [seq]\n",
        "    hidden_in = result[\"hidden_states\"][layer_idx][0].numpy()  # [seq, d_model]\n",
        "    context = weights @ hidden_in  # [d_model]\n",
        "    return context\n",
        "\n",
        "\n",
        "def fft_spectrum(signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"FFT of a real signal -> freqs, magnitudes, phases (radians).\"\"\"\n",
        "    fft_vals = fft(signal)\n",
        "    freqs = fftfreq(len(signal), d=1.0)\n",
        "    pos = freqs > 0\n",
        "    return freqs[pos], np.abs(fft_vals[pos]), np.angle(fft_vals[pos])\n",
        "\n",
        "\n",
        "def spectral_concentration_from_signal(signal: np.ndarray) -> float:\n",
        "    freqs, mags, _ = fft_spectrum(signal)\n",
        "    power = mags ** 2\n",
        "    power = power / (power.sum() + 1e-10)\n",
        "    entropy = -np.sum(power * np.log(power + 1e-10))\n",
        "    max_entropy = np.log(len(power))\n",
        "    return 1.0 - entropy / (max_entropy + 1e-10)\n",
        "\n",
        "\n",
        "def hilbert_phase(signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    analytic = hilbert(signal)\n",
        "    phase = np.unwrap(np.angle(analytic))\n",
        "    envelope = np.abs(analytic)\n",
        "    return phase, envelope\n",
        "\n",
        "\n",
        "def head_phase_coherence(result: Dict, layer_idx: int, top_k: int = 3) -> float:\n",
        "    \"\"\"Coherence of dominant FFT phases across heads at a layer (query = last token).\"\"\"\n",
        "    phases = []\n",
        "    for h in range(result['n_heads']):\n",
        "        ctx = head_context_from_hidden(result, layer_idx, h)\n",
        "        _, mags, phs = fft_spectrum(ctx)\n",
        "        if len(mags) == 0:\n",
        "            continue\n",
        "        top_idx = np.argsort(mags)[-top_k:]\n",
        "        phases.extend(list(phs[top_idx]))\n",
        "    if not phases:\n",
        "        return 0.0\n",
        "    return np.abs(np.mean(np.exp(1j * np.array(phases))))\n",
        "\n",
        "\n",
        "def head_interference(result: Dict, layer_idx: int) -> float:\n",
        "    \"\"\"Interference ratio across heads: power of mean context vs sum of powers.\"\"\"\n",
        "    contexts = []\n",
        "    for h in range(result['n_heads']):\n",
        "        ctx = head_context_from_hidden(result, layer_idx, h)\n",
        "        contexts.append(ctx)\n",
        "    contexts = np.stack(contexts, axis=0)  # [heads, d_model]\n",
        "    mean_ctx = contexts.mean(axis=0)\n",
        "    power_mean = (mean_ctx ** 2).mean()\n",
        "    power_sum = (contexts ** 2).mean()\n",
        "    return power_mean / (power_sum + 1e-10)\n",
        "\n",
        "print(\"Spectral helpers ready (FFT, Hilbert, per-head coherence/interference)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompts (more interesting cases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = {\n",
        "    \"quantum\": \"The quantum mechanical wave function describes tunneling inside stars\",\n",
        "    \"legal\": \"This agreement indemnifies the contractor against all third-party claims arising from negligence\",\n",
        "    \"riddle\": \"I speak without a mouth and hear without ears; what am I?\",\n",
        "    \"coding\": \"Write a Python function that returns the greatest common divisor using Euclid's algorithm\",\n",
        "    \"ambiguous\": \"Describe the color of a square circle\"\n",
        "}\n",
        "print(f\"Loaded {len(prompts)} prompts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run spectral analysis on activations\n",
        "analysis = {}\n",
        "\n",
        "for name, prompt in prompts.items():\n",
        "    print(f\"Processing: {name} -> '{prompt[:60]}...'\")\n",
        "    result = run_inference(prompt)\n",
        "    layer_coh = []\n",
        "    layer_inter = []\n",
        "    layer_spec = []\n",
        "    for layer_idx in range(result['n_layers']):\n",
        "        # per-head phase coherence\n",
        "        layer_coh.append(head_phase_coherence(result, layer_idx, top_k=3))\n",
        "        # interference across heads\n",
        "        layer_inter.append(head_interference(result, layer_idx))\n",
        "        # spectral concentration of mean context (heads averaged)\n",
        "        contexts = []\n",
        "        for h in range(result['n_heads']):\n",
        "            ctx = head_context_from_hidden(result, layer_idx, h)\n",
        "            contexts.append(ctx)\n",
        "        mean_ctx = np.stack(contexts, axis=0).mean(axis=0)\n",
        "        layer_spec.append(spectral_concentration_from_signal(mean_ctx))\n",
        "    logits = result['logits'][0, -1, :]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    entropy = -(probs * probs.clamp(min=1e-10).log()).sum().item()\n",
        "    top_token = tokenizer.decode([probs.argmax().item()])\n",
        "    analysis[name] = {\n",
        "        \"prompt\": prompt,\n",
        "        \"coherence\": layer_coh,\n",
        "        \"interference\": layer_inter,\n",
        "        \"spectral_concentration\": layer_spec,\n",
        "        \"output_entropy\": entropy,\n",
        "        \"top_token\": top_token,\n",
        "        \"tokens\": result['tokens']\n",
        "    }\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots: coherence, spectral concentration, interference (activations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
        "prompt_names = list(analysis.keys())\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(prompt_names)))\n",
        "\n",
        "# Coherence\n",
        "ax = axes[0, 0]\n",
        "for c, name in zip(colors, prompt_names):\n",
        "    ax.plot(analysis[name][\"coherence\"], label=name, color=c)\n",
        "ax.set_title(\"Head phase coherence across layers\")\n",
        "ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Coherence R\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# Spectral concentration\n",
        "ax = axes[0, 1]\n",
        "for c, name in zip(colors, prompt_names):\n",
        "    ax.plot(analysis[name][\"spectral_concentration\"], label=name, color=c)\n",
        "ax.set_title(\"Spectral concentration (mean context)\")\n",
        "ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Concentration\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Interference\n",
        "ax = axes[1, 0]\n",
        "for c, name in zip(colors, prompt_names):\n",
        "    ax.plot(analysis[name][\"interference\"], label=name, color=c)\n",
        "ax.axhline(1.0, color='black', linestyle='--', alpha=0.5)\n",
        "ax.set_title(\"Interference ratio across heads\")\n",
        "ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Power ratio\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Output entropy vs final coherence\n",
        "ax = axes[1, 1]\n",
        "for c, name in zip(colors, prompt_names):\n",
        "    ax.scatter(analysis[name][\"output_entropy\"], analysis[name][\"coherence\"][-1], color=c, label=name, s=80, edgecolors='black')\n",
        "ax.set_xlabel(\"Output entropy\")\n",
        "ax.set_ylabel(\"Final-layer coherence\")\n",
        "ax.set_title(\"Confidence vs coherence\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "\n",
        "# Table-like text\n",
        "ax = axes[2, 0]\n",
        "ax.axis('off')\n",
        "rows = []\n",
        "for name in prompt_names:\n",
        "    rows.append(f\"{name:12s} | top='{analysis[name]['top_token'].strip()[:12]:<12s}' | H={analysis[name]['output_entropy']:.2f}\")\n",
        "ax.text(0.0, 0.9, \"\\n\".join(rows), fontsize=10, va='top')\n",
        "ax.set_title(\"Top token / entropy\")\n",
        "\n",
        "# Empty placeholder for layout\n",
        "axes[2, 1].axis('off')\n",
        "\n",
        "plt.suptitle(\"Activation Spectral Metrics (interesting prompts)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"v3_activation_metrics.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-head spectra for an interesting prompt (quantum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_name = \"quantum\"\n",
        "prompt = prompts[target_name]\n",
        "print(f\"Per-head spectra for: {prompt}\")\n",
        "res = run_inference(prompt)\n",
        "\n",
        "layer_to_show = [0, 5, 11]\n",
        "fig, axes = plt.subplots(len(layer_to_show), 2, figsize=(12, 12))\n",
        "\n",
        "for row, layer_idx in enumerate(layer_to_show):\n",
        "    spectra = []\n",
        "    for h in range(res['n_heads']):\n",
        "        ctx = head_context_from_hidden(res, layer_idx, h)\n",
        "        freqs, mags, phs = fft_spectrum(ctx)\n",
        "        spectra.append(mags)\n",
        "    # Pad to same length\n",
        "    max_len = max(len(s) for s in spectra)\n",
        "    spec_mat = np.zeros((res['n_heads'], max_len))\n",
        "    for h, s in enumerate(spectra):\n",
        "        spec_mat[h, :len(s)] = s\n",
        "    ax = axes[row, 0]\n",
        "    im = ax.imshow(spec_mat, aspect='auto', cmap='magma', origin='lower')\n",
        "    ax.set_title(f\"Layer {layer_idx} head spectra (mag)\")\n",
        "    ax.set_ylabel(\"Head\")\n",
        "    plt.colorbar(im, ax=ax, shrink=0.7)\n",
        "    \n",
        "    # Coherence / interference summary\n",
        "    coh = head_phase_coherence(res, layer_idx)\n",
        "    inter = head_interference(res, layer_idx)\n",
        "    ax2 = axes[row, 1]\n",
        "    ax2.bar([\"coh\", \"inter\"], [coh, inter], color=['teal', 'coral'])\n",
        "    ax2.set_ylim(0, max(1.2, inter + 0.2))\n",
        "    ax2.set_title(f\"Layer {layer_idx}: coh={coh:.3f}, inter={inter:.3f}\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Per-head FFT magnitude heatmaps (quantum prompt)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"v3_quantum_head_spectra.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wave superposition (synthetic waves) for comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_wave = prompts[\"legal\"]\n",
        "print(f\"Wave superposition prompt: {prompt_wave}\")\n",
        "tokens, freqs, waves = wave_encoder.encode_sequence(prompt_wave)\n",
        "res_wave = run_inference(prompt_wave)\n",
        "\n",
        "layers_to_show = [0, 5, 11]\n",
        "fig, axes = plt.subplots(len(layers_to_show), 2, figsize=(14, 10))\n",
        "coherences = []\n",
        "\n",
        "for row, layer_idx in enumerate(layers_to_show):\n",
        "    attn = res_wave[\"attentions\"][layer_idx][0].mean(dim=0).numpy()\n",
        "    query_attn = attn[-1, :]\n",
        "    superposed = (query_attn[:, None] * waves).sum(axis=0)\n",
        "    coh = np.abs(superposed / (np.abs(waves * query_attn[:, None]).sum(axis=0) + 1e-10)).mean()\n",
        "    coherences.append(coh)\n",
        "    ax1 = axes[row, 0]\n",
        "    ax1.bar(range(len(tokens)), query_attn, color='steelblue')\n",
        "    ax1.set_title(f\"Layer {layer_idx} attention (query=last)\")\n",
        "    ax1.set_xticks(range(len(tokens)))\n",
        "    ax1.set_xticklabels([t.strip()[:8] for t in tokens], rotation=45, ha='right')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax2 = axes[row, 1]\n",
        "    t_show = wave_encoder.t[:80]\n",
        "    ax2.plot(t_show, superposed.real[:80], 'b-', linewidth=1.5)\n",
        "    ax2.fill_between(t_show, -np.abs(superposed[:80]), np.abs(superposed[:80]), color='blue', alpha=0.2)\n",
        "    ax2.set_title(f\"Layer {layer_idx} superposition (coh~{coh:.3f})\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "axes[-1, 0].set_xlabel(\"Token\")\n",
        "axes[-1, 1].set_xlabel(\"Time (s)\")\n",
        "plt.suptitle(\"Synthetic wave superposition (legal prompt)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"v3_wave_superposition_legal.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Layer coherences:\", dict(zip(layers_to_show, coherences)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save analysis (JSON-safe)\n",
        "def to_py(obj):\n",
        "    if isinstance(obj, (np.floating, np.integer)):\n",
        "        return obj.item()\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [to_py(x) for x in obj]\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: to_py(v) for k, v in obj.items()}\n",
        "    return obj\n",
        "\n",
        "summary = {\n",
        "    \"model\": model_name,\n",
        "    \"n_layers\": n_layers,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"prompts\": {\n",
        "        name: {\n",
        "            \"prompt\": data[\"prompt\"],\n",
        "            \"coherence\": [float(x) for x in data[\"coherence\"]],\n",
        "            \"interference\": [float(x) for x in data[\"interference\"]],\n",
        "            \"spectral_concentration\": [float(x) for x in data[\"spectral_concentration\"]],\n",
        "            \"output_entropy\": float(data[\"output_entropy\"]),\n",
        "            \"top_token\": data[\"top_token\"],\n",
        "            \"tokens\": data[\"tokens\"],\n",
        "        }\n",
        "        for name, data in analysis.items()\n",
        "    }\n",
        "}\n",
        "with open(FIG_DIR / \"wave_phase_results_v3.json\", \"w\") as f:\n",
        "    json.dump(to_py(summary), f, indent=2)\n",
        "print(f\"Saved to {FIG_DIR / 'wave_phase_results_v3.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "- Added activation-based spectra: FFT + Hilbert on head-specific contexts (using attention weights over layer inputs)\n",
        "- Metrics: head phase coherence, head interference, spectral concentration of mean context\n",
        "- More interesting prompts (quantum, legal, riddle, coding, ambiguous) for clearer visuals\n",
        "- Kept synthetic inverted-Zipf waves only as a side-by-side intuition check\n",
        "\n",
        "Key figures (saved to `figs_wave_v3/`):\n",
        "- `v3_activation_metrics.png`: layer curves for coherence, spectral concentration, interference\n",
        "- `v3_quantum_head_spectra.png`: per-head FFT heatmaps on the quantum prompt\n",
        "- `v3_wave_superposition_legal.png`: synthetic wave superposition on a legal prompt\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
