{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Belief Synchronization: Phase Coherence in Transformer Attention\n",
        "\n",
        "This notebook demonstrates **synchronization in belief systems** by tracking attention phase alignment during inference.\n",
        "\n",
        "## The hypothesis\n",
        "\n",
        "When a transformer makes a confident decision:\n",
        "1. **Entropy drops** through layers (belief concentrates)\n",
        "2. **Phase coherence increases** (attention patterns align)\n",
        "3. **Attention heads synchronize** (collective phase lock)\n",
        "\n",
        "This is analogous to:\n",
        "- Coupled oscillators locking to a common frequency\n",
        "- Superconductors achieving macroscopic phase coherence\n",
        "- The Pythagorean comma being distributed for global harmony\n",
        "\n",
        "## What we measure\n",
        "\n",
        "- **Attention entropy**: How spread is the attention? (uncertainty)\n",
        "- **Phase coherence R**: Are attended positions phase-aligned?\n",
        "- **Head synchronization**: Do different heads point the same direction?\n",
        "- **Confidence correlation**: Does coherence predict prediction confidence?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install transformers torch matplotlib numpy\n",
        "\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "# Transformer model\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Figure directory\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "FIG_DIR = NOTEBOOK_DIR / \"figs_sync\"\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"  # Can use \"gpt2-medium\" or \"gpt2-large\" for more layers\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.n_layer\n",
        "n_heads = model.config.n_head\n",
        "print(f\"Model loaded: {n_layers} layers, {n_heads} heads per layer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core functions: Phase and coherence computation\n",
        "\n",
        "We define \"phase\" of an attention pattern as the angle of its weighted position centroid on the unit circle. This maps the attention distribution to a complex number, where:\n",
        "- The **angle** is the phase (where attention is pointing)\n",
        "- The **magnitude** is the coherence (how concentrated the attention is)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def attention_to_phase(attn_weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Convert attention pattern to phase representation.\n",
        "    \n",
        "    Maps each position to an angle on the unit circle, then computes\n",
        "    the weighted centroid. The angle of this centroid is the \"phase\"\n",
        "    and its magnitude (normalized) is the \"coherence\".\n",
        "    \n",
        "    Args:\n",
        "        attn_weights: Attention weights [batch, heads, query_seq, key_seq]\n",
        "        \n",
        "    Returns:\n",
        "        phase: Phase angle in radians [batch, heads, query_seq]\n",
        "        coherence: Phase coherence 0-1 [batch, heads, query_seq]\n",
        "    \"\"\"\n",
        "    seq_len = attn_weights.size(-1)\n",
        "    \n",
        "    # Map positions to angles on unit circle\n",
        "    positions = torch.arange(seq_len, dtype=torch.float32, device=attn_weights.device)\n",
        "    angles = 2 * math.pi * positions / seq_len  # [0, 2pi)\n",
        "    \n",
        "    # Complex representation: each position is a point on unit circle\n",
        "    # Weighted sum gives centroid\n",
        "    real_part = (attn_weights * torch.cos(angles)).sum(dim=-1)\n",
        "    imag_part = (attn_weights * torch.sin(angles)).sum(dim=-1)\n",
        "    \n",
        "    # Phase is the angle of the centroid\n",
        "    phase = torch.atan2(imag_part, real_part)\n",
        "    \n",
        "    # Coherence is the magnitude (already normalized since attention sums to 1)\n",
        "    coherence = torch.sqrt(real_part**2 + imag_part**2)\n",
        "    \n",
        "    return phase, coherence\n",
        "\n",
        "\n",
        "def compute_attention_entropy(attn_weights: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute entropy of attention distribution.\n",
        "    \n",
        "    High entropy = diffuse attention = uncertain\n",
        "    Low entropy = focused attention = confident\n",
        "    \n",
        "    Args:\n",
        "        attn_weights: [batch, heads, query_seq, key_seq]\n",
        "        \n",
        "    Returns:\n",
        "        entropy: [batch, heads, query_seq] in nats\n",
        "    \"\"\"\n",
        "    p = attn_weights.clamp(min=1e-10)\n",
        "    entropy = -(p * p.log()).sum(dim=-1)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def compute_head_synchronization(phases: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Measure synchronization across attention heads.\n",
        "    \n",
        "    R = |mean(exp(i * phase_h))| across heads h\n",
        "    \n",
        "    R = 1: All heads pointing same direction (synchronized)\n",
        "    R = 0: Heads pointing random directions (desynchronized)\n",
        "    \n",
        "    Args:\n",
        "        phases: [batch, heads, query_seq]\n",
        "        \n",
        "    Returns:\n",
        "        R: [batch, query_seq] synchronization measure\n",
        "    \"\"\"\n",
        "    # Complex representation of each head's phase\n",
        "    complex_phases = torch.complex(\n",
        "        torch.cos(phases),\n",
        "        torch.sin(phases)\n",
        "    )\n",
        "    \n",
        "    # Mean across heads\n",
        "    mean_complex = complex_phases.mean(dim=1)  # [batch, query_seq]\n",
        "    \n",
        "    # Magnitude is the synchronization measure\n",
        "    R = torch.abs(mean_complex)\n",
        "    \n",
        "    return R\n",
        "\n",
        "\n",
        "print(\"Phase and coherence functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_inference_with_attention(model, tokenizer, prompt: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Run inference and extract attention patterns from all layers.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with attention patterns, logits, and metadata.\n",
        "    \"\"\"\n",
        "    # Tokenize\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    tokens = [tokenizer.decode([tid]) for tid in input_ids[0]]\n",
        "    \n",
        "    # Forward pass with attention output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids,\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "    \n",
        "    # Extract components\n",
        "    logits = outputs.logits  # [batch, seq, vocab]\n",
        "    attentions = outputs.attentions  # Tuple of [batch, heads, seq, seq] per layer\n",
        "    hidden_states = outputs.hidden_states  # Tuple of [batch, seq, hidden] per layer\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"tokens\": tokens,\n",
        "        \"logits\": logits,\n",
        "        \"attentions\": attentions,\n",
        "        \"hidden_states\": hidden_states,\n",
        "        \"n_layers\": len(attentions),\n",
        "        \"n_heads\": attentions[0].size(1),\n",
        "        \"seq_len\": input_ids.size(1)\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_layer_dynamics(result: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Compute entropy, phase coherence, and head synchronization\n",
        "    at each layer.\n",
        "    \"\"\"\n",
        "    attentions = result[\"attentions\"]\n",
        "    n_layers = result[\"n_layers\"]\n",
        "    \n",
        "    entropy_per_layer = []\n",
        "    coherence_per_layer = []\n",
        "    head_sync_per_layer = []\n",
        "    phases_per_layer = []\n",
        "    \n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        # attn: [batch, heads, seq, seq]\n",
        "        \n",
        "        # Entropy\n",
        "        entropy = compute_attention_entropy(attn)\n",
        "        entropy_per_layer.append(entropy.mean().item())\n",
        "        \n",
        "        # Phase and coherence\n",
        "        phases, coherences = attention_to_phase(attn)\n",
        "        coherence_per_layer.append(coherences.mean().item())\n",
        "        phases_per_layer.append(phases)\n",
        "        \n",
        "        # Head synchronization\n",
        "        head_sync = compute_head_synchronization(phases)\n",
        "        head_sync_per_layer.append(head_sync.mean().item())\n",
        "    \n",
        "    return {\n",
        "        \"entropy\": entropy_per_layer,\n",
        "        \"coherence\": coherence_per_layer,\n",
        "        \"head_sync\": head_sync_per_layer,\n",
        "        \"phases\": phases_per_layer\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_prediction_confidence(logits: torch.Tensor) -> Tuple[float, str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute prediction confidence for the last token.\n",
        "    \n",
        "    Returns:\n",
        "        confidence: Negative entropy (higher = more confident)\n",
        "        top_token: Most likely next token\n",
        "        probs: Full probability distribution\n",
        "    \"\"\"\n",
        "    last_logits = logits[0, -1, :]  # [vocab]\n",
        "    probs = F.softmax(last_logits, dim=-1)\n",
        "    \n",
        "    # Entropy of prediction\n",
        "    entropy = -(probs * probs.clamp(min=1e-10).log()).sum()\n",
        "    confidence = -entropy.item()\n",
        "    \n",
        "    # Top token\n",
        "    top_id = probs.argmax().item()\n",
        "    top_token = tokenizer.decode([top_id])\n",
        "    \n",
        "    return confidence, top_token, probs\n",
        "\n",
        "\n",
        "print(\"Inference and dynamics functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Layer-wise belief dynamics\n",
        "\n",
        "Track entropy and phase coherence through layers for a single prompt.\n",
        "\n",
        "**Prediction**: Entropy decreases, coherence increases as we go deeper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompt\n",
        "prompt = \"The capital of France is\"\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(\"Running inference...\")\n",
        "\n",
        "result = run_inference_with_attention(model, tokenizer, prompt)\n",
        "dynamics = compute_layer_dynamics(result)\n",
        "confidence, top_token, probs = compute_prediction_confidence(result[\"logits\"])\n",
        "\n",
        "print(f\"\\nTokens: {result['tokens']}\")\n",
        "print(f\"Predicted next token: '{top_token}'\")\n",
        "print(f\"Prediction confidence: {confidence:.2f}\")\n",
        "\n",
        "# Plot layer dynamics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "layers = range(result[\"n_layers\"])\n",
        "\n",
        "# Entropy\n",
        "ax1 = axes[0]\n",
        "ax1.plot(layers, dynamics[\"entropy\"], \"o-\", color=\"crimson\", linewidth=2)\n",
        "ax1.set_xlabel(\"Layer\")\n",
        "ax1.set_ylabel(\"Attention Entropy (nats)\")\n",
        "ax1.set_title(\"Entropy through layers\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Coherence\n",
        "ax2 = axes[1]\n",
        "ax2.plot(layers, dynamics[\"coherence\"], \"o-\", color=\"forestgreen\", linewidth=2)\n",
        "ax2.set_xlabel(\"Layer\")\n",
        "ax2.set_ylabel(\"Phase Coherence R\")\n",
        "ax2.set_title(\"Phase coherence through layers\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Head synchronization\n",
        "ax3 = axes[2]\n",
        "ax3.plot(layers, dynamics[\"head_sync\"], \"o-\", color=\"royalblue\", linewidth=2)\n",
        "ax3.set_xlabel(\"Layer\")\n",
        "ax3.set_ylabel(\"Head Synchronization R\")\n",
        "ax3.set_title(\"Head synchronization through layers\")\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f\"Belief Dynamics: '{prompt}' -> '{top_token}'\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"layer_dynamics_single.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Compare confident vs uncertain predictions\n",
        "\n",
        "Test hypothesis: Phase coherence correlates with prediction confidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompts designed to have varying confidence\n",
        "prompts = [\n",
        "    # High confidence expected (factual, clear continuation)\n",
        "    \"The capital of France is\",\n",
        "    \"2 + 2 =\",\n",
        "    \"The sun rises in the\",\n",
        "    \"Water freezes at zero degrees\",\n",
        "    \"The opposite of hot is\",\n",
        "    \n",
        "    # Medium confidence (common patterns but multiple options)\n",
        "    \"The best way to learn is to\",\n",
        "    \"I went to the store to buy\",\n",
        "    \"She looked at him and\",\n",
        "    \"The meeting was scheduled for\",\n",
        "    \"He opened the door and\",\n",
        "    \n",
        "    # Lower confidence expected (ambiguous, many valid continuations)\n",
        "    \"The thing about life is\",\n",
        "    \"In the distant future,\",\n",
        "    \"Some people believe that\",\n",
        "    \"The color of the sky reminded her of\",\n",
        "    \"When considering the implications,\",\n",
        "]\n",
        "\n",
        "results_list = []\n",
        "\n",
        "print(\"Analyzing prompts...\\n\")\n",
        "for prompt in prompts:\n",
        "    result = run_inference_with_attention(model, tokenizer, prompt)\n",
        "    dynamics = compute_layer_dynamics(result)\n",
        "    confidence, top_token, probs = compute_prediction_confidence(result[\"logits\"])\n",
        "    \n",
        "    # Get final layer metrics\n",
        "    final_coherence = dynamics[\"coherence\"][-1]\n",
        "    final_head_sync = dynamics[\"head_sync\"][-1]\n",
        "    final_entropy = dynamics[\"entropy\"][-1]\n",
        "    \n",
        "    results_list.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"top_token\": top_token,\n",
        "        \"confidence\": confidence,\n",
        "        \"final_coherence\": final_coherence,\n",
        "        \"final_head_sync\": final_head_sync,\n",
        "        \"final_entropy\": final_entropy,\n",
        "        \"dynamics\": dynamics\n",
        "    })\n",
        "    \n",
        "    print(f\"'{prompt[:40]:40s}' -> '{top_token:10s}' | conf={confidence:6.2f} | R={final_coherence:.3f} | sync={final_head_sync:.3f}\")\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: Confidence vs Phase Coherence\n",
        "confidences = [r[\"confidence\"] for r in results_list]\n",
        "coherences = [r[\"final_coherence\"] for r in results_list]\n",
        "head_syncs = [r[\"final_head_sync\"] for r in results_list]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Confidence vs Coherence\n",
        "ax1 = axes[0]\n",
        "ax1.scatter(confidences, coherences, s=80, c=\"teal\", alpha=0.7, edgecolors=\"black\")\n",
        "ax1.set_xlabel(\"Prediction Confidence (neg entropy)\")\n",
        "ax1.set_ylabel(\"Final Layer Phase Coherence R\")\n",
        "ax1.set_title(\"Confidence vs Phase Coherence\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add correlation\n",
        "corr_1 = np.corrcoef(confidences, coherences)[0, 1]\n",
        "ax1.annotate(f\"r = {corr_1:.3f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\", fontsize=11)\n",
        "\n",
        "# Confidence vs Head Sync\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(confidences, head_syncs, s=80, c=\"darkorange\", alpha=0.7, edgecolors=\"black\")\n",
        "ax2.set_xlabel(\"Prediction Confidence (neg entropy)\")\n",
        "ax2.set_ylabel(\"Final Layer Head Synchronization R\")\n",
        "ax2.set_title(\"Confidence vs Head Synchronization\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add correlation\n",
        "corr_2 = np.corrcoef(confidences, head_syncs)[0, 1]\n",
        "ax2.annotate(f\"r = {corr_2:.3f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\", fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"confidence_coherence_scatter.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCorrelation (confidence, coherence): {corr_1:.3f}\")\n",
        "print(f\"Correlation (confidence, head_sync): {corr_2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 3: Find the collapse layer (where entropy drops fastest)\n",
        "\n",
        "def find_collapse_layer(dynamics: Dict) -> int:\n",
        "    \"\"\"Find the layer where entropy drops most rapidly.\"\"\"\n",
        "    entropy = np.array(dynamics[\"entropy\"])\n",
        "    entropy_diff = np.diff(entropy)\n",
        "    collapse_layer = np.argmin(entropy_diff)  # Most negative = biggest drop\n",
        "    return collapse_layer\n",
        "\n",
        "# Analyze collapse layers across prompts\n",
        "collapse_layers = []\n",
        "for r in results_list:\n",
        "    cl = find_collapse_layer(r[\"dynamics\"])\n",
        "    collapse_layers.append(cl)\n",
        "    r[\"collapse_layer\"] = cl\n",
        "\n",
        "print(\"Collapse layers per prompt:\\n\")\n",
        "for r in results_list:\n",
        "    print(f\"'{r['prompt'][:40]:40s}' -> collapse at layer {r['collapse_layer']}\")\n",
        "\n",
        "# Histogram\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(collapse_layers, bins=range(n_layers + 1), color=\"mediumpurple\", edgecolor=\"black\", alpha=0.8)\n",
        "plt.xlabel(\"Layer index\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of collapse layers (where entropy drops most)\")\n",
        "plt.xticks(range(n_layers))\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"collapse_layer_histogram.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 4: Head phase evolution through layers\n",
        "# Visualize how individual head phases evolve and converge\n",
        "\n",
        "prompt = \"The capital of France is\"\n",
        "result = run_inference_with_attention(model, tokenizer, prompt)\n",
        "dynamics = compute_layer_dynamics(result)\n",
        "\n",
        "# Extract head phases for the last query position\n",
        "head_phases_by_layer = []\n",
        "for layer_phases in dynamics[\"phases\"]:\n",
        "    # layer_phases: [batch, heads, query_seq]\n",
        "    phases_last_pos = layer_phases[0, :, -1].cpu().numpy()  # [heads]\n",
        "    head_phases_by_layer.append(phases_last_pos)\n",
        "\n",
        "head_phases_by_layer = np.array(head_phases_by_layer)  # [layers, heads]\n",
        "\n",
        "# Plot head phases through layers\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Phase trajectories\n",
        "ax1 = axes[0]\n",
        "for h in range(n_heads):\n",
        "    unwrapped = np.unwrap(head_phases_by_layer[:, h])\n",
        "    ax1.plot(range(n_layers), unwrapped, \"o-\", alpha=0.7, label=f\"Head {h}\")\n",
        "\n",
        "ax1.set_xlabel(\"Layer\")\n",
        "ax1.set_ylabel(\"Phase (rad, unwrapped)\")\n",
        "ax1.set_title(f\"Head phase trajectories: '{prompt}'\")\n",
        "ax1.legend(ncol=2, fontsize=8)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Phase spread (circular std)\n",
        "ax2 = axes[1]\n",
        "\n",
        "def circular_std(phases):\n",
        "    \"\"\"Compute circular standard deviation.\"\"\"\n",
        "    R = np.abs(np.mean(np.exp(1j * phases)))\n",
        "    return np.sqrt(-2 * np.log(R + 1e-10))\n",
        "\n",
        "phase_spread = [circular_std(head_phases_by_layer[l, :]) for l in range(n_layers)]\n",
        "\n",
        "ax2.plot(range(n_layers), phase_spread, \"o-\", color=\"firebrick\", linewidth=2)\n",
        "ax2.set_xlabel(\"Layer\")\n",
        "ax2.set_ylabel(\"Phase spread (circular std)\")\n",
        "ax2.set_title(\"Head phase dispersion through layers\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Mark collapse layer\n",
        "cl = find_collapse_layer(dynamics)\n",
        "ax2.axvline(x=cl, color=\"purple\", linestyle=\"--\", alpha=0.7, label=f\"Collapse layer ({cl})\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"head_phase_evolution.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 5: Comprehensive collapse visualization\n",
        "# Show entropy, coherence, and sync together with collapse layer marked\n",
        "\n",
        "prompt = \"The capital of France is\"\n",
        "result = run_inference_with_attention(model, tokenizer, prompt)\n",
        "dynamics = compute_layer_dynamics(result)\n",
        "confidence, top_token, _ = compute_prediction_confidence(result[\"logits\"])\n",
        "collapse_layer = find_collapse_layer(dynamics)\n",
        "\n",
        "# Normalize entropy for comparison\n",
        "entropy_norm = np.array(dynamics[\"entropy\"])\n",
        "entropy_norm = (entropy_norm - entropy_norm.min()) / (entropy_norm.max() - entropy_norm.min() + 1e-10)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "layers = range(n_layers)\n",
        "\n",
        "# Plot all three metrics\n",
        "ax.plot(layers, entropy_norm, \"o-\", color=\"crimson\", linewidth=2, label=\"Entropy (normalized)\")\n",
        "ax.plot(layers, dynamics[\"coherence\"], \"s-\", color=\"forestgreen\", linewidth=2, label=\"Phase Coherence R\")\n",
        "ax.plot(layers, dynamics[\"head_sync\"], \"^-\", color=\"royalblue\", linewidth=2, label=\"Head Synchronization R\")\n",
        "\n",
        "# Mark collapse layer\n",
        "ax.axvline(x=collapse_layer, color=\"purple\", linestyle=\"--\", alpha=0.7, linewidth=2)\n",
        "ax.annotate(f\"Collapse\\n(layer {collapse_layer})\", \n",
        "            xy=(collapse_layer, 0.5), xytext=(collapse_layer + 1.5, 0.7),\n",
        "            fontsize=10, arrowprops=dict(arrowstyle=\"->\", color=\"purple\"))\n",
        "\n",
        "# Shade regions\n",
        "ax.axvspan(0, collapse_layer, alpha=0.1, color=\"red\", label=\"Before collapse\")\n",
        "ax.axvspan(collapse_layer, n_layers - 1, alpha=0.1, color=\"green\", label=\"After collapse\")\n",
        "\n",
        "ax.set_xlabel(\"Layer\", fontsize=12)\n",
        "ax.set_ylabel(\"Value\", fontsize=12)\n",
        "ax.set_title(f\"Belief Collapse: '{prompt}' -> '{top_token}'\\n\" +\n",
        "             f\"Entropy drops, coherence rises at the decision point\", fontsize=12)\n",
        "ax.legend(loc=\"center right\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(-0.5, n_layers - 0.5)\n",
        "ax.set_ylim(0, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"collapse_comparison.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON\n",
        "summary = {\n",
        "    \"model\": model_name,\n",
        "    \"n_layers\": n_layers,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"prompts\": [],\n",
        "    \"correlations\": {\n",
        "        \"confidence_vs_coherence\": float(corr_1),\n",
        "        \"confidence_vs_head_sync\": float(corr_2)\n",
        "    }\n",
        "}\n",
        "\n",
        "for r in results_list:\n",
        "    summary[\"prompts\"].append({\n",
        "        \"prompt\": r[\"prompt\"],\n",
        "        \"top_token\": r[\"top_token\"],\n",
        "        \"confidence\": r[\"confidence\"],\n",
        "        \"final_coherence\": r[\"final_coherence\"],\n",
        "        \"final_head_sync\": r[\"final_head_sync\"],\n",
        "        \"collapse_layer\": r.get(\"collapse_layer\", -1),\n",
        "        \"entropy_trajectory\": r[\"dynamics\"][\"entropy\"],\n",
        "        \"coherence_trajectory\": r[\"dynamics\"][\"coherence\"],\n",
        "        \"head_sync_trajectory\": r[\"dynamics\"][\"head_sync\"]\n",
        "    })\n",
        "\n",
        "with open(FIG_DIR / \"sync_results.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {FIG_DIR / 'sync_results.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What we measured\n",
        "\n",
        "1. **Attention entropy** through layers: uncertainty about where to attend\n",
        "2. **Phase coherence R** through layers: alignment of attention patterns\n",
        "3. **Head synchronization** through layers: agreement between attention heads\n",
        "4. **Correlation** between final-layer metrics and prediction confidence\n",
        "\n",
        "### Key findings\n",
        "\n",
        "- Entropy tends to decrease through layers (belief concentrates)\n",
        "- Phase coherence tends to increase through layers (patterns align)\n",
        "- Head synchronization shows the degree of collective agreement\n",
        "- The \"collapse layer\" is where entropy drops most rapidly\n",
        "- Confident predictions should correlate with higher phase coherence\n",
        "\n",
        "### Connection to AKIRA theory\n",
        "\n",
        "This demonstrates the **phase transition** described in `HARMONY_AND_COHERENCE.md`:\n",
        "\n",
        "- Before collapse: multiple hypotheses, incoherent phases, high entropy\n",
        "- After collapse: single interpretation, coherent phases, low entropy\n",
        "- The transition is analogous to superconductivity or coupled oscillator locking\n",
        "\n",
        "The synchronization of attention heads is the **collective phase lock** that enables confident, coherent predictions.\n",
        "\n",
        "### Key difference from previous notebook\n",
        "\n",
        "| Previous (zipf_wave_viz) | This notebook |\n",
        "|--------------------------|---------------|\n",
        "| Random attention | **Real GPT-2 attention** |\n",
        "| Static snapshot | **Layer-by-layer dynamics** |\n",
        "| Token position phases | **Attention pattern phases** |\n",
        "| No collapse event | **Collapse layer detection** |\n",
        "| Unconnected to confidence | **Confidence correlation** |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
