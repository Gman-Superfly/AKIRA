{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035I: AQ Excitation Threshold Detection\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## Core Hypothesis\n",
    "\n",
    "The AKIRA framework proposes that:\n",
    "1. AQ (Action Quanta) are quasiparticle excitations in the model's weight field\n",
    "2. A prompt must contain a MINIMUM number of AQ to excite the belief field sufficiently\n",
    "3. Below this threshold, the model cannot construct a coherent response\n",
    "4. Above threshold, AQ \"resonate\" with the weight field and bond into the answer\n",
    "\n",
    "This is analogous to:\n",
    "- Quantum mechanics: minimum energy to excite a state\n",
    "- Radar: minimum signal-to-noise for target detection\n",
    "- Neural activation: threshold potential for firing\n",
    "\n",
    "---\n",
    "\n",
    "## What We're Testing\n",
    "\n",
    "1. **Threshold Detection**: Find the minimum AQ count needed for coherent responses\n",
    "2. **Belief Field Visualization**: Measure activation coherence as proxy for field state\n",
    "3. **Resonance Patterns**: Track how AQ in prompts excite corresponding weight patterns\n",
    "4. **Phase Transition**: Identify the critical point where responses become coherent\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "We construct prompts with varying numbers of AQ (action-enabling discriminations):\n",
    "- 0 AQ: Pure noise / no actionable content\n",
    "- 1 AQ: Single discrimination (e.g., just \"threat\" without context)\n",
    "- 2 AQ: Two discriminations (e.g., \"threat\" + \"proximity\")\n",
    "- 3+ AQ: Multiple discriminations enabling full action\n",
    "\n",
    "We measure:\n",
    "- Response coherence/quality\n",
    "- Activation coherence (field excitation proxy)\n",
    "- Layer-wise activation magnitude (excitation strength)\n",
    "- Attention entropy (focus vs diffuse)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for Colab\n",
    "!pip install transformers torch numpy scikit-learn matplotlib seaborn scipy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for AQ threshold detection experiment.\"\"\"\n",
    "    \n",
    "    model_name: str = \"gpt2-medium\"\n",
    "    model_path: str = \"gpt2-medium\"\n",
    "    \n",
    "    # Number of prompts per AQ count level\n",
    "    n_prompts_per_level: int = 100\n",
    "    \n",
    "    # AQ count levels to test (0 = no AQ, 1 = single AQ, etc.)\n",
    "    aq_levels: List[int] = field(default_factory=lambda: [0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Layers to probe for field visualization\n",
    "    layers_to_probe: List[int] = field(default_factory=lambda: [0, 4, 8, 12, 16, 20, 23])\n",
    "    \n",
    "    # Number of tokens to generate for response quality\n",
    "    n_generate_tokens: int = 20\n",
    "    \n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"AQ levels to test: {config.aq_levels}\")\n",
    "print(f\"Prompts per level: {config.n_prompts_per_level}\")\n",
    "print(f\"Layers to probe: {config.layers_to_probe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AQ-Graded Prompt Construction\n",
    "\n",
    "We construct prompts with precisely controlled AQ content.\n",
    "\n",
    "Each AQ represents a discrimination that enables action:\n",
    "- THREAT_PRESENT: Is there a threat? (enables FLEE vs STAY)\n",
    "- PROXIMITY: How close? (enables URGENT vs DELAYED response)\n",
    "- DIRECTION: Which way? (enables LEFT vs RIGHT vs FORWARD)\n",
    "- MAGNITUDE: How severe? (enables PROPORTIONAL response)\n",
    "- AGENCY: Who acts? (enables SELF vs OTHER response)\n",
    "\n",
    "A prompt with 0 AQ has no actionable discriminations.\n",
    "A prompt with 5 AQ has all discriminations needed for precise action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AQ components that can be added to prompts\n",
    "# Each AQ enables a specific discrimination\n",
    "\n",
    "AQ_COMPONENTS = {\n",
    "    \"THREAT_PRESENT\": {\n",
    "        \"description\": \"Discriminates THREAT vs NO-THREAT\",\n",
    "        \"positive_markers\": [\"danger\", \"threat\", \"attack\", \"fire\", \"flood\", \"predator\", \"enemy\", \"poison\", \"collapse\"],\n",
    "        \"negative_markers\": [\"safe\", \"calm\", \"peaceful\", \"secure\", \"protected\"],\n",
    "        \"enabled_action\": \"FLEE vs STAY\"\n",
    "    },\n",
    "    \"PROXIMITY\": {\n",
    "        \"description\": \"Discriminates NEAR vs FAR\",\n",
    "        \"positive_markers\": [\"approaching\", \"close\", \"nearby\", \"imminent\", \"seconds away\", \"right here\", \"at your feet\"],\n",
    "        \"negative_markers\": [\"distant\", \"far away\", \"miles away\", \"hours away\"],\n",
    "        \"enabled_action\": \"URGENT vs DELAYED\"\n",
    "    },\n",
    "    \"DIRECTION\": {\n",
    "        \"description\": \"Discriminates directional alternatives\",\n",
    "        \"positive_markers\": [\"from the left\", \"from the right\", \"from above\", \"from behind\", \"from the north\"],\n",
    "        \"negative_markers\": [\"from somewhere\", \"from around\"],\n",
    "        \"enabled_action\": \"LEFT vs RIGHT vs FORWARD vs BACK\"\n",
    "    },\n",
    "    \"MAGNITUDE\": {\n",
    "        \"description\": \"Discriminates severity level\",\n",
    "        \"positive_markers\": [\"massive\", \"lethal\", \"catastrophic\", \"tiny\", \"minor\", \"severe\", \"critical\"],\n",
    "        \"negative_markers\": [\"some\", \"a bit of\", \"possibly\"],\n",
    "        \"enabled_action\": \"PROPORTIONAL response scaling\"\n",
    "    },\n",
    "    \"AGENCY\": {\n",
    "        \"description\": \"Discriminates who must act\",\n",
    "        \"positive_markers\": [\"you must\", \"you should\", \"you need to\", \"your responsibility\"],\n",
    "        \"negative_markers\": [\"someone should\", \"it might be\", \"perhaps\"],\n",
    "        \"enabled_action\": \"SELF-ACTION vs DELEGATE\"\n",
    "    },\n",
    "    \"TEMPORAL\": {\n",
    "        \"description\": \"Discriminates when to act\",\n",
    "        \"positive_markers\": [\"now\", \"immediately\", \"right now\", \"this instant\", \"before it's too late\"],\n",
    "        \"negative_markers\": [\"eventually\", \"sometime\", \"when possible\", \"later\"],\n",
    "        \"enabled_action\": \"NOW vs LATER\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(AQ_COMPONENTS)} AQ components:\")\n",
    "for name, data in AQ_COMPONENTS.items():\n",
    "    print(f\"  {name}: {data['description']} -> {data['enabled_action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graded_prompts(n_per_level: int = 100) -> Dict[int, List[Dict]]:\n",
    "    \"\"\"Generate prompts with precisely controlled AQ counts.\n",
    "    \n",
    "    Args:\n",
    "        n_per_level: Number of prompts per AQ level\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping AQ count to list of prompt dicts\n",
    "    \"\"\"\n",
    "    prompts_by_level = {level: [] for level in config.aq_levels}\n",
    "    \n",
    "    # Level 0: No AQ - completely ambiguous, no actionable discrimination\n",
    "    level_0_templates = [\n",
    "        \"Something might be somewhere.\",\n",
    "        \"There could be a thing.\",\n",
    "        \"It is possible that something exists.\",\n",
    "        \"One might consider that perhaps.\",\n",
    "        \"Things may or may not be happening.\",\n",
    "        \"An unspecified situation of unknown nature.\",\n",
    "        \"Conditions are in some state.\",\n",
    "        \"Elements are arranged somehow.\",\n",
    "        \"Factors exist in the environment.\",\n",
    "        \"A state of affairs persists.\",\n",
    "    ]\n",
    "    \n",
    "    # Level 1: Single AQ (one discrimination only)\n",
    "    # Just THREAT_PRESENT - you know there's danger but nothing else\n",
    "    level_1_templates = [\n",
    "        \"There is danger.\",\n",
    "        \"A threat exists.\",\n",
    "        \"Danger is present.\",\n",
    "        \"Something dangerous.\",\n",
    "        \"A hazard exists.\",\n",
    "        \"There is a predator.\",\n",
    "        \"An enemy is present.\",\n",
    "        \"Fire exists somewhere.\",\n",
    "        \"Poison is present.\",\n",
    "        \"A threat has appeared.\",\n",
    "    ]\n",
    "    \n",
    "    # Level 2: Two AQ (THREAT + PROXIMITY)\n",
    "    level_2_templates = [\n",
    "        \"Danger is approaching.\",\n",
    "        \"A threat is nearby.\",\n",
    "        \"Close danger exists.\",\n",
    "        \"An imminent threat.\",\n",
    "        \"Nearby hazard detected.\",\n",
    "        \"A predator is close.\",\n",
    "        \"The enemy approaches.\",\n",
    "        \"Fire is spreading toward you.\",\n",
    "        \"Poison is right here.\",\n",
    "        \"A threat is seconds away.\",\n",
    "    ]\n",
    "    \n",
    "    # Level 3: Three AQ (THREAT + PROXIMITY + DIRECTION)\n",
    "    level_3_templates = [\n",
    "        \"Danger approaches from the left.\",\n",
    "        \"A threat is coming from behind.\",\n",
    "        \"Close danger from the north.\",\n",
    "        \"Imminent threat from above.\",\n",
    "        \"Nearby hazard to your right.\",\n",
    "        \"A predator stalks from the shadows on your left.\",\n",
    "        \"The enemy charges from the east.\",\n",
    "        \"Fire spreads from the south toward you.\",\n",
    "        \"Poison gas drifts from the west.\",\n",
    "        \"A threat emerges from below, close.\",\n",
    "    ]\n",
    "    \n",
    "    # Level 4: Four AQ (THREAT + PROXIMITY + DIRECTION + MAGNITUDE)\n",
    "    level_4_templates = [\n",
    "        \"A massive danger rapidly approaches from the left.\",\n",
    "        \"A lethal threat is close behind you.\",\n",
    "        \"Critical danger from the north, imminent.\",\n",
    "        \"A severe, imminent threat from above.\",\n",
    "        \"A minor hazard nearby to your right.\",\n",
    "        \"A lethal predator stalks close from the left.\",\n",
    "        \"A massive enemy force charges from the east.\",\n",
    "        \"A catastrophic fire spreads rapidly from the south.\",\n",
    "        \"Lethal poison gas drifts nearby from the west.\",\n",
    "        \"A critical threat emerges from below, seconds away.\",\n",
    "    ]\n",
    "    \n",
    "    # Level 5: Five AQ (THREAT + PROXIMITY + DIRECTION + MAGNITUDE + AGENCY)\n",
    "    level_5_templates = [\n",
    "        \"A massive danger rapidly approaches from the left. You must act.\",\n",
    "        \"A lethal threat is close behind you. You need to respond.\",\n",
    "        \"Critical danger from the north is imminent. You should move.\",\n",
    "        \"A severe threat from above requires your immediate action.\",\n",
    "        \"You must handle the minor hazard nearby to your right.\",\n",
    "        \"A lethal predator stalks you from the left. You must flee.\",\n",
    "        \"A massive enemy charges from the east. You need to defend.\",\n",
    "        \"Catastrophic fire from the south. You must evacuate now.\",\n",
    "        \"Lethal gas from the west. You should hold your breath and run.\",\n",
    "        \"Critical threat from below. You must jump immediately.\",\n",
    "    ]\n",
    "    \n",
    "    template_sets = {\n",
    "        0: level_0_templates,\n",
    "        1: level_1_templates,\n",
    "        2: level_2_templates,\n",
    "        3: level_3_templates,\n",
    "        4: level_4_templates,\n",
    "        5: level_5_templates,\n",
    "    }\n",
    "    \n",
    "    # Generate prompts for each level\n",
    "    for level in config.aq_levels:\n",
    "        if level not in template_sets:\n",
    "            continue\n",
    "            \n",
    "        templates = template_sets[level]\n",
    "        \n",
    "        for i in range(n_per_level):\n",
    "            template = templates[i % len(templates)]\n",
    "            \n",
    "            # Add variation by slight rewording\n",
    "            variations = [\n",
    "                template,\n",
    "                template.replace(\".\", \"!\"),\n",
    "                \"Warning: \" + template,\n",
    "                template + \" What do you do?\",\n",
    "                \"Alert: \" + template,\n",
    "            ]\n",
    "            \n",
    "            prompt_text = variations[i % len(variations)]\n",
    "            \n",
    "            # Identify which AQ are present\n",
    "            present_aq = []\n",
    "            prompt_lower = prompt_text.lower()\n",
    "            \n",
    "            for aq_name, aq_data in AQ_COMPONENTS.items():\n",
    "                for marker in aq_data[\"positive_markers\"]:\n",
    "                    if marker.lower() in prompt_lower:\n",
    "                        present_aq.append(aq_name)\n",
    "                        break\n",
    "            \n",
    "            prompts_by_level[level].append({\n",
    "                \"text\": prompt_text,\n",
    "                \"aq_count\": level,\n",
    "                \"aq_present\": present_aq,\n",
    "                \"expected_action_clarity\": level / max(config.aq_levels)\n",
    "            })\n",
    "    \n",
    "    return prompts_by_level\n",
    "\n",
    "\n",
    "# Generate all prompts\n",
    "GRADED_PROMPTS = generate_graded_prompts(config.n_prompts_per_level)\n",
    "\n",
    "print(f\"\\nGenerated prompts by AQ level:\")\n",
    "for level, prompts in GRADED_PROMPTS.items():\n",
    "    print(f\"  Level {level}: {len(prompts)} prompts\")\n",
    "    if prompts:\n",
    "        print(f\"    Example: {prompts[0]['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(config.model_path)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Belief Field Measurement Functions\n",
    "\n",
    "The \"belief field\" is the model's internal representation state.\n",
    "We cannot see it directly, but we can measure proxies:\n",
    "\n",
    "1. **Activation Magnitude**: How \"excited\" is each layer?\n",
    "2. **Activation Coherence**: How aligned are the representations?\n",
    "3. **Attention Entropy**: How focused vs diffuse is attention?\n",
    "4. **Layer-wise Correlation**: How correlated are layers (field coupling)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeliefFieldProbe:\n",
    "    \"\"\"Probe the model's internal 'belief field' state.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"Initialize probe.\n",
    "        \n",
    "        Args:\n",
    "            model: The language model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "        self.stored_activations = {}\n",
    "        self.stored_attentions = {}\n",
    "        \n",
    "    def _get_activation_hook(self, layer_idx: int) -> Callable:\n",
    "        \"\"\"Create hook to store activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                self.stored_activations[layer_idx] = output[0].detach().clone()\n",
    "            else:\n",
    "                self.stored_activations[layer_idx] = output.detach().clone()\n",
    "        return hook\n",
    "    \n",
    "    def _get_attention_hook(self, layer_idx: int) -> Callable:\n",
    "        \"\"\"Create hook to store attention weights.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple) and len(output) > 1:\n",
    "                if output[1] is not None:\n",
    "                    self.stored_attentions[layer_idx] = output[1].detach().clone()\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self, layers: List[int]) -> None:\n",
    "        \"\"\"Register hooks to capture activations and attention.\"\"\"\n",
    "        self.clear_hooks()\n",
    "        \n",
    "        for layer_idx in layers:\n",
    "            if hasattr(self.model, 'transformer'):\n",
    "                block = self.model.transformer.h[layer_idx]\n",
    "                hook1 = block.register_forward_hook(self._get_activation_hook(layer_idx))\n",
    "                self.hooks.append(hook1)\n",
    "                if hasattr(block, 'attn'):\n",
    "                    hook2 = block.attn.register_forward_hook(self._get_attention_hook(layer_idx))\n",
    "                    self.hooks.append(hook2)\n",
    "    \n",
    "    def clear_hooks(self) -> None:\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        self.stored_activations = {}\n",
    "        self.stored_attentions = {}\n",
    "    \n",
    "    def compute_field_metrics(self, prompt: str, tokenizer: AutoTokenizer,\n",
    "                              layers: List[int]) -> Dict[str, Any]:\n",
    "        \"\"\"Compute belief field metrics for a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "            tokenizer: The tokenizer\n",
    "            layers: Layers to probe\n",
    "            \n",
    "        Returns:\n",
    "            Dict with field metrics\n",
    "        \"\"\"\n",
    "        self.register_hooks(layers)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        \n",
    "        metrics = {\n",
    "            \"activation_magnitude\": {},\n",
    "            \"activation_coherence\": {},\n",
    "            \"attention_entropy\": {},\n",
    "            \"layer_correlation\": {}\n",
    "        }\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in self.stored_activations:\n",
    "                act = self.stored_activations[layer]\n",
    "                magnitude = torch.norm(act, dim=-1).mean().item()\n",
    "                metrics[\"activation_magnitude\"][layer] = magnitude\n",
    "                \n",
    "                if act.shape[1] > 1:\n",
    "                    act_flat = act[0].cpu().numpy()\n",
    "                    cos_sim = cosine_similarity(act_flat)\n",
    "                    mask = ~np.eye(cos_sim.shape[0], dtype=bool)\n",
    "                    coherence = cos_sim[mask].mean()\n",
    "                    metrics[\"activation_coherence\"][layer] = float(coherence)\n",
    "                else:\n",
    "                    metrics[\"activation_coherence\"][layer] = 1.0\n",
    "        \n",
    "        if outputs.attentions is not None:\n",
    "            for layer in layers:\n",
    "                if layer < len(outputs.attentions):\n",
    "                    attn = outputs.attentions[layer]\n",
    "                    if attn is not None:\n",
    "                        attn_probs = attn[0].mean(dim=0)\n",
    "                        last_attn = attn_probs[-1]\n",
    "                        entropy = -torch.sum(last_attn * torch.log(last_attn + 1e-10)).item()\n",
    "                        metrics[\"attention_entropy\"][layer] = entropy\n",
    "        \n",
    "        layer_acts = []\n",
    "        for layer in sorted(layers):\n",
    "            if layer in self.stored_activations:\n",
    "                act = self.stored_activations[layer][0, -1, :].cpu().numpy()\n",
    "                layer_acts.append(act)\n",
    "        \n",
    "        if len(layer_acts) > 1:\n",
    "            correlations = []\n",
    "            for i in range(len(layer_acts) - 1):\n",
    "                corr = np.corrcoef(layer_acts[i], layer_acts[i+1])[0, 1]\n",
    "                correlations.append(corr)\n",
    "            metrics[\"layer_correlation\"][\"mean\"] = float(np.mean(correlations))\n",
    "            metrics[\"layer_correlation\"][\"values\"] = [float(c) for c in correlations]\n",
    "        \n",
    "        self.clear_hooks()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "probe = BeliefFieldProbe(model)\n",
    "print(\"BeliefFieldProbe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_response_quality(prompt: str, model: nn.Module, \n",
    "                             tokenizer: AutoTokenizer, n_tokens: int = 20) -> Dict[str, float]:\n",
    "    \"\"\"Measure quality of model's response to a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        n_tokens: Tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dict with quality metrics\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=n_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs.sequences[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    if outputs.scores:\n",
    "        probs = []\n",
    "        for i, score in enumerate(outputs.scores):\n",
    "            if i < len(generated_ids):\n",
    "                prob = torch.softmax(score[0], dim=-1)\n",
    "                token_prob = prob[generated_ids[i]].item()\n",
    "                probs.append(token_prob)\n",
    "        metrics[\"confidence\"] = float(np.mean(probs)) if probs else 0.0\n",
    "    else:\n",
    "        metrics[\"confidence\"] = 0.0\n",
    "    \n",
    "    action_words = [\"run\", \"flee\", \"escape\", \"move\", \"go\", \"leave\", \"stay\", \"wait\", \n",
    "                    \"fight\", \"defend\", \"hide\", \"duck\", \"jump\", \"stop\", \"avoid\",\n",
    "                    \"quickly\", \"immediately\", \"now\", \"fast\", \"slowly\", \"carefully\"]\n",
    "    action_count = sum(1 for w in action_words if w.lower() in generated_text.lower())\n",
    "    metrics[\"action_relevance\"] = min(1.0, action_count / 3)\n",
    "    \n",
    "    vague_words = [\"maybe\", \"perhaps\", \"might\", \"could\", \"possibly\", \"somehow\", \n",
    "                   \"something\", \"someone\", \"somewhere\", \"somewhat\"]\n",
    "    vague_count = sum(1 for w in vague_words if w.lower() in generated_text.lower())\n",
    "    metrics[\"specificity\"] = max(0.0, 1.0 - vague_count / 3)\n",
    "    \n",
    "    metrics[\"response_length\"] = len(generated_text.split())\n",
    "    \n",
    "    metrics[\"quality_score\"] = (\n",
    "        0.4 * metrics[\"confidence\"] +\n",
    "        0.3 * metrics[\"action_relevance\"] +\n",
    "        0.3 * metrics[\"specificity\"]\n",
    "    )\n",
    "    \n",
    "    metrics[\"generated_text\"] = generated_text\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Response quality measurement ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Threshold Detection Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RUNNING AQ THRESHOLD DETECTION EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "RESULTS_BY_LEVEL = {level: [] for level in config.aq_levels}\n",
    "\n",
    "for level in config.aq_levels:\n",
    "    print(f\"\\nProcessing AQ Level {level}...\")\n",
    "    prompts = GRADED_PROMPTS[level]\n",
    "    \n",
    "    for i, prompt_data in enumerate(tqdm(prompts, desc=f\"Level {level}\")):\n",
    "        prompt_text = prompt_data[\"text\"]\n",
    "        \n",
    "        field_metrics = probe.compute_field_metrics(\n",
    "            prompt_text, tokenizer, config.layers_to_probe\n",
    "        )\n",
    "        \n",
    "        response_metrics = measure_response_quality(\n",
    "            prompt_text, model, tokenizer, config.n_generate_tokens\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"prompt\": prompt_text,\n",
    "            \"aq_count\": level,\n",
    "            \"aq_present\": prompt_data[\"aq_present\"],\n",
    "            \"field_metrics\": field_metrics,\n",
    "            \"response_metrics\": response_metrics\n",
    "        }\n",
    "        \n",
    "        RESULTS_BY_LEVEL[level].append(result)\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
    "\n",
    "print(\"\\nExperiment complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Threshold Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS: THRESHOLD EFFECTS BY AQ LEVEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aggregated = {}\n",
    "\n",
    "for level in config.aq_levels:\n",
    "    results = RESULTS_BY_LEVEL[level]\n",
    "    \n",
    "    quality_scores = [r[\"response_metrics\"][\"quality_score\"] for r in results]\n",
    "    confidences = [r[\"response_metrics\"][\"confidence\"] for r in results]\n",
    "    action_relevances = [r[\"response_metrics\"][\"action_relevance\"] for r in results]\n",
    "    \n",
    "    mean_magnitudes = []\n",
    "    mean_coherences = []\n",
    "    mean_entropies = []\n",
    "    \n",
    "    for r in results:\n",
    "        fm = r[\"field_metrics\"]\n",
    "        if fm[\"activation_magnitude\"]:\n",
    "            mean_magnitudes.append(np.mean(list(fm[\"activation_magnitude\"].values())))\n",
    "        if fm[\"activation_coherence\"]:\n",
    "            mean_coherences.append(np.mean(list(fm[\"activation_coherence\"].values())))\n",
    "        if fm[\"attention_entropy\"]:\n",
    "            mean_entropies.append(np.mean(list(fm[\"attention_entropy\"].values())))\n",
    "    \n",
    "    aggregated[level] = {\n",
    "        \"quality_score\": {\n",
    "            \"mean\": float(np.mean(quality_scores)),\n",
    "            \"std\": float(np.std(quality_scores)),\n",
    "            \"values\": quality_scores\n",
    "        },\n",
    "        \"confidence\": {\n",
    "            \"mean\": float(np.mean(confidences)),\n",
    "            \"std\": float(np.std(confidences))\n",
    "        },\n",
    "        \"action_relevance\": {\n",
    "            \"mean\": float(np.mean(action_relevances)),\n",
    "            \"std\": float(np.std(action_relevances))\n",
    "        },\n",
    "        \"activation_magnitude\": {\n",
    "            \"mean\": float(np.mean(mean_magnitudes)) if mean_magnitudes else 0,\n",
    "            \"std\": float(np.std(mean_magnitudes)) if mean_magnitudes else 0\n",
    "        },\n",
    "        \"activation_coherence\": {\n",
    "            \"mean\": float(np.mean(mean_coherences)) if mean_coherences else 0,\n",
    "            \"std\": float(np.std(mean_coherences)) if mean_coherences else 0\n",
    "        },\n",
    "        \"attention_entropy\": {\n",
    "            \"mean\": float(np.mean(mean_entropies)) if mean_entropies else 0,\n",
    "            \"std\": float(np.std(mean_entropies)) if mean_entropies else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAQ Level {level}:\")\n",
    "    print(f\"  Quality Score: {aggregated[level]['quality_score']['mean']:.3f} +/- {aggregated[level]['quality_score']['std']:.3f}\")\n",
    "    print(f\"  Confidence: {aggregated[level]['confidence']['mean']:.3f}\")\n",
    "    print(f\"  Action Relevance: {aggregated[level]['action_relevance']['mean']:.3f}\")\n",
    "    print(f\"  Activation Magnitude: {aggregated[level]['activation_magnitude']['mean']:.3f}\")\n",
    "    print(f\"  Activation Coherence: {aggregated[level]['activation_coherence']['mean']:.3f}\")\n",
    "    print(f\"  Attention Entropy: {aggregated[level]['attention_entropy']['mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL TESTS: THRESHOLD DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "threshold_tests = []\n",
    "\n",
    "for i in range(len(config.aq_levels) - 1):\n",
    "    level_low = config.aq_levels[i]\n",
    "    level_high = config.aq_levels[i + 1]\n",
    "    \n",
    "    quality_low = aggregated[level_low][\"quality_score\"][\"values\"]\n",
    "    quality_high = aggregated[level_high][\"quality_score\"][\"values\"]\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(quality_low, quality_high)\n",
    "    \n",
    "    pooled_std = np.sqrt((np.var(quality_low) + np.var(quality_high)) / 2)\n",
    "    cohens_d = (np.mean(quality_high) - np.mean(quality_low)) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    threshold_tests.append({\n",
    "        \"transition\": f\"{level_low} -> {level_high}\",\n",
    "        \"mean_low\": np.mean(quality_low),\n",
    "        \"mean_high\": np.mean(quality_high),\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"cohens_d\": cohens_d\n",
    "    })\n",
    "    \n",
    "    sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "    print(f\"\\n{level_low} AQ -> {level_high} AQ:\")\n",
    "    print(f\"  Quality change: {np.mean(quality_low):.3f} -> {np.mean(quality_high):.3f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "    print(f\"  p-value: {p_value:.4f} {sig}\")\n",
    "    print(f\"  Cohen's d: {cohens_d:.3f}\")\n",
    "\n",
    "if threshold_tests:\n",
    "    critical_transition = max(threshold_tests, key=lambda x: x[\"cohens_d\"])\n",
    "    print(f\"\\n\" + \"-\" * 70)\n",
    "    print(f\"CRITICAL THRESHOLD DETECTED: {critical_transition['transition']}\")\n",
    "    print(f\"Effect size (d): {critical_transition['cohens_d']:.3f}\")\n",
    "    print(f\"p-value: {critical_transition['p_value']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Belief Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Response Quality by AQ Level\n",
    "ax = axes[0, 0]\n",
    "levels = list(aggregated.keys())\n",
    "quality_means = [aggregated[l][\"quality_score\"][\"mean\"] for l in levels]\n",
    "quality_stds = [aggregated[l][\"quality_score\"][\"std\"] for l in levels]\n",
    "ax.errorbar(levels, quality_means, yerr=quality_stds, marker='o', capsize=5, linewidth=2, markersize=8)\n",
    "ax.set_xlabel(\"AQ Count in Prompt\")\n",
    "ax.set_ylabel(\"Response Quality Score\")\n",
    "ax.set_title(\"Response Quality vs AQ Count\\n(Threshold Detection)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=np.mean(quality_means), color='r', linestyle='--', alpha=0.5, label='Mean')\n",
    "ax.legend()\n",
    "\n",
    "# 2. Activation Magnitude (Field Excitation)\n",
    "ax = axes[0, 1]\n",
    "mag_means = [aggregated[l][\"activation_magnitude\"][\"mean\"] for l in levels]\n",
    "mag_stds = [aggregated[l][\"activation_magnitude\"][\"std\"] for l in levels]\n",
    "ax.errorbar(levels, mag_means, yerr=mag_stds, marker='s', capsize=5, linewidth=2, markersize=8, color='green')\n",
    "ax.set_xlabel(\"AQ Count in Prompt\")\n",
    "ax.set_ylabel(\"Mean Activation Magnitude\")\n",
    "ax.set_title(\"Field Excitation vs AQ Count\\n(Activation Magnitude)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Activation Coherence (Field Alignment)\n",
    "ax = axes[0, 2]\n",
    "coh_means = [aggregated[l][\"activation_coherence\"][\"mean\"] for l in levels]\n",
    "coh_stds = [aggregated[l][\"activation_coherence\"][\"std\"] for l in levels]\n",
    "ax.errorbar(levels, coh_means, yerr=coh_stds, marker='^', capsize=5, linewidth=2, markersize=8, color='purple')\n",
    "ax.set_xlabel(\"AQ Count in Prompt\")\n",
    "ax.set_ylabel(\"Activation Coherence\")\n",
    "ax.set_title(\"Field Coherence vs AQ Count\\n(Representation Alignment)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Attention Entropy (Focus vs Diffuse)\n",
    "ax = axes[1, 0]\n",
    "ent_means = [aggregated[l][\"attention_entropy\"][\"mean\"] for l in levels]\n",
    "ent_stds = [aggregated[l][\"attention_entropy\"][\"std\"] for l in levels]\n",
    "ax.errorbar(levels, ent_means, yerr=ent_stds, marker='d', capsize=5, linewidth=2, markersize=8, color='orange')\n",
    "ax.set_xlabel(\"AQ Count in Prompt\")\n",
    "ax.set_ylabel(\"Attention Entropy\")\n",
    "ax.set_title(\"Attention Focus vs AQ Count\\n(Lower = More Focused)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Effect Size by Transition\n",
    "ax = axes[1, 1]\n",
    "if threshold_tests:\n",
    "    transitions = [t[\"transition\"] for t in threshold_tests]\n",
    "    effects = [t[\"cohens_d\"] for t in threshold_tests]\n",
    "    colors = ['green' if t[\"p_value\"] < 0.05 else 'gray' for t in threshold_tests]\n",
    "    ax.bar(transitions, effects, color=colors, alpha=0.7)\n",
    "    ax.axhline(y=0.8, color='red', linestyle='--', label='Large effect (d=0.8)')\n",
    "    ax.axhline(y=0.5, color='orange', linestyle='--', label='Medium effect (d=0.5)')\n",
    "    ax.set_xlabel(\"AQ Level Transition\")\n",
    "    ax.set_ylabel(\"Cohen's d\")\n",
    "    ax.set_title(\"Effect Size at Each AQ Transition\\n(Green = p < 0.05)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Confidence vs AQ Level\n",
    "ax = axes[1, 2]\n",
    "conf_means = [aggregated[l][\"confidence\"][\"mean\"] for l in levels]\n",
    "conf_stds = [aggregated[l][\"confidence\"][\"std\"] for l in levels]\n",
    "ax.errorbar(levels, conf_means, yerr=conf_stds, marker='o', capsize=5, linewidth=2, markersize=8, color='red')\n",
    "ax.set_xlabel(\"AQ Count in Prompt\")\n",
    "ax.set_ylabel(\"Model Confidence\")\n",
    "ax.set_title(\"Model Confidence vs AQ Count\\n(Token Generation Probability)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"035I: AQ Excitation Threshold - Belief Field Visualization\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"035I_threshold_detection.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 035I_threshold_detection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Layer-wise Field Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LAYER-WISE BELIEF FIELD ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "layer_data = {layer: {level: {\"magnitude\": [], \"coherence\": []} \n",
    "                      for level in config.aq_levels}\n",
    "              for layer in config.layers_to_probe}\n",
    "\n",
    "for level in config.aq_levels:\n",
    "    for result in RESULTS_BY_LEVEL[level]:\n",
    "        fm = result[\"field_metrics\"]\n",
    "        for layer in config.layers_to_probe:\n",
    "            if layer in fm[\"activation_magnitude\"]:\n",
    "                layer_data[layer][level][\"magnitude\"].append(fm[\"activation_magnitude\"][layer])\n",
    "            if layer in fm[\"activation_coherence\"]:\n",
    "                layer_data[layer][level][\"coherence\"].append(fm[\"activation_coherence\"][layer])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Magnitude heatmap\n",
    "ax = axes[0]\n",
    "mag_matrix = np.zeros((len(config.layers_to_probe), len(config.aq_levels)))\n",
    "for i, layer in enumerate(config.layers_to_probe):\n",
    "    for j, level in enumerate(config.aq_levels):\n",
    "        values = layer_data[layer][level][\"magnitude\"]\n",
    "        mag_matrix[i, j] = np.mean(values) if values else 0\n",
    "\n",
    "sns.heatmap(mag_matrix, ax=ax, cmap='YlOrRd',\n",
    "            xticklabels=config.aq_levels, yticklabels=config.layers_to_probe,\n",
    "            annot=True, fmt='.1f')\n",
    "ax.set_xlabel(\"AQ Count\")\n",
    "ax.set_ylabel(\"Layer\")\n",
    "ax.set_title(\"Activation Magnitude by Layer and AQ Level\\n(Field Excitation)\")\n",
    "\n",
    "# Coherence heatmap\n",
    "ax = axes[1]\n",
    "coh_matrix = np.zeros((len(config.layers_to_probe), len(config.aq_levels)))\n",
    "for i, layer in enumerate(config.layers_to_probe):\n",
    "    for j, level in enumerate(config.aq_levels):\n",
    "        values = layer_data[layer][level][\"coherence\"]\n",
    "        coh_matrix[i, j] = np.mean(values) if values else 0\n",
    "\n",
    "sns.heatmap(coh_matrix, ax=ax, cmap='YlGnBu',\n",
    "            xticklabels=config.aq_levels, yticklabels=config.layers_to_probe,\n",
    "            annot=True, fmt='.2f')\n",
    "ax.set_xlabel(\"AQ Count\")\n",
    "ax.set_ylabel(\"Layer\")\n",
    "ax.set_title(\"Activation Coherence by Layer and AQ Level\\n(Field Alignment)\")\n",
    "\n",
    "plt.suptitle(\"035I: Layer-wise Belief Field State\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"035I_layer_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 035I_layer_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 035I: AQ EXCITATION THRESHOLD - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nExperiment Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  AQ levels tested: {config.aq_levels}\")\n",
    "print(f\"  Prompts per level: {config.n_prompts_per_level}\")\n",
    "print(f\"  Total prompts: {sum(len(GRADED_PROMPTS[l]) for l in config.aq_levels)}\")\n",
    "\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if threshold_tests:\n",
    "    critical = max(threshold_tests, key=lambda x: x[\"cohens_d\"])\n",
    "    print(f\"\\n1. CRITICAL THRESHOLD:\")\n",
    "    print(f\"   The largest quality jump occurs at: {critical['transition']}\")\n",
    "    print(f\"   Effect size (Cohen's d): {critical['cohens_d']:.3f}\")\n",
    "    print(f\"   Statistical significance: p = {critical['p_value']:.6f}\")\n",
    "    \n",
    "    if critical['cohens_d'] > 0.8:\n",
    "        print(f\"   Interpretation: LARGE effect - strong evidence for threshold\")\n",
    "    elif critical['cohens_d'] > 0.5:\n",
    "        print(f\"   Interpretation: MEDIUM effect - moderate evidence for threshold\")\n",
    "    else:\n",
    "        print(f\"   Interpretation: SMALL effect - weak evidence for threshold\")\n",
    "\n",
    "print(f\"\\n2. FIELD EXCITATION PATTERN:\")\n",
    "mag_trend = [aggregated[l][\"activation_magnitude\"][\"mean\"] for l in config.aq_levels]\n",
    "mag_corr = np.corrcoef(config.aq_levels, mag_trend)[0,1]\n",
    "print(f\"   Magnitude correlation with AQ: r = {mag_corr:.3f}\")\n",
    "if mag_corr > 0.5:\n",
    "    print(f\"   Activation magnitude INCREASES with AQ count\")\n",
    "else:\n",
    "    print(f\"   Activation magnitude shows NO clear trend with AQ count\")\n",
    "\n",
    "print(f\"\\n3. FIELD COHERENCE PATTERN:\")\n",
    "coh_trend = [aggregated[l][\"activation_coherence\"][\"mean\"] for l in config.aq_levels]\n",
    "coh_corr = np.corrcoef(config.aq_levels, coh_trend)[0,1]\n",
    "print(f\"   Coherence correlation with AQ: r = {coh_corr:.3f}\")\n",
    "if coh_corr > 0.3:\n",
    "    print(f\"   Higher AQ count -> Higher coherence (more aligned field)\")\n",
    "elif coh_corr < -0.3:\n",
    "    print(f\"   Higher AQ count -> Lower coherence (more distributed field)\")\n",
    "else:\n",
    "    print(f\"   No strong coherence trend with AQ count\")\n",
    "\n",
    "print(f\"\\n4. ATTENTION PATTERN:\")\n",
    "ent_trend = [aggregated[l][\"attention_entropy\"][\"mean\"] for l in config.aq_levels]\n",
    "ent_corr = np.corrcoef(config.aq_levels, ent_trend)[0,1]\n",
    "print(f\"   Entropy correlation with AQ: r = {ent_corr:.3f}\")\n",
    "if ent_corr < -0.3:\n",
    "    print(f\"   Higher AQ count -> Lower entropy (more focused attention)\")\n",
    "elif ent_corr > 0.3:\n",
    "    print(f\"   Higher AQ count -> Higher entropy (more diffuse attention)\")\n",
    "else:\n",
    "    print(f\"   No strong entropy trend with AQ count\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCLUSIONS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "threshold_supported = (\n",
    "    threshold_tests and \n",
    "    critical['cohens_d'] > 0.5 and \n",
    "    critical['p_value'] < 0.05\n",
    ")\n",
    "\n",
    "if threshold_supported:\n",
    "    print(f\"\\n  THRESHOLD HYPOTHESIS: SUPPORTED\")\n",
    "    print(f\"  There IS a minimum AQ count required for coherent responses.\")\n",
    "    print(f\"  Critical threshold appears at: {critical['transition']}\")\n",
    "    print(f\"  Below this threshold, the model cannot construct coherent action responses.\")\n",
    "    print(f\"  Above this threshold, AQ resonate with the weight field and bond into answers.\")\n",
    "else:\n",
    "    print(f\"\\n  THRESHOLD HYPOTHESIS: NOT CLEARLY SUPPORTED\")\n",
    "    print(f\"  No sharp threshold detected in this experiment.\")\n",
    "    print(f\"  Quality may increase gradually rather than in a phase transition.\")\n",
    "\n",
    "print(f\"\\n  BELIEF FIELD VISUALIZATION:\")\n",
    "print(f\"  While we cannot 'see' the belief field directly, proxies show:\")\n",
    "print(f\"  - Activation magnitude varies by AQ count\")\n",
    "print(f\"  - Coherence patterns emerge at different layers\")\n",
    "print(f\"  - Attention focus changes with actionable information\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_output = {\n",
    "    \"config\": {\n",
    "        \"model\": config.model_name,\n",
    "        \"aq_levels\": config.aq_levels,\n",
    "        \"n_prompts_per_level\": config.n_prompts_per_level,\n",
    "        \"layers_probed\": config.layers_to_probe\n",
    "    },\n",
    "    \"aggregated_by_level\": {str(k): {kk: vv for kk, vv in v.items() if kk != \"quality_score\" or kk == \"quality_score\" and isinstance(vv, dict) and \"values\" not in vv} for k, v in aggregated.items()},\n",
    "    \"threshold_tests\": threshold_tests,\n",
    "    \"conclusions\": {\n",
    "        \"threshold_supported\": threshold_supported,\n",
    "        \"critical_transition\": critical[\"transition\"] if threshold_tests else None,\n",
    "        \"critical_effect_size\": critical[\"cohens_d\"] if threshold_tests else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Simplify for JSON serialization\n",
    "for level_key in results_output[\"aggregated_by_level\"]:\n",
    "    level_data = results_output[\"aggregated_by_level\"][level_key]\n",
    "    if \"quality_score\" in level_data and \"values\" in level_data[\"quality_score\"]:\n",
    "        del level_data[\"quality_score\"][\"values\"]\n",
    "\n",
    "with open(\"035I_results.json\", \"w\") as f:\n",
    "    json.dump(results_output, f, indent=2, default=str)\n",
    "\n",
    "print(\"Results saved to 035I_results.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
