{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035E: Cross-Model AQ Validation with Real AQ Prompts\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "Test whether AQ (Action Quanta) cluster by **action discrimination type** across multiple models.\n",
    "\n",
    "Unlike previous experiments that tested output format categories (COMPUTE_NUMBER, etc.),\n",
    "this experiment tests **true AQ** - patterns that enable discrimination between action alternatives:\n",
    "- FLEE vs STAY\n",
    "- APPROACH vs AVOID\n",
    "- URGENT vs DELAYED\n",
    "- ATTACK vs DEFEND\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "If AQ are fundamental to how LLMs represent actionable information:\n",
    "1. Prompts requiring the same action discrimination should cluster together\n",
    "2. This clustering should appear across different model families\n",
    "3. Clustering should be strongest in late layers (where crystallization occurs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for Colab\n",
    "!pip install transformers torch numpy scikit-learn matplotlib seaborn scipy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for cross-model AQ validation.\"\"\"\n",
    "    \n",
    "    models: Dict[str, str] = field(default_factory=lambda: {\n",
    "        \"gpt2-medium\": \"gpt2-medium\",\n",
    "        \"pythia-410m\": \"EleutherAI/pythia-410m\",\n",
    "        \"pythia-1.4b\": \"EleutherAI/pythia-1.4b\",\n",
    "    })\n",
    "    \n",
    "    prompts_per_category: int = 200\n",
    "    n_bootstrap: int = 1000\n",
    "    test_size: float = 0.2\n",
    "    random_seed: int = 42\n",
    "    layers_to_probe: List[int] = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Models to test: {list(config.models.keys())}\")\n",
    "print(f\"Prompts per category: {config.prompts_per_category}\")\n",
    "print(f\"Bootstrap samples: {config.n_bootstrap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. True AQ Prompt Generation\n",
    "\n",
    "These prompts test **action discriminations**, not output formats:\n",
    "\n",
    "| AQ Type | Discrimination | Example |\n",
    "|---------|----------------|----------|\n",
    "| FLEE_STAY | Should I flee or stay? | \"A predator approaches...\" |\n",
    "| APPROACH_AVOID | Should I approach or avoid? | \"A strange object appears...\" |\n",
    "| URGENT_DELAYED | Act now or wait? | \"The situation is escalating...\" |\n",
    "| ATTACK_DEFEND | Offensive or defensive? | \"The enemy is vulnerable...\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRUE AQ CATEGORIES - Action Discriminations\n",
    "\n",
    "AQ_CATEGORIES = {\n",
    "    \"FLEE_STAY\": {\n",
    "        \"description\": \"Discriminates between fleeing and staying\",\n",
    "        \"discrimination\": \"FLEE vs STAY\",\n",
    "        \"templates\": [\n",
    "            \"A predator is approaching rapidly. You should\",\n",
    "            \"The building is on fire and spreading. You must\",\n",
    "            \"An attacker is coming toward you. You need to\",\n",
    "            \"The flood waters are rising quickly. You should\",\n",
    "            \"A dangerous snake is nearby. You must\",\n",
    "            \"The volcano is erupting. You need to\",\n",
    "            \"Toxic gas is leaking into the room. You should\",\n",
    "            \"The ice beneath you is cracking. You must\",\n",
    "            \"A swarm of bees is heading your way. You need to\",\n",
    "            \"The ceiling is about to collapse. You should\",\n",
    "            \"A wild bear has spotted you. You must\",\n",
    "            \"The tsunami warning has sounded. You need to\",\n",
    "            \"Gunshots are getting closer. You should\",\n",
    "            \"The car is about to explode. You must\",\n",
    "            \"A tornado is forming nearby. You need to\",\n",
    "            \"The bridge is collapsing. You should\",\n",
    "            \"Lava is flowing toward you. You must\",\n",
    "            \"The bomb will detonate soon. You need to\",\n",
    "            \"A pack of wolves surrounds you. You should\",\n",
    "            \"The radiation levels are rising. You must\",\n",
    "        ]\n",
    "    },\n",
    "    \"APPROACH_AVOID\": {\n",
    "        \"description\": \"Discriminates between approaching and avoiding\",\n",
    "        \"discrimination\": \"APPROACH vs AVOID\",\n",
    "        \"templates\": [\n",
    "            \"A strange glowing object appears ahead. You should\",\n",
    "            \"An unknown person is waving at you. You need to\",\n",
    "            \"A mysterious door has opened. You should\",\n",
    "            \"An unfamiliar animal is watching you. You must\",\n",
    "            \"A new path has revealed itself. You need to\",\n",
    "            \"Something is moving in the shadows. You should\",\n",
    "            \"A light flickers in the distance. You must\",\n",
    "            \"An unmarked package sits on the ground. You need to\",\n",
    "            \"A voice calls from the darkness. You should\",\n",
    "            \"A hidden passage has been discovered. You must\",\n",
    "            \"An abandoned vehicle blocks the road. You need to\",\n",
    "            \"A figure stands motionless ahead. You should\",\n",
    "            \"Something shiny catches your eye. You must\",\n",
    "            \"A cave entrance appears before you. You need to\",\n",
    "            \"An unusual sound comes from nearby. You should\",\n",
    "            \"A locked chest sits in the corner. You must\",\n",
    "            \"A trail of footprints leads away. You need to\",\n",
    "            \"A reflection appears in the water. You should\",\n",
    "            \"A signal fire burns on the hill. You must\",\n",
    "            \"A message is carved into the tree. You need to\",\n",
    "        ]\n",
    "    },\n",
    "    \"URGENT_DELAYED\": {\n",
    "        \"description\": \"Discriminates between acting now and waiting\",\n",
    "        \"discrimination\": \"NOW vs LATER\",\n",
    "        \"templates\": [\n",
    "            \"The window of opportunity is closing. You should\",\n",
    "            \"The situation is rapidly escalating. You must\",\n",
    "            \"Time is running out to decide. You need to\",\n",
    "            \"The moment to act is almost gone. You should\",\n",
    "            \"Events are unfolding quickly. You must\",\n",
    "            \"The deadline is approaching fast. You need to\",\n",
    "            \"Circumstances are changing rapidly. You should\",\n",
    "            \"The critical moment is here. You must\",\n",
    "            \"Things are moving too fast. You need to\",\n",
    "            \"The situation demands immediate action. You should\",\n",
    "            \"Everything is happening at once. You must\",\n",
    "            \"The pressure is mounting quickly. You need to\",\n",
    "            \"Events are spiraling out of control. You should\",\n",
    "            \"The turning point has arrived. You must\",\n",
    "            \"Momentum is building rapidly. You need to\",\n",
    "            \"The situation is reaching a climax. You should\",\n",
    "            \"Forces are converging now. You must\",\n",
    "            \"The crisis point is imminent. You need to\",\n",
    "            \"Everything hangs in the balance. You should\",\n",
    "            \"The decisive moment approaches. You must\",\n",
    "        ]\n",
    "    },\n",
    "    \"ATTACK_DEFEND\": {\n",
    "        \"description\": \"Discriminates between offensive and defensive action\",\n",
    "        \"discrimination\": \"ATTACK vs DEFEND\",\n",
    "        \"templates\": [\n",
    "            \"The enemy has left an opening. You should\",\n",
    "            \"Your opponent is off balance. You must\",\n",
    "            \"A weakness has been revealed. You need to\",\n",
    "            \"The adversary is distracted. You should\",\n",
    "            \"An opportunity to strike appears. You must\",\n",
    "            \"The hostile force is vulnerable. You need to\",\n",
    "            \"Your rival has made a mistake. You should\",\n",
    "            \"The threat has lowered its guard. You must\",\n",
    "            \"A gap in the defense appears. You need to\",\n",
    "            \"The aggressor is momentarily stunned. You should\",\n",
    "            \"Your foe is recovering from a blow. You must\",\n",
    "            \"The opposition is in disarray. You need to\",\n",
    "            \"A critical vulnerability is exposed. You should\",\n",
    "            \"The enemy formation has broken. You must\",\n",
    "            \"Your adversary is retreating. You need to\",\n",
    "            \"The hostile target is isolated. You should\",\n",
    "            \"A flanking opportunity presents itself. You must\",\n",
    "            \"The opposing force is overextended. You need to\",\n",
    "            \"Your enemy is out of position. You should\",\n",
    "            \"The threat has exhausted its resources. You must\",\n",
    "        ]\n",
    "    },\n",
    "    \"SHARE_KEEP\": {\n",
    "        \"description\": \"Discriminates between sharing and keeping\",\n",
    "        \"discrimination\": \"SHARE vs KEEP\",\n",
    "        \"templates\": [\n",
    "            \"You have found limited supplies. You should\",\n",
    "            \"Resources are scarce for everyone. You must\",\n",
    "            \"Others are in need of what you have. You need to\",\n",
    "            \"Your provisions could help the group. You should\",\n",
    "            \"The community is struggling. You must\",\n",
    "            \"You possess something valuable. You need to\",\n",
    "            \"Others depend on your generosity. You should\",\n",
    "            \"Your reserves could make a difference. You must\",\n",
    "            \"The group faces shortage. You need to\",\n",
    "            \"What you have could save others. You should\",\n",
    "            \"Your surplus could help many. You must\",\n",
    "            \"The collective need is great. You need to\",\n",
    "            \"Your abundance contrasts with others' lack. You should\",\n",
    "            \"Sharing would cost you significantly. You must\",\n",
    "            \"Others are watching what you do. You need to\",\n",
    "            \"Your choice affects the community. You should\",\n",
    "            \"Generosity has consequences here. You must\",\n",
    "            \"Self-preservation conflicts with helping. You need to\",\n",
    "            \"The moral choice is unclear. You should\",\n",
    "            \"Your decision will be remembered. You must\",\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(AQ_CATEGORIES)} AQ categories:\")\n",
    "for name, data in AQ_CATEGORIES.items():\n",
    "    print(f\"  {name}: {data['discrimination']} ({len(data['templates'])} templates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aq_prompts(n_per_category: int = 200) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Generate prompts for each AQ category.\n",
    "    \n",
    "    Args:\n",
    "        n_per_category: Number of prompts per AQ category\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (prompts, labels)\n",
    "    \"\"\"\n",
    "    all_prompts = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for category_name, category_data in AQ_CATEGORIES.items():\n",
    "        templates = category_data[\"templates\"]\n",
    "        \n",
    "        for i in range(n_per_category):\n",
    "            template = templates[i % len(templates)]\n",
    "            \n",
    "            # Add variations\n",
    "            variations = [\n",
    "                template,\n",
    "                template.replace(\"You should\", \"You must\"),\n",
    "                template.replace(\"You should\", \"You need to\"),\n",
    "                template.replace(\"You must\", \"You should\"),\n",
    "                template.replace(\"You need to\", \"You should\"),\n",
    "                \"Warning: \" + template,\n",
    "                \"Alert: \" + template,\n",
    "                template + \" quickly\",\n",
    "                template + \" immediately\",\n",
    "                template + \" now\",\n",
    "            ]\n",
    "            \n",
    "            prompt = variations[i % len(variations)]\n",
    "            all_prompts.append(prompt)\n",
    "            all_labels.append(category_name)\n",
    "    \n",
    "    return all_prompts, all_labels\n",
    "\n",
    "\n",
    "PROMPTS, LABELS = generate_aq_prompts(config.prompts_per_category)\n",
    "print(f\"\\nGenerated {len(PROMPTS)} prompts total\")\n",
    "print(f\"Categories: {list(set(LABELS))}\")\n",
    "print(f\"\\nExample prompts:\")\n",
    "for cat in list(AQ_CATEGORIES.keys())[:3]:\n",
    "    idx = LABELS.index(cat)\n",
    "    print(f\"  {cat}: {PROMPTS[idx][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layers(model_name: str) -> List[int]:\n",
    "    \"\"\"Get layer indices to probe for a model.\"\"\"\n",
    "    if \"gpt2\" in model_name:\n",
    "        return [0, 4, 8, 12, 16, 20, 23]\n",
    "    elif \"pythia-410m\" in model_name:\n",
    "        return [0, 4, 8, 12, 16, 20, 23]\n",
    "    elif \"pythia-1.4b\" in model_name:\n",
    "        return [0, 4, 8, 12, 16, 20, 23]\n",
    "    else:\n",
    "        return [0, 4, 8, 12]\n",
    "\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    \"\"\"Load model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_activation(prompt: str, model, tokenizer, layer: int) -> np.ndarray:\n",
    "    \"\"\"Get activation at specified layer for a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    activations = {}\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations[\"act\"] = output[0].detach()\n",
    "        else:\n",
    "            activations[\"act\"] = output.detach()\n",
    "    \n",
    "    # Register hook\n",
    "    if hasattr(model, 'transformer'):\n",
    "        handle = model.transformer.h[layer].register_forward_hook(hook_fn)\n",
    "    elif hasattr(model, 'gpt_neox'):\n",
    "        handle = model.gpt_neox.layers[layer].register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        handle = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    \n",
    "    handle.remove()\n",
    "    \n",
    "    # Get last token activation\n",
    "    act = activations[\"act\"][0, -1, :].cpu().numpy()\n",
    "    return act\n",
    "\n",
    "\n",
    "print(\"Activation extraction functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_silhouette(activations: np.ndarray, labels: List[str], \n",
    "                         n_bootstrap: int = 1000) -> Tuple[float, float, float]:\n",
    "    \"\"\"Compute silhouette score with bootstrap confidence interval.\"\"\"\n",
    "    n_samples = len(labels)\n",
    "    bootstrap_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        boot_acts = activations[indices]\n",
    "        boot_labels = [labels[i] for i in indices]\n",
    "        \n",
    "        if len(set(boot_labels)) > 1:\n",
    "            score = silhouette_score(boot_acts, boot_labels)\n",
    "            bootstrap_scores.append(score)\n",
    "    \n",
    "    mean_score = np.mean(bootstrap_scores)\n",
    "    ci_low = np.percentile(bootstrap_scores, 2.5)\n",
    "    ci_high = np.percentile(bootstrap_scores, 97.5)\n",
    "    \n",
    "    return mean_score, ci_low, ci_high\n",
    "\n",
    "\n",
    "def compute_cohens_d(activations: np.ndarray, labels: List[str]) -> float:\n",
    "    \"\"\"Compute Cohen's d for within vs between cluster distances.\"\"\"\n",
    "    unique_labels = list(set(labels))\n",
    "    \n",
    "    within_distances = []\n",
    "    between_distances = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i+1, len(labels)):\n",
    "            dist = np.linalg.norm(activations[i] - activations[j])\n",
    "            if labels[i] == labels[j]:\n",
    "                within_distances.append(dist)\n",
    "            else:\n",
    "                between_distances.append(dist)\n",
    "    \n",
    "    within_mean = np.mean(within_distances)\n",
    "    between_mean = np.mean(between_distances)\n",
    "    pooled_std = np.sqrt((np.var(within_distances) + np.var(between_distances)) / 2)\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (between_mean - within_mean) / pooled_std\n",
    "\n",
    "\n",
    "def compute_distance_ratio(activations: np.ndarray, labels: List[str]) -> float:\n",
    "    \"\"\"Compute ratio of between-cluster to within-cluster distance.\"\"\"\n",
    "    within_distances = []\n",
    "    between_distances = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i+1, len(labels)):\n",
    "            dist = np.linalg.norm(activations[i] - activations[j])\n",
    "            if labels[i] == labels[j]:\n",
    "                within_distances.append(dist)\n",
    "            else:\n",
    "                between_distances.append(dist)\n",
    "    \n",
    "    if np.mean(within_distances) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.mean(between_distances) / np.mean(within_distances)\n",
    "\n",
    "\n",
    "print(\"Statistical functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Cross-Model Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_for_model(model_name: str, model_path: str, \n",
    "                              prompts: List[str], labels: List[str]) -> Dict:\n",
    "    \"\"\"Run full experiment for a single model.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model, tokenizer = load_model(model_path)\n",
    "    layers = get_model_layers(model_name)\n",
    "    \n",
    "    print(f\"Model loaded. Layers to probe: {layers}\")\n",
    "    \n",
    "    results = {\"layers\": {}}\n",
    "    \n",
    "    for layer in layers:\n",
    "        print(f\"\\n  Layer {layer}...\")\n",
    "        \n",
    "        # Extract activations\n",
    "        activations = []\n",
    "        for prompt in tqdm(prompts, desc=f\"    Extracting\"):\n",
    "            act = get_activation(prompt, model, tokenizer, layer)\n",
    "            activations.append(act)\n",
    "        \n",
    "        activations = np.array(activations)\n",
    "        \n",
    "        # Train/test split\n",
    "        train_acts, test_acts, train_labels, test_labels = train_test_split(\n",
    "            activations, labels, test_size=config.test_size, \n",
    "            random_state=config.random_seed, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Compute metrics on training set\n",
    "        sil_mean, sil_low, sil_high = bootstrap_silhouette(\n",
    "            train_acts, train_labels, config.n_bootstrap\n",
    "        )\n",
    "        \n",
    "        cohens_d = compute_cohens_d(train_acts, train_labels)\n",
    "        dist_ratio = compute_distance_ratio(train_acts, train_labels)\n",
    "        \n",
    "        # Test set validation\n",
    "        test_sil = silhouette_score(test_acts, test_labels)\n",
    "        \n",
    "        # K-means clustering and ARI\n",
    "        n_clusters = len(set(labels))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=config.random_seed, n_init=10)\n",
    "        pred_labels = kmeans.fit_predict(train_acts)\n",
    "        \n",
    "        label_to_int = {l: i for i, l in enumerate(sorted(set(train_labels)))}\n",
    "        true_labels_int = [label_to_int[l] for l in train_labels]\n",
    "        ari = adjusted_rand_score(true_labels_int, pred_labels)\n",
    "        \n",
    "        results[\"layers\"][layer] = {\n",
    "            \"silhouette\": sil_mean,\n",
    "            \"silhouette_ci\": [sil_low, sil_high],\n",
    "            \"cohens_d\": cohens_d,\n",
    "            \"distance_ratio\": dist_ratio,\n",
    "            \"test_silhouette\": test_sil,\n",
    "            \"ari\": ari\n",
    "        }\n",
    "        \n",
    "        print(f\"    Silhouette: {sil_mean:.3f} [{sil_low:.3f}, {sil_high:.3f}]\")\n",
    "        print(f\"    Cohen's d: {cohens_d:.3f}\")\n",
    "        print(f\"    Test Silhouette: {test_sil:.3f}\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del activations, train_acts, test_acts\n",
    "        torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
    "    \n",
    "    # Cleanup model\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Experiment runner ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING CROSS-MODEL AQ VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ALL_RESULTS = {}\n",
    "\n",
    "for model_name, model_path in config.models.items():\n",
    "    try:\n",
    "        results = run_experiment_for_model(model_name, model_path, PROMPTS, LABELS)\n",
    "        ALL_RESULTS[model_name] = results\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nAll models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Silhouette by Layer\n",
    "ax = axes[0, 0]\n",
    "for model_name, results in ALL_RESULTS.items():\n",
    "    layers = sorted(results[\"layers\"].keys())\n",
    "    sils = [results[\"layers\"][l][\"silhouette\"] for l in layers]\n",
    "    ax.plot(layers, sils, marker='o', label=model_name, linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(y=0.15, color='r', linestyle='--', alpha=0.7, label='Threshold (0.15)')\n",
    "ax.set_xlabel(\"Layer\")\n",
    "ax.set_ylabel(\"Silhouette Score\")\n",
    "ax.set_title(\"Silhouette Score by Layer (Higher = Better Clustering)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Effect Size by Layer\n",
    "ax = axes[0, 1]\n",
    "for model_name, results in ALL_RESULTS.items():\n",
    "    layers = sorted(results[\"layers\"].keys())\n",
    "    effects = [results[\"layers\"][l][\"cohens_d\"] for l in layers]\n",
    "    ax.plot(layers, effects, marker='s', label=model_name, linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Medium effect (0.5)')\n",
    "ax.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Large effect (0.8)')\n",
    "ax.set_xlabel(\"Layer\")\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title(\"Effect Size by Layer\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Best Silhouette per Model\n",
    "ax = axes[1, 0]\n",
    "model_names = list(ALL_RESULTS.keys())\n",
    "best_sils = []\n",
    "ci_lows = []\n",
    "ci_highs = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    results = ALL_RESULTS[model_name]\n",
    "    best_layer = max(results[\"layers\"].keys(), \n",
    "                     key=lambda l: results[\"layers\"][l][\"silhouette\"])\n",
    "    best_sils.append(results[\"layers\"][best_layer][\"silhouette\"])\n",
    "    ci = results[\"layers\"][best_layer][\"silhouette_ci\"]\n",
    "    ci_lows.append(best_sils[-1] - ci[0])\n",
    "    ci_highs.append(ci[1] - best_sils[-1])\n",
    "\n",
    "ax.bar(model_names, best_sils, yerr=[ci_lows, ci_highs], capsize=5, alpha=0.7)\n",
    "ax.axhline(y=0.15, color='r', linestyle='--', label='Threshold')\n",
    "ax.set_ylabel(\"Silhouette Score\")\n",
    "ax.set_title(\"Best Silhouette Score per Model (95% CI)\")\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Distance Ratio per Model\n",
    "ax = axes[1, 1]\n",
    "ratios = []\n",
    "for model_name in model_names:\n",
    "    results = ALL_RESULTS[model_name]\n",
    "    best_layer = max(results[\"layers\"].keys(), \n",
    "                     key=lambda l: results[\"layers\"][l][\"silhouette\"])\n",
    "    ratios.append(results[\"layers\"][best_layer][\"distance_ratio\"])\n",
    "\n",
    "ax.bar(model_names, ratios, alpha=0.7, color='green')\n",
    "ax.axhline(y=1.0, color='r', linestyle='--', label='No separation')\n",
    "ax.set_ylabel(\"Between/Within Distance Ratio\")\n",
    "ax.set_title(\"Cluster Separation Ratio per Model\")\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle(\"035E: Cross-Model AQ Validation (Real AQ Prompts)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"035E_real_aq_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 035E_real_aq_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBest Layer Results per Model:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Model':<15} {'Layer':<6} {'Silhouette':<12} {'95% CI':<20} {'Cohen d':<10} {'Ratio':<8} {'Test Sil':<10} {'ARI':<8}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for model_name, results in ALL_RESULTS.items():\n",
    "    best_layer = max(results[\"layers\"].keys(), \n",
    "                     key=lambda l: results[\"layers\"][l][\"silhouette\"])\n",
    "    best = results[\"layers\"][best_layer]\n",
    "    ci = best[\"silhouette_ci\"]\n",
    "    \n",
    "    print(f\"{model_name:<15} {best_layer:<6} {best['silhouette']:<12.3f} \"\n",
    "          f\"[{ci[0]:.3f}, {ci[1]:.3f}]{'':>5} {best['cohens_d']:<10.3f} \"\n",
    "          f\"{best['distance_ratio']:<8.3f} {best['test_silhouette']:<10.3f} {best['ari']:<8.3f}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Save results\n",
    "with open(\"035E_real_aq_results.json\", \"w\") as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nResults saved to 035E_real_aq_results.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
