{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035H: Causal Intervention\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "Move beyond correlation to causation using activation patching.\n",
    "\n",
    "Previous experiments (035A-G) showed correlational evidence:\n",
    "- AQ patterns cluster by action type\n",
    "- Bonded states contain component signatures\n",
    "- Phase alignment in compatible pairs\n",
    "\n",
    "This experiment tests CAUSATION:\n",
    "- If we PATCH an AQ pattern into a different context, does it CHANGE the output?\n",
    "- Does patching COMPUTE_NUMBER pattern into ANSWER_BOOLEAN shift output toward numbers?\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "If AQ patterns causally determine action type:\n",
    "1. Patching action A pattern into action B prompt should shift output toward A\n",
    "2. Effect should be largest at middle layers (peak crystallization)\n",
    "3. Random patching should NOT shift output predictably\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# !pip install transformers torch numpy scikit-learn matplotlib seaborn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for causal intervention experiment.\"\"\"\n",
    "    \n",
    "    model_name: str = \"gpt2-medium\"\n",
    "    model_path: str = \"gpt2-medium\"\n",
    "    \n",
    "    # Samples for computing mean activation patterns\n",
    "    n_pattern_samples: int = 30\n",
    "    \n",
    "    # Samples for testing intervention effects\n",
    "    n_intervention_samples: int = 50\n",
    "    \n",
    "    # Layers to test intervention\n",
    "    layers_to_test: List[int] = field(default_factory=lambda: [0, 4, 8, 12, 16, 20, 23])\n",
    "    \n",
    "    # Number of tokens to generate for measuring effect\n",
    "    n_generate_tokens: int = 5\n",
    "    \n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Layers to test: {config.layers_to_test}\")\n",
    "print(f\"Pattern samples: {config.n_pattern_samples}\")\n",
    "print(f\"Intervention samples: {config.n_intervention_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Type Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action types and their characteristic outputs\n",
    "ACTION_TYPES = {\n",
    "    \"COMPUTE_NUMBER\": {\n",
    "        \"prompts\": [\n",
    "            \"What is 5 + 3? The answer is\",\n",
    "            \"Calculate 12 minus 7. The result is\",\n",
    "            \"How much is 4 times 6? It equals\",\n",
    "            \"The sum of 9 and 4 is\",\n",
    "            \"15 plus 8 equals\",\n",
    "            \"What do you get adding 7 and 9? You get\",\n",
    "            \"Compute 20 minus 11. Answer:\",\n",
    "            \"The total of 6 and 7 is\",\n",
    "            \"3 times 9 is\",\n",
    "            \"Adding 14 to 5 gives\",\n",
    "        ],\n",
    "        \"expected_tokens\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "        \"description\": \"Numerical computation\"\n",
    "    },\n",
    "    \"ANSWER_BOOLEAN\": {\n",
    "        \"prompts\": [\n",
    "            \"Is the sky blue? Answer:\",\n",
    "            \"True or false: Water is wet.\",\n",
    "            \"Is Paris the capital of France? Yes or no:\",\n",
    "            \"Fire is cold. True or false?\",\n",
    "            \"Do fish swim? Answer:\",\n",
    "            \"Is 2 + 2 equal to 5? Response:\",\n",
    "            \"The Earth is flat. True or false?\",\n",
    "            \"Can birds fly? Answer:\",\n",
    "            \"Is ice hot? Yes or no:\",\n",
    "            \"Dogs are mammals. True or false?\",\n",
    "        ],\n",
    "        \"expected_tokens\": [\"Yes\", \"No\", \"True\", \"False\", \"yes\", \"no\", \"true\", \"false\"],\n",
    "        \"description\": \"Boolean/yes-no answers\"\n",
    "    },\n",
    "    \"THREAT_RESPONSE\": {\n",
    "        \"prompts\": [\n",
    "            \"A fire is spreading toward you. You should\",\n",
    "            \"A dangerous snake is nearby. You should\",\n",
    "            \"The building is collapsing. You should\",\n",
    "            \"An attacker approaches. You should\",\n",
    "            \"Toxic gas is leaking. You should\",\n",
    "            \"The flood is rising fast. You should\",\n",
    "            \"A predator is hunting you. You should\",\n",
    "            \"The bomb will explode. You should\",\n",
    "            \"The car is speeding toward you. You should\",\n",
    "            \"The ice is cracking. You should\",\n",
    "        ],\n",
    "        \"expected_tokens\": [\"run\", \"flee\", \"escape\", \"leave\", \"move\", \"get\", \"avoid\"],\n",
    "        \"description\": \"Threat/escape responses\"\n",
    "    },\n",
    "    \"PROVIDE_FACT\": {\n",
    "        \"prompts\": [\n",
    "            \"The capital of France is\",\n",
    "            \"The largest planet is\",\n",
    "            \"Water is made of hydrogen and\",\n",
    "            \"The speed of light is approximately\",\n",
    "            \"The chemical symbol for gold is\",\n",
    "            \"The Earth orbits the\",\n",
    "            \"The human body has 206\",\n",
    "            \"The Pacific is the largest\",\n",
    "            \"DNA stands for deoxyribo\",\n",
    "            \"The Great Wall is in\",\n",
    "        ],\n",
    "        \"expected_tokens\": [\"Paris\", \"Jupiter\", \"oxygen\", \"Sun\", \"bones\", \"ocean\", \"China\", \"Au\"],\n",
    "        \"description\": \"Factual information\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Action types: {list(ACTION_TYPES.keys())}\")\n",
    "for name, data in ACTION_TYPES.items():\n",
    "    print(f\"  {name}: {len(data['prompts'])} prompts, expects: {data['expected_tokens'][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Patching Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationPatcher:\n",
    "    \"\"\"Framework for patching activations at specific layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"Initialize patcher.\n",
    "        \n",
    "        Args:\n",
    "            model: The language model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "        self.stored_activations = {}\n",
    "        self.patch_config = None\n",
    "        \n",
    "    def _get_activation_hook(self, layer_idx: int) -> Callable:\n",
    "        \"\"\"Create hook to store activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # output is (hidden_states, ...)\n",
    "            if isinstance(output, tuple):\n",
    "                self.stored_activations[layer_idx] = output[0].detach().clone()\n",
    "            else:\n",
    "                self.stored_activations[layer_idx] = output.detach().clone()\n",
    "        return hook\n",
    "    \n",
    "    def _get_patch_hook(self, layer_idx: int, patch_activation: torch.Tensor, \n",
    "                        position: int = -1) -> Callable:\n",
    "        \"\"\"Create hook to patch activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                hidden_states = output[0]\n",
    "                # Patch at specified position (default: last token)\n",
    "                hidden_states[:, position, :] = patch_activation\n",
    "                return (hidden_states,) + output[1:]\n",
    "            else:\n",
    "                output[:, position, :] = patch_activation\n",
    "                return output\n",
    "        return hook\n",
    "    \n",
    "    def register_storage_hooks(self, layers: List[int]) -> None:\n",
    "        \"\"\"Register hooks to store activations at specified layers.\"\"\"\n",
    "        self.clear_hooks()\n",
    "        for layer_idx in layers:\n",
    "            # Access transformer blocks\n",
    "            if hasattr(self.model, 'transformer'):\n",
    "                block = self.model.transformer.h[layer_idx]\n",
    "            else:\n",
    "                block = self.model.model.layers[layer_idx]\n",
    "            hook = block.register_forward_hook(self._get_activation_hook(layer_idx))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def register_patch_hook(self, layer_idx: int, patch_activation: torch.Tensor,\n",
    "                            position: int = -1) -> None:\n",
    "        \"\"\"Register hook to patch activation at specified layer.\"\"\"\n",
    "        self.clear_hooks()\n",
    "        if hasattr(self.model, 'transformer'):\n",
    "            block = self.model.transformer.h[layer_idx]\n",
    "        else:\n",
    "            block = self.model.model.layers[layer_idx]\n",
    "        hook = block.register_forward_hook(\n",
    "            self._get_patch_hook(layer_idx, patch_activation, position)\n",
    "        )\n",
    "        self.hooks.append(hook)\n",
    "    \n",
    "    def clear_hooks(self) -> None:\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        self.stored_activations = {}\n",
    "    \n",
    "    def get_stored_activation(self, layer_idx: int) -> Optional[torch.Tensor]:\n",
    "        \"\"\"Get stored activation for a layer.\"\"\"\n",
    "        return self.stored_activations.get(layer_idx, None)\n",
    "\n",
    "\n",
    "print(\"ActivationPatcher class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(config.model_path)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "patcher = ActivationPatcher(model)\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Mean Activation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_activation(prompts: List[str], model: nn.Module, \n",
    "                            tokenizer: AutoTokenizer, patcher: ActivationPatcher,\n",
    "                            layers: List[int]) -> Dict[int, torch.Tensor]:\n",
    "    \"\"\"Compute mean activation pattern across prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompts\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        patcher: ActivationPatcher instance\n",
    "        layers: Layers to capture\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping layer to mean activation tensor\n",
    "    \"\"\"\n",
    "    all_activations = {layer: [] for layer in layers}\n",
    "    \n",
    "    patcher.register_storage_hooks(layers)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        \n",
    "        for layer in layers:\n",
    "            act = patcher.get_stored_activation(layer)\n",
    "            if act is not None:\n",
    "                # Get last token activation\n",
    "                all_activations[layer].append(act[0, -1, :].cpu())\n",
    "    \n",
    "    patcher.clear_hooks()\n",
    "    \n",
    "    # Compute means\n",
    "    mean_activations = {}\n",
    "    for layer in layers:\n",
    "        if all_activations[layer]:\n",
    "            stacked = torch.stack(all_activations[layer])\n",
    "            mean_activations[layer] = stacked.mean(dim=0).to(DEVICE)\n",
    "    \n",
    "    return mean_activations\n",
    "\n",
    "\n",
    "print(\"Computing mean activation patterns for each action type...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MEAN_PATTERNS = {}\n",
    "\n",
    "for action_type, data in ACTION_TYPES.items():\n",
    "    prompts = data[\"prompts\"][:config.n_pattern_samples]\n",
    "    print(f\"  {action_type}: {len(prompts)} prompts...\")\n",
    "    \n",
    "    MEAN_PATTERNS[action_type] = compute_mean_activation(\n",
    "        prompts, model, tokenizer, patcher, config.layers_to_test\n",
    "    )\n",
    "\n",
    "print(\"\\nMean patterns computed for all action types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Intervention Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_intervention(prompt: str, patch_activation: torch.Tensor, layer: int,\n",
    "                     model: nn.Module, tokenizer: AutoTokenizer, \n",
    "                     patcher: ActivationPatcher, n_tokens: int = 5) -> Tuple[str, List[float]]:\n",
    "    \"\"\"Run model with patched activation and generate tokens.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        patch_activation: Activation to patch in\n",
    "        layer: Layer to patch\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        patcher: ActivationPatcher instance\n",
    "        n_tokens: Number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (generated_text, token_probs)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    generated_tokens = []\n",
    "    token_probs = []\n",
    "    \n",
    "    for _ in range(n_tokens):\n",
    "        # Register patch hook\n",
    "        patcher.register_patch_hook(layer, patch_activation, position=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get next token\n",
    "            next_token = torch.argmax(probs, dim=-1)\n",
    "            generated_tokens.append(next_token.item())\n",
    "            token_probs.append(probs[0, next_token].item())\n",
    "            \n",
    "            # Append to input\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        patcher.clear_hooks()\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text, token_probs\n",
    "\n",
    "\n",
    "def run_baseline(prompt: str, model: nn.Module, tokenizer: AutoTokenizer,\n",
    "                 n_tokens: int = 5) -> Tuple[str, List[float]]:\n",
    "    \"\"\"Run model without intervention.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        n_tokens: Number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (generated_text, token_probs)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    generated_tokens = []\n",
    "    token_probs = []\n",
    "    \n",
    "    for _ in range(n_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            next_token = torch.argmax(probs, dim=-1)\n",
    "            generated_tokens.append(next_token.item())\n",
    "            token_probs.append(probs[0, next_token].item())\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text, token_probs\n",
    "\n",
    "\n",
    "print(\"Intervention functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_shift_toward_target(generated_text: str, target_tokens: List[str]) -> float:\n",
    "    \"\"\"Measure how much generated text matches target action type.\n",
    "    \n",
    "    Args:\n",
    "        generated_text: Text generated by model\n",
    "        target_tokens: Expected tokens for target action type\n",
    "        \n",
    "    Returns:\n",
    "        Score (0-1) indicating match with target\n",
    "    \"\"\"\n",
    "    generated_lower = generated_text.lower()\n",
    "    \n",
    "    # Check if any target token appears\n",
    "    matches = 0\n",
    "    for token in target_tokens:\n",
    "        if token.lower() in generated_lower:\n",
    "            matches += 1\n",
    "    \n",
    "    return min(1.0, matches / max(1, len(target_tokens) / 2))\n",
    "\n",
    "\n",
    "def compute_effect_size(baseline_scores: List[float], \n",
    "                        intervention_scores: List[float]) -> float:\n",
    "    \"\"\"Compute Cohen's d effect size.\n",
    "    \n",
    "    Args:\n",
    "        baseline_scores: Scores without intervention\n",
    "        intervention_scores: Scores with intervention\n",
    "        \n",
    "    Returns:\n",
    "        Cohen's d\n",
    "    \"\"\"\n",
    "    baseline_mean = np.mean(baseline_scores)\n",
    "    intervention_mean = np.mean(intervention_scores)\n",
    "    \n",
    "    pooled_std = np.sqrt((np.var(baseline_scores) + np.var(intervention_scores)) / 2)\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (intervention_mean - baseline_mean) / pooled_std\n",
    "\n",
    "\n",
    "print(\"Measurement functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Intervention Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intervention pairs\n",
    "# Format: (source_action, target_prompt_action)\n",
    "INTERVENTION_PAIRS = [\n",
    "    (\"COMPUTE_NUMBER\", \"ANSWER_BOOLEAN\"),  # Patch number pattern into boolean context\n",
    "    (\"ANSWER_BOOLEAN\", \"COMPUTE_NUMBER\"),  # Patch boolean pattern into number context\n",
    "    (\"THREAT_RESPONSE\", \"PROVIDE_FACT\"),   # Patch threat pattern into fact context\n",
    "    (\"PROVIDE_FACT\", \"THREAT_RESPONSE\"),   # Patch fact pattern into threat context\n",
    "]\n",
    "\n",
    "print(\"Intervention pairs:\")\n",
    "for source, target in INTERVENTION_PAIRS:\n",
    "    print(f\"  Patch {source} pattern -> {target} prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_intervention_experiment(source_type: str, target_type: str,\n",
    "                                 mean_patterns: Dict, action_types: Dict,\n",
    "                                 model, tokenizer, patcher,\n",
    "                                 layers: List[int], n_samples: int) -> Dict[str, Any]:\n",
    "    \"\"\"Run intervention experiment for a source->target pair.\n",
    "    \n",
    "    Args:\n",
    "        source_type: Action type to get pattern from\n",
    "        target_type: Action type to patch into\n",
    "        mean_patterns: Dict of mean activation patterns\n",
    "        action_types: Dict of action type data\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        patcher: ActivationPatcher\n",
    "        layers: Layers to test\n",
    "        n_samples: Number of samples\n",
    "        \n",
    "    Returns:\n",
    "        Dict with results per layer\n",
    "    \"\"\"\n",
    "    results = {\"layers\": {}}\n",
    "    \n",
    "    target_prompts = action_types[target_type][\"prompts\"][:n_samples]\n",
    "    source_expected = action_types[source_type][\"expected_tokens\"]\n",
    "    target_expected = action_types[target_type][\"expected_tokens\"]\n",
    "    \n",
    "    for layer in layers:\n",
    "        print(f\"    Layer {layer}...\", end=\" \")\n",
    "        \n",
    "        source_pattern = mean_patterns[source_type][layer]\n",
    "        \n",
    "        baseline_source_scores = []\n",
    "        baseline_target_scores = []\n",
    "        intervention_source_scores = []\n",
    "        intervention_target_scores = []\n",
    "        \n",
    "        for prompt in target_prompts:\n",
    "            # Baseline (no intervention)\n",
    "            baseline_text, _ = run_baseline(prompt, model, tokenizer, config.n_generate_tokens)\n",
    "            baseline_source_scores.append(measure_shift_toward_target(baseline_text, source_expected))\n",
    "            baseline_target_scores.append(measure_shift_toward_target(baseline_text, target_expected))\n",
    "            \n",
    "            # With intervention\n",
    "            intervention_text, _ = run_intervention(\n",
    "                prompt, source_pattern, layer, model, tokenizer, patcher, config.n_generate_tokens\n",
    "            )\n",
    "            intervention_source_scores.append(measure_shift_toward_target(intervention_text, source_expected))\n",
    "            intervention_target_scores.append(measure_shift_toward_target(intervention_text, target_expected))\n",
    "        \n",
    "        # Compute effect sizes\n",
    "        source_effect = compute_effect_size(baseline_source_scores, intervention_source_scores)\n",
    "        target_effect = compute_effect_size(baseline_target_scores, intervention_target_scores)\n",
    "        \n",
    "        # T-test for significance\n",
    "        t_stat, p_value = stats.ttest_rel(intervention_source_scores, baseline_source_scores)\n",
    "        \n",
    "        results[\"layers\"][layer] = {\n",
    "            \"source_effect_d\": float(source_effect),\n",
    "            \"target_effect_d\": float(target_effect),\n",
    "            \"baseline_source_mean\": float(np.mean(baseline_source_scores)),\n",
    "            \"intervention_source_mean\": float(np.mean(intervention_source_scores)),\n",
    "            \"baseline_target_mean\": float(np.mean(baseline_target_scores)),\n",
    "            \"intervention_target_mean\": float(np.mean(intervention_target_scores)),\n",
    "            \"t_statistic\": float(t_stat),\n",
    "            \"p_value\": float(p_value)\n",
    "        }\n",
    "        \n",
    "        print(f\"d={source_effect:.2f}, p={p_value:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Experiment runner ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all intervention experiments\n",
    "print(\"\\nRunning intervention experiments...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "INTERVENTION_RESULTS = {}\n",
    "\n",
    "for source_type, target_type in INTERVENTION_PAIRS:\n",
    "    pair_name = f\"{source_type}_to_{target_type}\"\n",
    "    print(f\"\\n{pair_name}:\")\n",
    "    \n",
    "    results = run_intervention_experiment(\n",
    "        source_type, target_type,\n",
    "        MEAN_PATTERNS, ACTION_TYPES,\n",
    "        model, tokenizer, patcher,\n",
    "        config.layers_to_test, config.n_intervention_samples\n",
    "    )\n",
    "    \n",
    "    INTERVENTION_RESULTS[pair_name] = results\n",
    "\n",
    "print(\"\\nAll experiments complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Control: Random Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control: Random activation patching (should NOT produce systematic effects)\n",
    "print(\"\\nRunning random patching control...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "RANDOM_CONTROL_RESULTS = {}\n",
    "\n",
    "# Generate random activation pattern\n",
    "hidden_size = model.config.n_embd\n",
    "random_pattern = torch.randn(hidden_size).to(DEVICE)\n",
    "\n",
    "for target_type in [\"ANSWER_BOOLEAN\", \"COMPUTE_NUMBER\"]:\n",
    "    print(f\"\\nRandom -> {target_type}:\")\n",
    "    \n",
    "    results = {\"layers\": {}}\n",
    "    target_prompts = ACTION_TYPES[target_type][\"prompts\"][:config.n_intervention_samples]\n",
    "    target_expected = ACTION_TYPES[target_type][\"expected_tokens\"]\n",
    "    \n",
    "    for layer in config.layers_to_test:\n",
    "        print(f\"    Layer {layer}...\", end=\" \")\n",
    "        \n",
    "        baseline_scores = []\n",
    "        intervention_scores = []\n",
    "        \n",
    "        for prompt in target_prompts:\n",
    "            baseline_text, _ = run_baseline(prompt, model, tokenizer, config.n_generate_tokens)\n",
    "            baseline_scores.append(measure_shift_toward_target(baseline_text, target_expected))\n",
    "            \n",
    "            intervention_text, _ = run_intervention(\n",
    "                prompt, random_pattern, layer, model, tokenizer, patcher, config.n_generate_tokens\n",
    "            )\n",
    "            intervention_scores.append(measure_shift_toward_target(intervention_text, target_expected))\n",
    "        \n",
    "        effect = compute_effect_size(baseline_scores, intervention_scores)\n",
    "        t_stat, p_value = stats.ttest_rel(intervention_scores, baseline_scores)\n",
    "        \n",
    "        results[\"layers\"][layer] = {\n",
    "            \"effect_d\": float(effect),\n",
    "            \"t_statistic\": float(t_stat),\n",
    "            \"p_value\": float(p_value)\n",
    "        }\n",
    "        \n",
    "        print(f\"d={effect:.2f}, p={p_value:.4f}\")\n",
    "    \n",
    "    RANDOM_CONTROL_RESULTS[f\"random_to_{target_type}\"] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot effect sizes across layers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot intervention effects\n",
    "for idx, (pair_name, results) in enumerate(INTERVENTION_RESULTS.items()):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    layers = list(results[\"layers\"].keys())\n",
    "    effects = [results[\"layers\"][l][\"source_effect_d\"] for l in layers]\n",
    "    p_values = [results[\"layers\"][l][\"p_value\"] for l in layers]\n",
    "    \n",
    "    # Color by significance\n",
    "    colors = ['green' if p < 0.05 else 'gray' for p in p_values]\n",
    "    \n",
    "    ax.bar(layers, effects, color=colors, alpha=0.7)\n",
    "    ax.axhline(y=0.3, color='orange', linestyle='--', label='Medium effect (d=0.3)', alpha=0.7)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.set_ylabel(\"Cohen's d\")\n",
    "    ax.set_title(f\"{pair_name}\\n(Green = p < 0.05)\")\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(\"035H: Causal Intervention Effects by Layer\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"035H_intervention_effects.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 035H_intervention_effects.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 035H: CAUSAL INTERVENTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nExperiment Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Pattern samples: {config.n_pattern_samples}\")\n",
    "print(f\"  Intervention samples: {config.n_intervention_samples}\")\n",
    "print(f\"  Layers tested: {config.layers_to_test}\")\n",
    "\n",
    "print(f\"\\nIntervention Results:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "n_significant = 0\n",
    "n_total = 0\n",
    "best_effects = []\n",
    "\n",
    "for pair_name, results in INTERVENTION_RESULTS.items():\n",
    "    print(f\"\\n{pair_name}:\")\n",
    "    \n",
    "    # Find best layer\n",
    "    best_layer = max(results[\"layers\"].keys(), \n",
    "                     key=lambda l: results[\"layers\"][l][\"source_effect_d\"])\n",
    "    best = results[\"layers\"][best_layer]\n",
    "    \n",
    "    print(f\"  Best layer: {best_layer}\")\n",
    "    print(f\"  Effect size (d): {best['source_effect_d']:.3f}\")\n",
    "    print(f\"  p-value: {best['p_value']:.4f}\")\n",
    "    \n",
    "    best_effects.append(best['source_effect_d'])\n",
    "    \n",
    "    if best['p_value'] < 0.05 and best['source_effect_d'] > 0.3:\n",
    "        n_significant += 1\n",
    "        print(f\"  STATUS: SIGNIFICANT CAUSAL EFFECT\")\n",
    "    else:\n",
    "        print(f\"  STATUS: No significant effect\")\n",
    "    n_total += 1\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 80)\n",
    "print(f\"\\nRandom Control Results:\")\n",
    "for pair_name, results in RANDOM_CONTROL_RESULTS.items():\n",
    "    effects = [results[\"layers\"][l][\"effect_d\"] for l in results[\"layers\"]]\n",
    "    print(f\"  {pair_name}: mean d = {np.mean(effects):.3f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"CONCLUSIONS:\")\n",
    "print(f\"  Pairs with significant causal effect: {n_significant}/{n_total}\")\n",
    "print(f\"  Mean best effect size: {np.mean(best_effects):.3f}\")\n",
    "\n",
    "success = n_significant >= n_total // 2 and np.mean(best_effects) > 0.3\n",
    "\n",
    "if success:\n",
    "    print(f\"\\n  CONCLUSION: Evidence SUPPORTS causal role of AQ patterns.\")\n",
    "    print(f\"  Patching AQ patterns causally shifts model output.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: Evidence does NOT clearly support causal role.\")\n",
    "    print(f\"  AQ patterns may be correlational rather than causal.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "results_output = {\n",
    "    \"config\": {\n",
    "        \"model\": config.model_name,\n",
    "        \"n_pattern_samples\": config.n_pattern_samples,\n",
    "        \"n_intervention_samples\": config.n_intervention_samples,\n",
    "        \"layers\": config.layers_to_test\n",
    "    },\n",
    "    \"intervention_results\": make_serializable(INTERVENTION_RESULTS),\n",
    "    \"random_control_results\": make_serializable(RANDOM_CONTROL_RESULTS),\n",
    "    \"summary\": {\n",
    "        \"n_significant\": n_significant,\n",
    "        \"n_total\": n_total,\n",
    "        \"mean_best_effect\": float(np.mean(best_effects)),\n",
    "        \"conclusion\": \"SUPPORTS\" if success else \"DOES NOT SUPPORT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"035H_results.json\", \"w\") as f:\n",
    "    json.dump(results_output, f, indent=2)\n",
    "\n",
    "print(\"Results saved to 035H_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
