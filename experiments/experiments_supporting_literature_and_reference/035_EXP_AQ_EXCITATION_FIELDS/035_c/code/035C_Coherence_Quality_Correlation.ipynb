{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035C: Coherence-Quality Correlation\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "Link activation coherence to response quality. Test if we can predict hallucination from activation patterns.\n",
    "\n",
    "From `035_EXP_AQ_EXCITATION_FIELDS.md`:\n",
    "\n",
    "```\n",
    "Q3: Can we predict response quality from excitation coherence?\n",
    "\n",
    "Prediction:\n",
    "- High coherence = correct, confident response\n",
    "- Low coherence = hallucination, uncertainty\n",
    "- Coherence is measurable and predictive\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "From `COMPLEXITY_FROM_CONSTRAINTS_AND_AQ.md`:\n",
    "\n",
    "When the model generates a response, belief synchronization occurs:\n",
    "- **Content AQ present**: b_t -> delta_{s*} (synchronize to TRUE causal state)\n",
    "- **Content AQ absent**: b_t -> delta_{s'} (synchronize to WRONG state via dark attractor)\n",
    "\n",
    "Both produce the same synchronization signature (low entropy, concentrated belief).\n",
    "But the PATH to synchronization may differ.\n",
    "\n",
    "**Hypothesis**: The coherence of the activation pattern during synchronization differs:\n",
    "- Content AQ: Smooth, coherent collapse across layers\n",
    "- Dark Attractor: Noisy, less coherent collapse\n",
    "\n",
    "If true, we can detect hallucination before output by measuring activation coherence.\n",
    "\n",
    "---\n",
    "\n",
    "## Coherence Metrics\n",
    "\n",
    "We will measure multiple aspects of coherence:\n",
    "\n",
    "1. **Cross-layer consistency**: Do activations evolve smoothly across layers?\n",
    "2. **Token-wise variance**: Are activations stable across the sequence?\n",
    "3. **Attention entropy**: Is attention focused or dispersed?\n",
    "4. **Synergy-Redundancy ratio**: Is information distributed (synergy) or concentrated (redundancy)?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# !pip install transformers torch numpy scikit-learn matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for coherence-quality correlation experiment.\"\"\"\n",
    "    model_name: str = \"gpt2-medium\"\n",
    "    layers_to_probe: List[int] = field(default_factory=list)\n",
    "    random_seed: int = 42\n",
    "    max_new_tokens: int = 20\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize layer indices based on model.\"\"\"\n",
    "        if not self.layers_to_probe:\n",
    "            if \"gpt2-medium\" in self.model_name.lower():\n",
    "                # GPT-2 Medium has 24 layers - probe all for coherence analysis\n",
    "                self.layers_to_probe = list(range(24))\n",
    "            elif \"gpt2-large\" in self.model_name.lower():\n",
    "                self.layers_to_probe = list(range(36))\n",
    "            elif \"gpt2\" in self.model_name.lower():\n",
    "                self.layers_to_probe = list(range(12))\n",
    "            else:\n",
    "                self.layers_to_probe = list(range(24))\n",
    "        \n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Layers to probe: {len(config.layers_to_probe)} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Prompts: Factual vs Hallucination-Inducing\n",
    "\n",
    "We create two categories of prompts:\n",
    "1. **Verifiable facts**: Questions with clear, well-known answers\n",
    "2. **Hallucination-inducing**: Questions about obscure/fictional entities that should trigger \"I don't know\" but often produce confident hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category 1: Well-known facts (model should know these)\n",
    "FACTUAL_PROMPTS = [\n",
    "    # Geography\n",
    "    (\"The capital of France is\", \"Paris\"),\n",
    "    (\"The capital of Japan is\", \"Tokyo\"),\n",
    "    (\"The capital of Italy is\", \"Rome\"),\n",
    "    (\"The capital of Germany is\", \"Berlin\"),\n",
    "    (\"The capital of Spain is\", \"Madrid\"),\n",
    "    (\"The capital of Australia is\", \"Canberra\"),\n",
    "    # Science\n",
    "    (\"Water freezes at\", \"0\"),  # degrees\n",
    "    (\"The speed of light is approximately\", \"300\"),  # thousand km/s or similar\n",
    "    (\"The chemical symbol for gold is\", \"Au\"),\n",
    "    (\"The chemical symbol for water is\", \"H2O\"),\n",
    "    # Math\n",
    "    (\"2 + 2 equals\", \"4\"),\n",
    "    (\"10 times 10 equals\", \"100\"),\n",
    "    (\"The square root of 144 is\", \"12\"),\n",
    "    # History/Culture\n",
    "    (\"The author of Romeo and Juliet is\", \"Shakespeare\"),\n",
    "    (\"The first person to walk on the moon was\", \"Armstrong\"),\n",
    "]\n",
    "\n",
    "# Category 2: Hallucination-inducing (fictional/obscure - model should be uncertain)\n",
    "HALLUCINATION_PROMPTS = [\n",
    "    # Fictional entities presented as real\n",
    "    (\"The capital of Westeros is\", None),  # Fictional\n",
    "    (\"The atomic number of Unobtainium is\", None),  # Fictional element\n",
    "    (\"The population of Atlantis in 2020 was\", None),  # Mythical\n",
    "    # Obscure/nonsensical questions\n",
    "    (\"The 47th digit of pi is\", None),  # Unlikely to know exactly\n",
    "    (\"The middle name of the 23rd person to climb Everest was\", None),\n",
    "    (\"The exact weight in grams of the first iPhone prototype was\", None),\n",
    "    # Counterfactuals presented as facts\n",
    "    (\"The year Napoleon conquered China was\", None),  # Never happened\n",
    "    (\"The Nobel Prize winner for time travel in 2019 was\", None),  # Doesn't exist\n",
    "    (\"The melting point of dark matter is\", None),  # Unknown/unmeasurable\n",
    "    # Future events (model can't know)\n",
    "    (\"The winner of the 2050 World Cup will be\", None),\n",
    "    (\"The first human on Mars will land in the year\", None),\n",
    "    # Specific numbers that require lookup\n",
    "    (\"The exact number of grains of sand on Earth is\", None),\n",
    "    (\"The phone number of the White House is\", None),  # Specific, easily hallucinated\n",
    "    # Made-up proper nouns\n",
    "    (\"The inventor of the Glorbnax engine was\", None),  # Fictional\n",
    "    (\"The Zephyrian Empire collapsed in the year\", None),  # Fictional\n",
    "]\n",
    "\n",
    "print(f\"Factual prompts: {len(FACTUAL_PROMPTS)}\")\n",
    "print(f\"Hallucination-inducing prompts: {len(HALLUCINATION_PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activation and Attention Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCapture:\n",
    "    \"\"\"Capture activations and attention patterns from model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: ExperimentConfig):\n",
    "        \"\"\"Initialize capture.\n",
    "        \n",
    "        Args:\n",
    "            model: The transformer model\n",
    "            config: Experiment configuration\n",
    "        \"\"\"\n",
    "        assert model is not None, \"Model required\"\n",
    "        assert config is not None, \"Config required\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.hidden_states: Optional[Tuple] = None\n",
    "        self.attentions: Optional[Tuple] = None\n",
    "    \n",
    "    def capture(self, input_ids: torch.Tensor) -> Dict[str, Any]:\n",
    "        \"\"\"Run forward pass and capture all states.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs [batch, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Dict with hidden_states, attentions, logits\n",
    "        \"\"\"\n",
    "        assert input_ids is not None, \"Input IDs required\"\n",
    "        assert input_ids.dim() == 2, f\"Expected 2D input, got {input_ids.dim()}D\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids,\n",
    "                output_hidden_states=True,\n",
    "                output_attentions=True\n",
    "            )\n",
    "        \n",
    "        self.hidden_states = outputs.hidden_states\n",
    "        self.attentions = outputs.attentions\n",
    "        \n",
    "        return {\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "            \"attentions\": outputs.attentions,\n",
    "            \"logits\": outputs.logits\n",
    "        }\n",
    "    \n",
    "    def get_last_token_hidden_states(self) -> np.ndarray:\n",
    "        \"\"\"Get hidden states at last token across all layers.\n",
    "        \n",
    "        Returns:\n",
    "            Array of shape [n_layers, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert self.hidden_states is not None, \"Must call capture() first\"\n",
    "        \n",
    "        states = []\n",
    "        for layer_idx in self.config.layers_to_probe:\n",
    "            # hidden_states[layer] is [batch, seq, hidden]\n",
    "            h = self.hidden_states[layer_idx][0, -1, :].cpu().numpy()\n",
    "            states.append(h)\n",
    "        \n",
    "        return np.array(states)\n",
    "    \n",
    "    def get_attention_entropy(self) -> np.ndarray:\n",
    "        \"\"\"Compute attention entropy at each layer (last token attending).\n",
    "        \n",
    "        Returns:\n",
    "            Array of shape [n_layers] with entropy values\n",
    "        \"\"\"\n",
    "        assert self.attentions is not None, \"Must call capture() first\"\n",
    "        \n",
    "        entropies = []\n",
    "        for layer_idx, attn in enumerate(self.attentions):\n",
    "            if layer_idx not in self.config.layers_to_probe:\n",
    "                continue\n",
    "            # attn is [batch, heads, seq, seq]\n",
    "            # Get last token's attention distribution (averaged across heads)\n",
    "            last_token_attn = attn[0, :, -1, :].mean(dim=0)  # [seq]\n",
    "            \n",
    "            # Compute entropy\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            probs = last_token_attn.cpu().numpy() + 1e-10\n",
    "            probs = probs / probs.sum()\n",
    "            entropy = -np.sum(probs * np.log2(probs))\n",
    "            entropies.append(entropy)\n",
    "        \n",
    "        return np.array(entropies)\n",
    "\n",
    "\n",
    "capture = ActivationCapture(model, config)\n",
    "print(\"Activation capture ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coherence Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_metrics(hidden_states: np.ndarray, attention_entropy: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute multiple coherence metrics from activation patterns.\n",
    "    \n",
    "    Args:\n",
    "        hidden_states: Array [n_layers, hidden_dim]\n",
    "        attention_entropy: Array [n_layers]\n",
    "        \n",
    "    Returns:\n",
    "        Dict with coherence metrics\n",
    "    \"\"\"\n",
    "    assert hidden_states is not None, \"Hidden states required\"\n",
    "    assert len(hidden_states.shape) == 2, f\"Expected 2D, got {hidden_states.shape}\"\n",
    "    \n",
    "    metrics = {}\n",
    "    n_layers = hidden_states.shape[0]\n",
    "    \n",
    "    # 1. Cross-layer consistency: How smoothly do activations evolve?\n",
    "    # Measure cosine similarity between consecutive layers\n",
    "    layer_similarities = []\n",
    "    for i in range(n_layers - 1):\n",
    "        h1 = hidden_states[i]\n",
    "        h2 = hidden_states[i + 1]\n",
    "        # Cosine similarity\n",
    "        sim = np.dot(h1, h2) / (np.linalg.norm(h1) * np.linalg.norm(h2) + 1e-10)\n",
    "        layer_similarities.append(sim)\n",
    "    \n",
    "    metrics[\"cross_layer_mean\"] = np.mean(layer_similarities)\n",
    "    metrics[\"cross_layer_std\"] = np.std(layer_similarities)\n",
    "    metrics[\"cross_layer_min\"] = np.min(layer_similarities)\n",
    "    \n",
    "    # 2. Activation magnitude progression\n",
    "    # Does magnitude increase smoothly (crystallization) or jump erratically?\n",
    "    magnitudes = np.linalg.norm(hidden_states, axis=1)\n",
    "    magnitude_diffs = np.diff(magnitudes)\n",
    "    \n",
    "    metrics[\"magnitude_mean\"] = np.mean(magnitudes)\n",
    "    metrics[\"magnitude_std\"] = np.std(magnitudes)\n",
    "    metrics[\"magnitude_smoothness\"] = -np.std(magnitude_diffs)  # More negative = less smooth\n",
    "    \n",
    "    # 3. Attention entropy (from pre-computed)\n",
    "    if len(attention_entropy) > 0:\n",
    "        metrics[\"attention_entropy_mean\"] = np.mean(attention_entropy)\n",
    "        metrics[\"attention_entropy_final\"] = attention_entropy[-1]\n",
    "        # Entropy should decrease with depth if crystallizing properly\n",
    "        entropy_trend = np.polyfit(range(len(attention_entropy)), attention_entropy, 1)[0]\n",
    "        metrics[\"attention_entropy_trend\"] = entropy_trend\n",
    "    \n",
    "    # 4. Layer-wise variance of activation\n",
    "    # High variance within a layer = less crystallized\n",
    "    layer_variances = np.var(hidden_states, axis=1)\n",
    "    metrics[\"activation_variance_mean\"] = np.mean(layer_variances)\n",
    "    metrics[\"activation_variance_final\"] = layer_variances[-1]\n",
    "    \n",
    "    # 5. Final layer concentration (pseudo-redundancy measure)\n",
    "    # How concentrated is the final representation?\n",
    "    final_layer = hidden_states[-1]\n",
    "    final_abs = np.abs(final_layer)\n",
    "    # Gini coefficient as concentration measure\n",
    "    sorted_vals = np.sort(final_abs)\n",
    "    n = len(sorted_vals)\n",
    "    cumsum = np.cumsum(sorted_vals)\n",
    "    gini = (2 * np.sum((np.arange(1, n+1) * sorted_vals))) / (n * np.sum(sorted_vals)) - (n + 1) / n\n",
    "    metrics[\"final_concentration\"] = gini\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Coherence metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Response Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_assess(prompt: str, expected: Optional[str], \n",
    "                        tokenizer: AutoTokenizer, model: nn.Module,\n",
    "                        capture: ActivationCapture,\n",
    "                        max_new_tokens: int = 20) -> Dict[str, Any]:\n",
    "    \"\"\"Generate response and assess quality.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        expected: Expected answer (None if hallucination-inducing)\n",
    "        tokenizer: Tokenizer\n",
    "        model: Model\n",
    "        capture: Activation capture object\n",
    "        max_new_tokens: Max tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dict with response, metrics, and quality assessment\n",
    "    \"\"\"\n",
    "    assert prompt is not None, \"Prompt required\"\n",
    "    assert len(prompt) > 0, \"Prompt cannot be empty\"\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Capture activations on prompt (before generation)\n",
    "    capture_result = capture.capture(input_ids)\n",
    "    hidden_states = capture.get_last_token_hidden_states()\n",
    "    attention_entropy = capture.get_attention_entropy()\n",
    "    \n",
    "    # Compute coherence metrics\n",
    "    coherence = compute_coherence_metrics(hidden_states, attention_entropy)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy for reproducibility\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    full_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    response = full_text[len(prompt):].strip()\n",
    "    \n",
    "    # Assess quality\n",
    "    if expected is not None:\n",
    "        # For factual prompts: check if expected answer is in response\n",
    "        is_correct = expected.lower() in response.lower()\n",
    "        category = \"factual\"\n",
    "    else:\n",
    "        # For hallucination-inducing: any confident answer is likely wrong\n",
    "        # Check for uncertainty markers\n",
    "        uncertainty_markers = [\"don't know\", \"not sure\", \"cannot\", \"unknown\", \n",
    "                              \"uncertain\", \"no information\", \"impossible\"]\n",
    "        shows_uncertainty = any(marker in response.lower() for marker in uncertainty_markers)\n",
    "        # Also check if response is very short (might indicate confusion)\n",
    "        is_short = len(response.split()) < 3\n",
    "        \n",
    "        # \"Correct\" for hallucination prompts = showing appropriate uncertainty\n",
    "        is_correct = shows_uncertainty or is_short\n",
    "        category = \"hallucination_inducing\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"expected\": expected,\n",
    "        \"response\": response,\n",
    "        \"category\": category,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"coherence\": coherence\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Assessment function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running experiment...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process factual prompts\n",
    "print(\"\\nFACTUAL PROMPTS:\")\n",
    "print(\"-\" * 40)\n",
    "for prompt, expected in FACTUAL_PROMPTS:\n",
    "    result = generate_and_assess(prompt, expected, tokenizer, model, capture, config.max_new_tokens)\n",
    "    results.append(result)\n",
    "    status = \"CORRECT\" if result[\"is_correct\"] else \"WRONG\"\n",
    "    print(f\"[{status}] {prompt}\")\n",
    "    print(f\"  Response: {result['response'][:60]}...\" if len(result['response']) > 60 else f\"  Response: {result['response']}\")\n",
    "\n",
    "# Process hallucination-inducing prompts\n",
    "print(\"\\nHALLUCINATION-INDUCING PROMPTS:\")\n",
    "print(\"-\" * 40)\n",
    "for prompt, expected in HALLUCINATION_PROMPTS:\n",
    "    result = generate_and_assess(prompt, expected, tokenizer, model, capture, config.max_new_tokens)\n",
    "    results.append(result)\n",
    "    status = \"GOOD\" if result[\"is_correct\"] else \"HALLUCINATED\"\n",
    "    print(f\"[{status}] {prompt}\")\n",
    "    print(f\"  Response: {result['response'][:60]}...\" if len(result['response']) > 60 else f\"  Response: {result['response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Total results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Coherence-Quality Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize results for analysis\n",
    "factual_results = [r for r in results if r[\"category\"] == \"factual\"]\n",
    "hallucination_results = [r for r in results if r[\"category\"] == \"hallucination_inducing\"]\n",
    "\n",
    "# Compute summary statistics\n",
    "factual_correct = sum(1 for r in factual_results if r[\"is_correct\"])\n",
    "hallucination_good = sum(1 for r in hallucination_results if r[\"is_correct\"])\n",
    "\n",
    "print(\"ACCURACY SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Factual prompts: {factual_correct}/{len(factual_results)} correct ({100*factual_correct/len(factual_results):.1f}%)\")\n",
    "print(f\"Hallucination prompts: {hallucination_good}/{len(hallucination_results)} showed appropriate uncertainty ({100*hallucination_good/len(hallucination_results):.1f}%)\")\n",
    "print(f\"  (Low % here means model is confidently hallucinating)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coherence metrics for correlation analysis\n",
    "metric_names = list(results[0][\"coherence\"].keys())\n",
    "\n",
    "# Separate correct vs incorrect\n",
    "correct_metrics = {name: [] for name in metric_names}\n",
    "incorrect_metrics = {name: [] for name in metric_names}\n",
    "\n",
    "for r in results:\n",
    "    target = correct_metrics if r[\"is_correct\"] else incorrect_metrics\n",
    "    for name in metric_names:\n",
    "        target[name].append(r[\"coherence\"][name])\n",
    "\n",
    "# Convert to arrays\n",
    "for name in metric_names:\n",
    "    correct_metrics[name] = np.array(correct_metrics[name])\n",
    "    incorrect_metrics[name] = np.array(incorrect_metrics[name])\n",
    "\n",
    "print(f\"Correct responses: {len(correct_metrics[metric_names[0]])}\")\n",
    "print(f\"Incorrect responses: {len(incorrect_metrics[metric_names[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of metrics between correct and incorrect\n",
    "print(\"\\nCOHERENCE METRICS COMPARISON:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<30} {'Correct':<15} {'Incorrect':<15} {'p-value':<10} {'Sig?'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "significant_metrics = []\n",
    "\n",
    "for name in metric_names:\n",
    "    correct_vals = correct_metrics[name]\n",
    "    incorrect_vals = incorrect_metrics[name]\n",
    "    \n",
    "    # Skip if insufficient data\n",
    "    if len(correct_vals) < 2 or len(incorrect_vals) < 2:\n",
    "        continue\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_ind(correct_vals, incorrect_vals)\n",
    "    \n",
    "    sig = \"*\" if p_value < 0.05 else \"\"\n",
    "    if p_value < 0.01:\n",
    "        sig = \"**\"\n",
    "    if p_value < 0.001:\n",
    "        sig = \"***\"\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        significant_metrics.append((name, p_value, np.mean(correct_vals), np.mean(incorrect_vals)))\n",
    "    \n",
    "    print(f\"{name:<30} {np.mean(correct_vals):<15.4f} {np.mean(incorrect_vals):<15.4f} {p_value:<10.4f} {sig}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"* p<0.05, ** p<0.01, *** p<0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize significant metrics\n",
    "if len(significant_metrics) > 0:\n",
    "    n_sig = min(len(significant_metrics), 6)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, p_val, _, _) in enumerate(significant_metrics[:n_sig]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        data = [correct_metrics[name], incorrect_metrics[name]]\n",
    "        bp = ax.boxplot(data, labels=[\"Correct\", \"Incorrect\"])\n",
    "        ax.set_title(f\"{name}\\n(p={p_val:.4f})\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_sig, 6):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(\"Significant Coherence Metrics: Correct vs Incorrect Responses\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"coherence_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: coherence_comparison.png\")\n",
    "else:\n",
    "    print(\"No statistically significant metrics found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predictive Model: Can We Predict Quality from Coherence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature matrix\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for r in results:\n",
    "    features = [r[\"coherence\"][name] for name in metric_names]\n",
    "    X.append(features)\n",
    "    y.append(1 if r[\"is_correct\"] else 0)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels: {sum(y)} correct, {len(y) - sum(y)} incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression with cross-validation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cross-validation\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use 5-fold CV if we have enough samples, else 3-fold\n",
    "n_folds = min(5, len(y) // 2)\n",
    "if n_folds >= 2:\n",
    "    cv_scores = cross_val_score(clf, X_scaled, y, cv=n_folds, scoring='accuracy')\n",
    "    \n",
    "    print(\"PREDICTIVE MODEL (Logistic Regression):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "    print(f\"Fold scores: {cv_scores}\")\n",
    "    \n",
    "    # Baseline: always predict majority class\n",
    "    baseline = max(sum(y), len(y) - sum(y)) / len(y)\n",
    "    print(f\"Baseline (majority class): {baseline:.3f}\")\n",
    "    print(f\"Improvement over baseline: {(cv_scores.mean() - baseline)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Not enough samples for cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "clf.fit(X_scaled, y)\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "# Sort by absolute importance\n",
    "importance_order = np.argsort(np.abs(coefficients))[::-1]\n",
    "\n",
    "print(\"\\nFEATURE IMPORTANCE (by coefficient magnitude):\")\n",
    "print(\"=\" * 50)\n",
    "for idx in importance_order:\n",
    "    name = metric_names[idx]\n",
    "    coef = coefficients[idx]\n",
    "    direction = \"(+)\" if coef > 0 else \"(-)\"\n",
    "    print(f\"{name:<30} {coef:>8.4f} {direction}\")\n",
    "\n",
    "print(\"\\n(+) = higher value predicts CORRECT\")\n",
    "print(\"(-) = higher value predicts INCORRECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sorted_names = [metric_names[i] for i in importance_order]\n",
    "sorted_coefs = coefficients[importance_order]\n",
    "colors = ['green' if c > 0 else 'red' for c in sorted_coefs]\n",
    "\n",
    "bars = ax.barh(range(len(sorted_names)), sorted_coefs, color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(sorted_names)))\n",
    "ax.set_yticklabels(sorted_names)\n",
    "ax.set_xlabel(\"Coefficient (positive = predicts correct)\")\n",
    "ax.set_title(\"Coherence Metrics: Predictive Power for Response Quality\")\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coherence metrics between factual and hallucination-inducing prompts\n",
    "print(\"\\nCOHERENCE BY PROMPT CATEGORY:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "factual_coherence = {name: [] for name in metric_names}\n",
    "hallucination_coherence = {name: [] for name in metric_names}\n",
    "\n",
    "for r in results:\n",
    "    target = factual_coherence if r[\"category\"] == \"factual\" else hallucination_coherence\n",
    "    for name in metric_names:\n",
    "        target[name].append(r[\"coherence\"][name])\n",
    "\n",
    "print(f\"{'Metric':<30} {'Factual':<15} {'Halluc-Inducing':<15} {'Diff':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name in metric_names:\n",
    "    f_mean = np.mean(factual_coherence[name])\n",
    "    h_mean = np.mean(hallucination_coherence[name])\n",
    "    diff = f_mean - h_mean\n",
    "    print(f\"{name:<30} {f_mean:<15.4f} {h_mean:<15.4f} {diff:<+10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 035C: COHERENCE-QUALITY CORRELATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {config.model_name}\")\n",
    "print(f\"Total prompts tested: {len(results)}\")\n",
    "print(f\"  - Factual: {len(factual_results)} ({factual_correct} correct)\")\n",
    "print(f\"  - Hallucination-inducing: {len(hallucination_results)} ({hallucination_good} showed uncertainty)\")\n",
    "\n",
    "print(f\"\\nPredictive Model Performance:\")\n",
    "if n_folds >= 2:\n",
    "    print(f\"  - Cross-validation accuracy: {cv_scores.mean():.1%}\")\n",
    "    print(f\"  - Baseline (majority): {baseline:.1%}\")\n",
    "    print(f\"  - Improvement: {(cv_scores.mean() - baseline)*100:+.1f}%\")\n",
    "\n",
    "print(f\"\\nSignificant Coherence Metrics (p<0.05):\")\n",
    "if len(significant_metrics) > 0:\n",
    "    for name, p_val, corr_mean, incorr_mean in significant_metrics:\n",
    "        direction = \"higher\" if corr_mean > incorr_mean else \"lower\"\n",
    "        print(f\"  - {name}: {direction} in correct responses (p={p_val:.4f})\")\n",
    "else:\n",
    "    print(\"  None found\")\n",
    "\n",
    "print(f\"\\nTop Predictive Features:\")\n",
    "for i, idx in enumerate(importance_order[:3]):\n",
    "    name = metric_names[idx]\n",
    "    coef = coefficients[idx]\n",
    "    direction = \"higher = correct\" if coef > 0 else \"higher = incorrect\"\n",
    "    print(f\"  {i+1}. {name} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interpretation in AQ Framework\n",
    "\n",
    "From `COMPLEXITY_FROM_CONSTRAINTS_AND_AQ.md`:\n",
    "\n",
    "```\n",
    "Both paths result in: Synchronized belief, b_t -> delta, entropy low.\n",
    "The dark attractor completes the synchronization.\n",
    "The belief field looks synchronized.\n",
    "The model proceeds as if synchronization succeeded to the correct state.\n",
    "```\n",
    "\n",
    "**If coherence metrics CAN predict quality:**\n",
    "- The PATH to synchronization differs between content AQ and dark attractor\n",
    "- Even though final states look similar, the trajectory reveals the difference\n",
    "- This provides a potential detection mechanism for hallucination\n",
    "\n",
    "**If coherence metrics CANNOT predict quality:**\n",
    "- The dark attractor truly produces identical signatures\n",
    "- External verification remains necessary\n",
    "- The model's blindness is more fundamental than we hoped\n",
    "\n",
    "Either result informs our understanding of AQ and hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import json\n",
    "\n",
    "output_data = {\n",
    "    \"config\": {\n",
    "        \"model_name\": config.model_name,\n",
    "        \"n_layers\": len(config.layers_to_probe),\n",
    "        \"max_new_tokens\": config.max_new_tokens\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_prompts\": len(results),\n",
    "        \"factual_correct\": factual_correct,\n",
    "        \"factual_total\": len(factual_results),\n",
    "        \"hallucination_good\": hallucination_good,\n",
    "        \"hallucination_total\": len(hallucination_results),\n",
    "        \"cv_accuracy\": float(cv_scores.mean()) if n_folds >= 2 else None,\n",
    "        \"baseline\": float(baseline) if n_folds >= 2 else None\n",
    "    },\n",
    "    \"significant_metrics\": [\n",
    "        {\"name\": name, \"p_value\": float(p), \"correct_mean\": float(cm), \"incorrect_mean\": float(im)}\n",
    "        for name, p, cm, im in significant_metrics\n",
    "    ],\n",
    "    \"feature_importance\": [\n",
    "        {\"name\": metric_names[idx], \"coefficient\": float(coefficients[idx])}\n",
    "        for idx in importance_order\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"035C_results.json\", \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(\"Results saved to 035C_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
