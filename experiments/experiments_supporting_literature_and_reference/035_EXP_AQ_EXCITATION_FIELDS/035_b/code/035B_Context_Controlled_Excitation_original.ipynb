{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035B: Context-Controlled Excitation\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Experiment Tests\n",
    "\n",
    "In 035A, we showed that different discrimination types produce different activation patterns.\n",
    "\n",
    "Now we test something stronger: **the same tokens produce different activations depending on context**.\n",
    "\n",
    "### The Setup\n",
    "\n",
    "We use a fixed query phrase like \"The answer is\" and vary the context:\n",
    "\n",
    "```\n",
    "MATH CONTEXT:    \"2 + 2 = ? The answer is\"\n",
    "TRIVIA CONTEXT:  \"What is the capital of France? The answer is\"\n",
    "SENTIMENT:       \"How do you feel today? The answer is\"\n",
    "```\n",
    "\n",
    "The tokens \"The answer is\" are identical, but the activations should differ based on what action is required.\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "If AQ theory is correct:\n",
    "- Same surface tokens + different context = different activation patterns\n",
    "- Context determines which AQ excite, not the tokens themselves\n",
    "- Activations for \"The answer is\" should cluster by CONTEXT TYPE, not by token identity\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This directly tests the claim that context SELECTS which AQ excite from the weight field.\n",
    "\n",
    "If activations were purely token-based, \"The answer is\" would always produce similar patterns.\n",
    "If activations are context-controlled (AQ theory), the same tokens produce different patterns based on what came before.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "!pip install transformers torch numpy scikit-learn matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for Context-Controlled Excitation experiment.\"\"\"\n",
    "    model_name: str = \"gpt2\"\n",
    "    layers_to_probe: List[int] = field(default_factory=list)\n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        if not self.layers_to_probe:\n",
    "            if \"gpt2\" in self.model_name.lower():\n",
    "                self.layers_to_probe = [0, 3, 6, 9, 11]\n",
    "            elif \"pythia\" in self.model_name.lower():\n",
    "                self.layers_to_probe = [0, 2, 4, 5]\n",
    "            else:\n",
    "                self.layers_to_probe = [0, 3, 6, 9, 11]\n",
    "        \n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Layers to probe: {config.layers_to_probe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context-Controlled Probes\n",
    "\n",
    "Each probe has:\n",
    "- A **context** that sets up a specific discrimination type\n",
    "- A **fixed query** (\"The answer is\") that we measure activations for\n",
    "\n",
    "The key: the query tokens are IDENTICAL across all probes, only context differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fixed query phrase we'll measure activations for\n",
    "QUERY_PHRASE = \"The answer is\"\n",
    "\n",
    "# Context + Query probes organized by context type\n",
    "CONTEXT_PROBES: Dict[str, List[str]] = {\n",
    "    'math': [\n",
    "        \"What is 2 + 2? The answer is\",\n",
    "        \"Calculate 5 times 3. The answer is\",\n",
    "        \"What is 10 divided by 2? The answer is\",\n",
    "        \"Solve: 7 - 4 = ? The answer is\",\n",
    "        \"What is 8 + 1? The answer is\",\n",
    "        \"Compute 6 times 2. The answer is\",\n",
    "    ],\n",
    "    'geography': [\n",
    "        \"What is the capital of France? The answer is\",\n",
    "        \"Name the largest country by area. The answer is\",\n",
    "        \"What continent is Brazil in? The answer is\",\n",
    "        \"What is the capital of Japan? The answer is\",\n",
    "        \"Name the longest river in Africa. The answer is\",\n",
    "        \"What ocean is between Europe and America? The answer is\",\n",
    "    ],\n",
    "    'science': [\n",
    "        \"What is the chemical symbol for water? The answer is\",\n",
    "        \"At what temperature does water boil? The answer is\",\n",
    "        \"What planet is closest to the sun? The answer is\",\n",
    "        \"What gas do plants produce? The answer is\",\n",
    "        \"What is the speed of light? The answer is\",\n",
    "        \"How many bones in the human body? The answer is\",\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        \"How are you feeling today? The answer is\",\n",
    "        \"What is your mood right now? The answer is\",\n",
    "        \"Are you happy or sad? The answer is\",\n",
    "        \"How would you describe your emotions? The answer is\",\n",
    "        \"What is your emotional state? The answer is\",\n",
    "        \"Do you feel good or bad? The answer is\",\n",
    "    ],\n",
    "    'yesno': [\n",
    "        \"Is the sky blue? The answer is\",\n",
    "        \"Can fish swim? The answer is\",\n",
    "        \"Is fire cold? The answer is\",\n",
    "        \"Do birds fly? The answer is\",\n",
    "        \"Is water wet? The answer is\",\n",
    "        \"Can humans breathe underwater? The answer is\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Colors for visualization\n",
    "CONTEXT_COLORS: Dict[str, str] = {\n",
    "    'math': '#3498db',        # blue\n",
    "    'geography': '#9b59b6',   # purple\n",
    "    'science': '#f39c12',     # orange\n",
    "    'sentiment': '#2ecc71',   # green\n",
    "    'yesno': '#e74c3c',       # red\n",
    "}\n",
    "\n",
    "print(f\"Query phrase: '{QUERY_PHRASE}'\")\n",
    "print(f\"Number of context types: {len(CONTEXT_PROBES)}\")\n",
    "print(f\"Total probes: {sum(len(v) for v in CONTEXT_PROBES.values())}\")\n",
    "for ctx, prompts in CONTEXT_PROBES.items():\n",
    "    print(f\"  {ctx}: {len(prompts)} probes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Capture Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCapture:\n",
    "    \"\"\"Captures activations from specified layers using forward hooks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, layer_indices: List[int]) -> None:\n",
    "        assert len(layer_indices) > 0, \"Must specify at least one layer to probe\"\n",
    "        \n",
    "        self.activations: Dict[int, torch.Tensor] = {}\n",
    "        self.hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "        self.layer_indices = layer_indices\n",
    "        \n",
    "        # Get the transformer blocks\n",
    "        if hasattr(model, 'transformer'):\n",
    "            layers = model.transformer.h\n",
    "        elif hasattr(model, 'gpt_neox'):\n",
    "            layers = model.gpt_neox.layers\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model architecture: {type(model)}\")\n",
    "        \n",
    "        assert len(layers) > max(layer_indices), \\\n",
    "            f\"Model has {len(layers)} layers but requested layer {max(layer_indices)}\"\n",
    "        \n",
    "        for idx in layer_indices:\n",
    "            layer = layers[idx]\n",
    "            hook = layer.register_forward_hook(self._make_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "        \n",
    "        print(f\"Registered hooks on layers: {layer_indices}\")\n",
    "    \n",
    "    def _make_hook(self, layer_idx: int):\n",
    "        def hook(module: nn.Module, input: Tuple, output: Tuple) -> None:\n",
    "            if isinstance(output, tuple):\n",
    "                hidden_states = output[0]\n",
    "            else:\n",
    "                hidden_states = output\n",
    "            self.activations[layer_idx] = hidden_states.detach()\n",
    "        return hook\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.activations = {}\n",
    "    \n",
    "    def remove_hooks(self) -> None:\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"Removed all hooks\")\n",
    "    \n",
    "    def get_activation_at_positions(self, layer_idx: int, positions: List[int]) -> np.ndarray:\n",
    "        \"\"\"Get activations for specific token positions.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: Which layer\n",
    "            positions: List of token positions to extract\n",
    "            \n",
    "        Returns:\n",
    "            Array of shape [len(positions), hidden_dim]\n",
    "        \"\"\"\n",
    "        assert layer_idx in self.activations, f\"Layer {layer_idx} not captured\"\n",
    "        act = self.activations[layer_idx]\n",
    "        \n",
    "        # Extract specified positions\n",
    "        extracted = act[0, positions, :].cpu().numpy()\n",
    "        return extracted\n",
    "    \n",
    "    def get_mean_activation(self, layer_idx: int, positions: List[int]) -> np.ndarray:\n",
    "        \"\"\"Get mean activation across specified positions.\"\"\"\n",
    "        extracted = self.get_activation_at_positions(layer_idx, positions)\n",
    "        return extracted.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_category_distances(\n",
    "    activations: np.ndarray,\n",
    "    labels: List[str]\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"Compute within-category and between-category distances.\"\"\"\n",
    "    within_distances = []\n",
    "    between_distances = []\n",
    "    \n",
    "    distances = pairwise_distances(activations, metric='euclidean')\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] == labels[j]:\n",
    "                within_distances.append(distances[i, j])\n",
    "            else:\n",
    "                between_distances.append(distances[i, j])\n",
    "    \n",
    "    within_mean = np.mean(within_distances) if within_distances else 0\n",
    "    between_mean = np.mean(between_distances) if between_distances else 0\n",
    "    ratio = between_mean / within_mean if within_mean > 0 else float('inf')\n",
    "    \n",
    "    return within_mean, between_mean, ratio\n",
    "\n",
    "\n",
    "def compute_silhouette(activations: np.ndarray, labels: List[str]) -> float:\n",
    "    \"\"\"Compute silhouette score.\"\"\"\n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "    int_labels = [label_to_int[label] for label in labels]\n",
    "    \n",
    "    if len(set(int_labels)) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    return silhouette_score(activations, int_labels)\n",
    "\n",
    "\n",
    "def run_pca_analysis(\n",
    "    activations: np.ndarray,\n",
    "    n_components: int = 2,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[np.ndarray, PCA]:\n",
    "    \"\"\"Apply PCA.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    activations_scaled = scaler.fit_transform(activations)\n",
    "    \n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced = pca.fit_transform(activations_scaled)\n",
    "    \n",
    "    if verbose:\n",
    "        explained_var = sum(pca.explained_variance_ratio_) * 100\n",
    "        print(f\"PCA: {n_components} components explain {explained_var:.1f}% variance\")\n",
    "    \n",
    "    return reduced, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_scatter(\n",
    "    activations_2d: np.ndarray,\n",
    "    labels: List[str],\n",
    "    layer_idx: int,\n",
    "    title_suffix: str = \"\"\n",
    ") -> None:\n",
    "    \"\"\"Create 2D scatter plot.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for category in CONTEXT_COLORS:\n",
    "        mask = [l == category for l in labels]\n",
    "        if any(mask):\n",
    "            indices = [i for i, m in enumerate(mask) if m]\n",
    "            plt.scatter(\n",
    "                activations_2d[indices, 0],\n",
    "                activations_2d[indices, 1],\n",
    "                c=CONTEXT_COLORS[category],\n",
    "                label=category,\n",
    "                alpha=0.7,\n",
    "                s=100,\n",
    "                edgecolors='white',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('PC1', fontsize=12)\n",
    "    plt.ylabel('PC2', fontsize=12)\n",
    "    plt.title(f'Context-Controlled Activations - Layer {layer_idx}{title_suffix}', fontsize=14)\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_similarity_matrix(\n",
    "    activations: np.ndarray,\n",
    "    labels: List[str],\n",
    "    layer_idx: int\n",
    ") -> None:\n",
    "    \"\"\"Create similarity heatmap.\"\"\"\n",
    "    norms = np.linalg.norm(activations, axis=1, keepdims=True)\n",
    "    normalized = activations / (norms + 1e-8)\n",
    "    similarity = normalized @ normalized.T\n",
    "    \n",
    "    sorted_indices = sorted(range(len(labels)), key=lambda i: labels[i])\n",
    "    similarity_sorted = similarity[sorted_indices][:, sorted_indices]\n",
    "    labels_sorted = [labels[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        similarity_sorted,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Cosine Similarity'}\n",
    "    )\n",
    "    \n",
    "    # Add category boundaries\n",
    "    unique_labels = []\n",
    "    boundaries = [0]\n",
    "    for i, label in enumerate(labels_sorted):\n",
    "        if label not in unique_labels:\n",
    "            unique_labels.append(label)\n",
    "            if i > 0:\n",
    "                boundaries.append(i)\n",
    "    boundaries.append(len(labels_sorted))\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        plt.axhline(y=b, color='black', linewidth=2)\n",
    "        plt.axvline(x=b, color='black', linewidth=2)\n",
    "    \n",
    "    plt.title(f'Activation Similarity for \"{QUERY_PHRASE}\" - Layer {layer_idx}', fontsize=14)\n",
    "    plt.xlabel('Probe Index (sorted by context type)')\n",
    "    plt.ylabel('Probe Index (sorted by context type)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_layer_comparison(metrics_by_layer: Dict[int, Dict[str, float]]) -> None:\n",
    "    \"\"\"Plot metrics across layers.\"\"\"\n",
    "    layers = sorted(metrics_by_layer.keys())\n",
    "    \n",
    "    silhouettes = [metrics_by_layer[l]['silhouette'] for l in layers]\n",
    "    ratios = [metrics_by_layer[l]['distance_ratio'] for l in layers]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].plot(layers, silhouettes, 'o-', color='#3498db', linewidth=2, markersize=10)\n",
    "    axes[0].set_xlabel('Layer Index', fontsize=12)\n",
    "    axes[0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "    axes[0].set_title('Context Clustering Quality by Layer', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(-0.2, 1.0)\n",
    "    \n",
    "    axes[1].plot(layers, ratios, 'o-', color='#e74c3c', linewidth=2, markersize=10)\n",
    "    axes[1].set_xlabel('Layer Index', fontsize=12)\n",
    "    axes[1].set_ylabel('Between/Within Distance Ratio', fontsize=12)\n",
    "    axes[1].set_title('Context Separation by Layer', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {DEVICE}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find Query Token Positions\n",
    "\n",
    "We need to identify which token positions correspond to \"The answer is\" in each probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_query_positions(full_text: str, query: str, tokenizer) -> List[int]:\n",
    "    \"\"\"Find token positions of the query phrase within the full text.\n",
    "    \n",
    "    Returns positions of query tokens in the tokenized full text.\n",
    "    \"\"\"\n",
    "    # Tokenize full text\n",
    "    full_tokens = tokenizer.encode(full_text, add_special_tokens=False)\n",
    "    \n",
    "    # Tokenize query (with space prefix to match how it appears in context)\n",
    "    query_tokens = tokenizer.encode(\" \" + query, add_special_tokens=False)\n",
    "    \n",
    "    # Find where query tokens appear in full tokens\n",
    "    query_len = len(query_tokens)\n",
    "    for i in range(len(full_tokens) - query_len + 1):\n",
    "        if full_tokens[i:i+query_len] == query_tokens:\n",
    "            return list(range(i, i + query_len))\n",
    "    \n",
    "    # Fallback: try without space prefix\n",
    "    query_tokens = tokenizer.encode(query, add_special_tokens=False)\n",
    "    query_len = len(query_tokens)\n",
    "    for i in range(len(full_tokens) - query_len + 1):\n",
    "        if full_tokens[i:i+query_len] == query_tokens:\n",
    "            return list(range(i, i + query_len))\n",
    "    \n",
    "    # If still not found, return last N positions (where N = query token count)\n",
    "    print(f\"Warning: Could not find exact query match in '{full_text[:50]}...'\")\n",
    "    return list(range(len(full_tokens) - query_len, len(full_tokens)))\n",
    "\n",
    "\n",
    "# Test on first probe\n",
    "test_probe = list(CONTEXT_PROBES.values())[0][0]\n",
    "test_positions = find_query_positions(test_probe, QUERY_PHRASE, tokenizer)\n",
    "print(f\"Test probe: '{test_probe}'\")\n",
    "print(f\"Query positions: {test_positions}\")\n",
    "\n",
    "# Show what tokens those positions correspond to\n",
    "full_tokens = tokenizer.encode(test_probe, add_special_tokens=False)\n",
    "query_token_ids = [full_tokens[p] for p in test_positions]\n",
    "query_decoded = tokenizer.decode(query_token_ids)\n",
    "print(f\"Tokens at those positions: '{query_decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Probes and Capture Activations\n",
    "\n",
    "For each probe, we capture the activations ONLY at the positions corresponding to \"The answer is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up activation capture\n",
    "capture = ActivationCapture(model, config.layers_to_probe)\n",
    "\n",
    "# Storage for results\n",
    "all_activations: Dict[int, List[np.ndarray]] = {\n",
    "    layer: [] for layer in config.layers_to_probe\n",
    "}\n",
    "all_labels: List[str] = []\n",
    "all_texts: List[str] = []\n",
    "\n",
    "print(f\"Running context-controlled probes...\")\n",
    "print(f\"Extracting activations for tokens: '{QUERY_PHRASE}'\\n\")\n",
    "\n",
    "for context_type, prompts in CONTEXT_PROBES.items():\n",
    "    for prompt in prompts:\n",
    "        # Find positions of query phrase\n",
    "        query_positions = find_query_positions(prompt, QUERY_PHRASE, tokenizer)\n",
    "        \n",
    "        # Tokenize and run\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        capture.clear()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract activations at query positions (mean across positions)\n",
    "        for layer_idx in config.layers_to_probe:\n",
    "            act = capture.get_mean_activation(layer_idx, query_positions)\n",
    "            all_activations[layer_idx].append(act)\n",
    "        \n",
    "        all_labels.append(context_type)\n",
    "        all_texts.append(prompt)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    all_activations[layer_idx] = np.array(all_activations[layer_idx])\n",
    "\n",
    "print(f\"\\nCollected activations for {len(all_labels)} probes\")\n",
    "print(f\"Activation shape per layer: {all_activations[config.layers_to_probe[0]].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Layer-by-Layer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LAYER-BY-LAYER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMeasuring activations for IDENTICAL tokens '{QUERY_PHRASE}'\")\n",
    "print(\"If context controls AQ, same tokens should cluster by context type\\n\")\n",
    "\n",
    "metrics_by_layer: Dict[int, Dict[str, float]] = {}\n",
    "pca_results_by_layer: Dict[int, np.ndarray] = {}\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    print(f\"\\n--- Layer {layer_idx} ---\")\n",
    "    \n",
    "    activations = all_activations[layer_idx]\n",
    "    \n",
    "    within_dist, between_dist, ratio = compute_category_distances(activations, all_labels)\n",
    "    print(f\"Within-context distance:  {within_dist:.4f}\")\n",
    "    print(f\"Between-context distance: {between_dist:.4f}\")\n",
    "    print(f\"Distance ratio: {ratio:.4f}\")\n",
    "    \n",
    "    silhouette = compute_silhouette(activations, all_labels)\n",
    "    print(f\"Silhouette score: {silhouette:.4f}\")\n",
    "    \n",
    "    metrics_by_layer[layer_idx] = {\n",
    "        'within_distance': within_dist,\n",
    "        'between_distance': between_dist,\n",
    "        'distance_ratio': ratio,\n",
    "        'silhouette': silhouette\n",
    "    }\n",
    "    \n",
    "    pca_2d, _ = run_pca_analysis(activations, n_components=2)\n",
    "    pca_results_by_layer[layer_idx] = pca_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations\n",
    "\n",
    "### 11.1 Scatter Plots\n",
    "\n",
    "Each point represents the activation pattern for \"The answer is\" - colored by what CONTEXT preceded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in config.layers_to_probe:\n",
    "    plot_activation_scatter(\n",
    "        pca_results_by_layer[layer_idx],\n",
    "        all_labels,\n",
    "        layer_idx,\n",
    "        f\"\\n(Same tokens '{QUERY_PHRASE}' - different contexts)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Similarity Matrix\n",
    "\n",
    "Shows pairwise similarity between activations for \"The answer is\" across all probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = config.layers_to_probe[-1]\n",
    "plot_similarity_matrix(all_activations[final_layer], all_labels, final_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_comparison(metrics_by_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: CONTEXT-CONTROLLED EXCITATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_layer = config.layers_to_probe[-1]\n",
    "final_silhouette = metrics_by_layer[final_layer]['silhouette']\n",
    "final_ratio = metrics_by_layer[final_layer]['distance_ratio']\n",
    "\n",
    "silhouettes = [metrics_by_layer[l]['silhouette'] for l in config.layers_to_probe]\n",
    "ratios = [metrics_by_layer[l]['distance_ratio'] for l in config.layers_to_probe]\n",
    "\n",
    "print(f\"\\nKEY QUESTION: Do IDENTICAL tokens ('{QUERY_PHRASE}') produce\")\n",
    "print(f\"             DIFFERENT activations based on context?\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"RESULTS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n1. Do same-context probes cluster together?\")\n",
    "if final_silhouette > 0.1:\n",
    "    print(f\"   YES - Silhouette score {final_silhouette:.3f} > 0.1\")\n",
    "    print(f\"   The tokens '{QUERY_PHRASE}' cluster by CONTEXT, not token identity\")\n",
    "else:\n",
    "    print(f\"   UNCLEAR - Silhouette score {final_silhouette:.3f} is low\")\n",
    "\n",
    "print(f\"\\n2. Do different contexts produce different patterns?\")\n",
    "if final_ratio > 1.2:\n",
    "    print(f\"   YES - Distance ratio {final_ratio:.3f} > 1.2\")\n",
    "    print(f\"   Context determines activation pattern, not surface tokens\")\n",
    "else:\n",
    "    print(f\"   UNCLEAR - Distance ratio {final_ratio:.3f} is low\")\n",
    "\n",
    "print(f\"\\n3. Does context control strengthen with depth?\")\n",
    "if silhouettes[-1] > silhouettes[0]:\n",
    "    print(f\"   YES - Silhouette increases: {silhouettes[0]:.3f} -> {silhouettes[-1]:.3f}\")\n",
    "else:\n",
    "    print(f\"   NO - Silhouette does not increase with depth\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"OVERALL ASSESSMENT:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "evidence_score = 0\n",
    "if final_silhouette > 0.1:\n",
    "    evidence_score += 1\n",
    "if final_ratio > 1.2:\n",
    "    evidence_score += 1\n",
    "if silhouettes[-1] > silhouettes[0]:\n",
    "    evidence_score += 1\n",
    "\n",
    "if evidence_score >= 2:\n",
    "    print(f\"\\nSTRONG EVIDENCE that context controls AQ excitation\")\n",
    "    print(f\"The same tokens produce different activations based on context.\")\n",
    "    print(f\"This supports the AQ theory: context SELECTS which patterns excite.\")\n",
    "elif evidence_score == 1:\n",
    "    print(f\"\\nWEAK EVIDENCE for context-controlled excitation\")\n",
    "else:\n",
    "    print(f\"\\nNO CLEAR EVIDENCE for context-controlled excitation\")\n",
    "\n",
    "print(f\"\\n(Evidence score: {evidence_score}/3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL METRICS BY LAYER:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Layer':>6} | {'Silhouette':>12} | {'Distance Ratio':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for layer_idx, metrics in metrics_by_layer.items():\n",
    "    print(f\"{layer_idx:>6} | {metrics['silhouette']:>12.3f} | {metrics['distance_ratio']:>15.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture.remove_hooks()\n",
    "\n",
    "results = {\n",
    "    'config': config,\n",
    "    'activations': all_activations,\n",
    "    'labels': all_labels,\n",
    "    'texts': all_texts,\n",
    "    'metrics_by_layer': metrics_by_layer,\n",
    "    'pca_results': pca_results_by_layer,\n",
    "    'evidence_score': evidence_score\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 035B COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpretation Guide\n",
    "\n",
    "### What This Experiment Shows\n",
    "\n",
    "We took the SAME tokens (\"The answer is\") and measured their activations in different contexts:\n",
    "- After a math question\n",
    "- After a geography question\n",
    "- After a science question\n",
    "- After a sentiment question\n",
    "- After a yes/no question\n",
    "\n",
    "**If tokens determine activations**: All points would cluster together (same tokens = same pattern)\n",
    "\n",
    "**If context determines activations (AQ theory)**: Points cluster by context type (same tokens = different patterns based on what precedes them)\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This directly tests the core AQ claim: **context selects which AQ excite from the weight field**.\n",
    "\n",
    "The weights contain many possible excitation patterns. The context (what comes before) determines which pattern manifests for any given token. This is why the same word can mean/do different things in different contexts.\n",
    "\n",
    "### Relation to 035A\n",
    "\n",
    "- **035A** showed: Different discrimination types produce different patterns\n",
    "- **035B** shows: Context controls which pattern activates, even for identical tokens\n",
    "\n",
    "Together, they support the AQ theory that weights store crystallized patterns and context selects which ones excite.\n",
    "\n",
    "---\n",
    "\n",
    "**AKIRA Project - Experiment 035B**  \n",
    "Oscar Goldman - Shogu Research Group @ Datamutant.ai"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
