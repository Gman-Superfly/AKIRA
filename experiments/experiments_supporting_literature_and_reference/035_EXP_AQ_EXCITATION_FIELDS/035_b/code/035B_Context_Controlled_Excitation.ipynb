{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035B: Context-Controlled Excitation (Extended)\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Insight: AQ Are About DISCRIMINATION\n",
    "\n",
    "From `ACTION_QUANTA.md`:\n",
    "\n",
    "```\n",
    "An Action Quantum (AQ) is:\n",
    "  THE MINIMUM PATTERN THAT ENABLES CORRECT DECISION\n",
    "  \n",
    "Key aspects:\n",
    "  - MINIMUM: Cannot be reduced further\n",
    "  - PATTERN: Has internal structure\n",
    "  - ENABLES: Makes action possible\n",
    "  - CORRECT: Leads to appropriate response\n",
    "  - DECISION: Discriminates between alternatives\n",
    "```\n",
    "\n",
    "The key word is **DISCRIMINATION**. An AQ must discriminate between action alternatives.\n",
    "\n",
    "---\n",
    "\n",
    "## Why The Previous 035B Was Weak\n",
    "\n",
    "We measured \"The answer is\" - a structural phrase that appears in many contexts.\n",
    "This phrase doesn't DISCRIMINATE actions. It's scaffolding, not content.\n",
    "\n",
    "From `LANGUAGE_ACTION_CONTEXT.md`:\n",
    "\n",
    "```\n",
    "AQ don't exist in the signal alone.\n",
    "AQ don't exist in the context alone.\n",
    "AQ CRYSTALLIZE from signal + context interaction.\n",
    "```\n",
    "\n",
    "The phrase \"The answer is\" is AQ-neutral. It doesn't carry discrimination.\n",
    "\n",
    "---\n",
    "\n",
    "## What We Need to Test\n",
    "\n",
    "We need to test at the **DECISION POINT** - where the model must commit to a specific output.\n",
    "\n",
    "Three approaches:\n",
    "\n",
    "1. **Same word, different required action** - \"bank\" in finance vs river context\n",
    "2. **Last token before prediction** - where AQ must crystallize\n",
    "3. **Disambiguation points** - where context resolves ambiguity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "!pip install transformers torch numpy scikit-learn matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for extended context-controlled excitation experiment.\"\"\"\n",
    "    model_name: str = \"gpt2-medium\"  # Larger model for A100\n",
    "    layers_to_probe: List[int] = field(default_factory=list)\n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        if not self.layers_to_probe:\n",
    "            if \"gpt2-medium\" in self.model_name.lower():\n",
    "                # GPT-2 Medium has 24 layers\n",
    "                self.layers_to_probe = [0, 4, 8, 12, 16, 20, 23]\n",
    "            elif \"gpt2-large\" in self.model_name.lower():\n",
    "                # GPT-2 Large has 36 layers\n",
    "                self.layers_to_probe = [0, 6, 12, 18, 24, 30, 35]\n",
    "            elif \"gpt2-xl\" in self.model_name.lower():\n",
    "                # GPT-2 XL has 48 layers\n",
    "                self.layers_to_probe = [0, 8, 16, 24, 32, 40, 47]\n",
    "            elif \"gpt2\" in self.model_name.lower():\n",
    "                self.layers_to_probe = [0, 3, 6, 9, 11]\n",
    "            else:\n",
    "                self.layers_to_probe = [0, 4, 8, 12, 16, 20, 23]\n",
    "        \n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Layers to probe: {config.layers_to_probe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCapture:\n",
    "    \"\"\"Captures activations from specified layers using forward hooks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, layer_indices: List[int]) -> None:\n",
    "        assert len(layer_indices) > 0, \"Must specify at least one layer to probe\"\n",
    "        \n",
    "        self.activations: Dict[int, torch.Tensor] = {}\n",
    "        self.hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "        self.layer_indices = layer_indices\n",
    "        \n",
    "        if hasattr(model, 'transformer'):\n",
    "            layers = model.transformer.h\n",
    "        elif hasattr(model, 'gpt_neox'):\n",
    "            layers = model.gpt_neox.layers\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model architecture: {type(model)}\")\n",
    "        \n",
    "        assert len(layers) > max(layer_indices), \\\n",
    "            f\"Model has {len(layers)} layers but requested layer {max(layer_indices)}\"\n",
    "        \n",
    "        for idx in layer_indices:\n",
    "            layer = layers[idx]\n",
    "            hook = layer.register_forward_hook(self._make_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "        \n",
    "        print(f\"Registered hooks on layers: {layer_indices}\")\n",
    "    \n",
    "    def _make_hook(self, layer_idx: int):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                self.activations[layer_idx] = output[0].detach()\n",
    "            else:\n",
    "                self.activations[layer_idx] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.activations = {}\n",
    "    \n",
    "    def remove_hooks(self) -> None:\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def get_last_token_activation(self, layer_idx: int) -> np.ndarray:\n",
    "        \"\"\"Get activation at the LAST token (decision point).\"\"\"\n",
    "        assert layer_idx in self.activations, f\"Layer {layer_idx} not captured\"\n",
    "        act = self.activations[layer_idx]\n",
    "        return act[0, -1, :].cpu().numpy()\n",
    "    \n",
    "    def get_token_activation(self, layer_idx: int, position: int) -> np.ndarray:\n",
    "        \"\"\"Get activation at a specific token position.\"\"\"\n",
    "        assert layer_idx in self.activations, f\"Layer {layer_idx} not captured\"\n",
    "        act = self.activations[layer_idx]\n",
    "        return act[0, position, :].cpu().numpy()\n",
    "    \n",
    "    def get_all_activations(self, layer_idx: int) -> np.ndarray:\n",
    "        \"\"\"Get all token activations.\"\"\"\n",
    "        assert layer_idx in self.activations, f\"Layer {layer_idx} not captured\"\n",
    "        return self.activations[layer_idx][0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(activations: np.ndarray, labels: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Compute clustering metrics.\"\"\"\n",
    "    within_distances = []\n",
    "    between_distances = []\n",
    "    \n",
    "    distances = pairwise_distances(activations, metric='euclidean')\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] == labels[j]:\n",
    "                within_distances.append(distances[i, j])\n",
    "            else:\n",
    "                between_distances.append(distances[i, j])\n",
    "    \n",
    "    within_mean = np.mean(within_distances) if within_distances else 0\n",
    "    between_mean = np.mean(between_distances) if between_distances else 0\n",
    "    ratio = between_mean / within_mean if within_mean > 0 else float('inf')\n",
    "    \n",
    "    # Silhouette\n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_int = {l: i for i, l in enumerate(unique_labels)}\n",
    "    int_labels = [label_to_int[l] for l in labels]\n",
    "    silhouette = silhouette_score(activations, int_labels) if len(set(int_labels)) >= 2 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'within_distance': within_mean,\n",
    "        'between_distance': between_mean,\n",
    "        'distance_ratio': ratio,\n",
    "        'silhouette': silhouette\n",
    "    }\n",
    "\n",
    "\n",
    "def run_pca(activations: np.ndarray, n_components: int = 2) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Apply PCA and return reduced activations and explained variance.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(activations)\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced = pca.fit_transform(scaled)\n",
    "    explained = sum(pca.explained_variance_ratio_) * 100\n",
    "    return reduced, explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(activations_2d: np.ndarray, labels: List[str], \n",
    "                 colors: Dict[str, str], title: str, texts: List[str] = None) -> None:\n",
    "    \"\"\"Create scatter plot with optional annotations.\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for category in colors:\n",
    "        mask = [l == category for l in labels]\n",
    "        if any(mask):\n",
    "            indices = [i for i, m in enumerate(mask) if m]\n",
    "            plt.scatter(\n",
    "                activations_2d[indices, 0],\n",
    "                activations_2d[indices, 1],\n",
    "                c=colors[category],\n",
    "                label=category.replace('_', ' '),\n",
    "                alpha=0.7,\n",
    "                s=150,\n",
    "                edgecolors='white',\n",
    "                linewidth=1\n",
    "            )\n",
    "            \n",
    "            # Add text annotations if provided\n",
    "            if texts:\n",
    "                for idx in indices:\n",
    "                    # Truncate text for display\n",
    "                    short_text = texts[idx][:30] + \"...\" if len(texts[idx]) > 30 else texts[idx]\n",
    "                    plt.annotate(short_text, \n",
    "                                (activations_2d[idx, 0], activations_2d[idx, 1]),\n",
    "                                fontsize=7, alpha=0.6)\n",
    "    \n",
    "    plt.xlabel('PC1', fontsize=12)\n",
    "    plt.ylabel('PC2', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_similarity_heatmap(activations: np.ndarray, labels: List[str], title: str) -> None:\n",
    "    \"\"\"Create similarity heatmap.\"\"\"\n",
    "    norms = np.linalg.norm(activations, axis=1, keepdims=True)\n",
    "    normalized = activations / (norms + 1e-8)\n",
    "    similarity = normalized @ normalized.T\n",
    "    \n",
    "    sorted_idx = sorted(range(len(labels)), key=lambda i: labels[i])\n",
    "    similarity_sorted = similarity[sorted_idx][:, sorted_idx]\n",
    "    labels_sorted = [labels[i] for i in sorted_idx]\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(similarity_sorted, cmap='RdYlBu_r', vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={'label': 'Cosine Similarity'})\n",
    "    \n",
    "    # Category boundaries\n",
    "    unique = []\n",
    "    boundaries = [0]\n",
    "    for i, l in enumerate(labels_sorted):\n",
    "        if l not in unique:\n",
    "            unique.append(l)\n",
    "            if i > 0:\n",
    "                boundaries.append(i)\n",
    "    boundaries.append(len(labels_sorted))\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        plt.axhline(y=b, color='black', linewidth=2)\n",
    "        plt.axvline(x=b, color='black', linewidth=2)\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_layer_progression(metrics_by_layer: Dict[int, Dict[str, float]], title: str) -> None:\n",
    "    \"\"\"Plot metrics across layers.\"\"\"\n",
    "    layers = sorted(metrics_by_layer.keys())\n",
    "    silhouettes = [metrics_by_layer[l]['silhouette'] for l in layers]\n",
    "    ratios = [metrics_by_layer[l]['distance_ratio'] for l in layers]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].plot(layers, silhouettes, 'o-', color='#3498db', linewidth=2, markersize=10)\n",
    "    axes[0].set_xlabel('Layer Index', fontsize=12)\n",
    "    axes[0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "    axes[0].set_title(f'{title} - Clustering Quality', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(-0.2, 1.0)\n",
    "    \n",
    "    axes[1].plot(layers, ratios, 'o-', color='#e74c3c', linewidth=2, markersize=10)\n",
    "    axes[1].set_xlabel('Layer Index', fontsize=12)\n",
    "    axes[1].set_ylabel('Between/Within Distance Ratio', fontsize=12)\n",
    "    axes[1].set_title(f'{title} - Category Separation', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {DEVICE}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Layers: {len(model.transformer.h)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPERIMENT A: Polysemous Words at Decision Point\n",
    "\n",
    "## The Test\n",
    "\n",
    "The word \"bank\" can mean:\n",
    "- Financial institution\n",
    "- River edge\n",
    "- To tilt (aircraft)\n",
    "- To rely on\n",
    "\n",
    "Same word. Different DISCRIMINATIONS required. Different ACTIONS enabled.\n",
    "\n",
    "If AQ theory is correct:\n",
    "- The SAME token \"bank\" should produce DIFFERENT activations\n",
    "- Activations should cluster by MEANING (action required), not by token identity\n",
    "\n",
    "We measure at the LAST TOKEN (decision point) where the model must commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polysemous word probes - same word, different meanings\n",
    "POLYSEMY_PROBES = {\n",
    "    'bank_financial': [\n",
    "        \"I need to deposit money at the bank\",\n",
    "        \"The bank approved my loan application\",\n",
    "        \"She works as a teller at the bank\",\n",
    "        \"The bank charges high interest rates\",\n",
    "        \"I opened a savings account at the bank\",\n",
    "        \"The bank is closed on Sundays\",\n",
    "        \"He withdrew cash from the bank\",\n",
    "        \"The bank sent my monthly statement\",\n",
    "    ],\n",
    "    'bank_river': [\n",
    "        \"We sat on the bank watching the river\",\n",
    "        \"The fisherman stood on the bank\",\n",
    "        \"Flowers grew along the river bank\",\n",
    "        \"The children played on the bank\",\n",
    "        \"We walked along the bank of the stream\",\n",
    "        \"Trees lined the bank on both sides\",\n",
    "        \"The boat was tied to the bank\",\n",
    "        \"Ducks swam near the bank\",\n",
    "    ],\n",
    "    'spring_season': [\n",
    "        \"The flowers bloom in spring\",\n",
    "        \"Spring is my favorite season\",\n",
    "        \"The birds return in spring\",\n",
    "        \"We plant gardens in spring\",\n",
    "        \"Spring brings warmer weather\",\n",
    "        \"The days get longer in spring\",\n",
    "        \"Spring rain helps the crops grow\",\n",
    "        \"Children play outside in spring\",\n",
    "    ],\n",
    "    'spring_coil': [\n",
    "        \"The spring in the mattress broke\",\n",
    "        \"A spring provides tension in the mechanism\",\n",
    "        \"The spring bounced back into shape\",\n",
    "        \"He compressed the spring tightly\",\n",
    "        \"The spring stores mechanical energy\",\n",
    "        \"A broken spring caused the malfunction\",\n",
    "        \"The spring mechanism needs repair\",\n",
    "        \"Metal springs are used in cars\",\n",
    "    ],\n",
    "    'bat_animal': [\n",
    "        \"A bat flew out of the cave\",\n",
    "        \"Bats are nocturnal mammals\",\n",
    "        \"The bat uses echolocation\",\n",
    "        \"We saw a bat hanging upside down\",\n",
    "        \"The bat caught insects in flight\",\n",
    "        \"Bats sleep during the day\",\n",
    "        \"A bat colony lives in the attic\",\n",
    "        \"The bat spread its wings\",\n",
    "    ],\n",
    "    'bat_sports': [\n",
    "        \"He swung the bat and hit a home run\",\n",
    "        \"The baseball bat was made of wood\",\n",
    "        \"She gripped the bat tightly\",\n",
    "        \"The bat cracked on impact\",\n",
    "        \"He practiced with the bat for hours\",\n",
    "        \"The bat connected with the ball\",\n",
    "        \"A new bat improved his hitting\",\n",
    "        \"The bat felt heavy in his hands\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "POLYSEMY_COLORS = {\n",
    "    'bank_financial': '#3498db',  # blue\n",
    "    'bank_river': '#2ecc71',      # green\n",
    "    'spring_season': '#f39c12',   # orange\n",
    "    'spring_coil': '#9b59b6',     # purple\n",
    "    'bat_animal': '#e74c3c',      # red\n",
    "    'bat_sports': '#1abc9c',      # teal\n",
    "}\n",
    "\n",
    "print(f\"Polysemy probes: {len(POLYSEMY_PROBES)} categories\")\n",
    "print(f\"Total probes: {sum(len(v) for v in POLYSEMY_PROBES.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run polysemy experiment\n",
    "capture = ActivationCapture(model, config.layers_to_probe)\n",
    "\n",
    "polysemy_activations = {layer: [] for layer in config.layers_to_probe}\n",
    "polysemy_labels = []\n",
    "polysemy_texts = []\n",
    "\n",
    "print(\"Running polysemy probes (measuring at LAST token - decision point)...\")\n",
    "\n",
    "for category, prompts in POLYSEMY_PROBES.items():\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        capture.clear()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get LAST token activation (decision point)\n",
    "        for layer_idx in config.layers_to_probe:\n",
    "            act = capture.get_last_token_activation(layer_idx)\n",
    "            polysemy_activations[layer_idx].append(act)\n",
    "        \n",
    "        polysemy_labels.append(category)\n",
    "        polysemy_texts.append(prompt)\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    polysemy_activations[layer_idx] = np.array(polysemy_activations[layer_idx])\n",
    "\n",
    "print(f\"Collected {len(polysemy_labels)} activations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze polysemy results\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT A: POLYSEMOUS WORDS AT DECISION POINT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQuestion: Does the SAME word produce DIFFERENT activations\")\n",
    "print(\"          based on the DISCRIMINATION required?\\n\")\n",
    "\n",
    "polysemy_metrics = {}\n",
    "polysemy_pca = {}\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    print(f\"--- Layer {layer_idx} ---\")\n",
    "    \n",
    "    acts = polysemy_activations[layer_idx]\n",
    "    metrics = compute_metrics(acts, polysemy_labels)\n",
    "    polysemy_metrics[layer_idx] = metrics\n",
    "    \n",
    "    print(f\"Silhouette: {metrics['silhouette']:.4f}\")\n",
    "    print(f\"Distance ratio: {metrics['distance_ratio']:.4f}\")\n",
    "    \n",
    "    pca_2d, explained = run_pca(acts)\n",
    "    polysemy_pca[layer_idx] = pca_2d\n",
    "    print(f\"PCA variance explained: {explained:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize polysemy results\n",
    "final_layer = config.layers_to_probe[-1]\n",
    "\n",
    "plot_scatter(polysemy_pca[final_layer], polysemy_labels, POLYSEMY_COLORS,\n",
    "             f\"Polysemous Words - Layer {final_layer} (Decision Point)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show early vs late layer comparison\n",
    "early_layer = config.layers_to_probe[1]  # Second layer\n",
    "plot_scatter(polysemy_pca[early_layer], polysemy_labels, POLYSEMY_COLORS,\n",
    "             f\"Polysemous Words - Layer {early_layer} (Early)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity_heatmap(polysemy_activations[final_layer], polysemy_labels,\n",
    "                        f\"Polysemy Similarity - Layer {final_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_progression(polysemy_metrics, \"Polysemy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPERIMENT B: Required Action Discrimination\n",
    "\n",
    "## The Test\n",
    "\n",
    "Different contexts require different ACTIONS:\n",
    "- Arithmetic: compute a number\n",
    "- Yes/No: produce true/false\n",
    "- Completion: continue text naturally\n",
    "- Definition: explain meaning\n",
    "\n",
    "Same final structure \"...is\" but different AQ must crystallize because different DISCRIMINATIONS are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action discrimination probes\n",
    "ACTION_PROBES = {\n",
    "    'compute_number': [\n",
    "        \"2 + 3 is\",\n",
    "        \"10 - 4 is\",\n",
    "        \"5 times 2 is\",\n",
    "        \"8 divided by 2 is\",\n",
    "        \"The sum of 7 and 3 is\",\n",
    "        \"Half of 20 is\",\n",
    "        \"3 squared is\",\n",
    "        \"The product of 4 and 5 is\",\n",
    "    ],\n",
    "    'answer_yesno': [\n",
    "        \"The sky is blue. This is\",\n",
    "        \"Fire is cold. This is\",\n",
    "        \"Water is wet. This is\",\n",
    "        \"Ice is hot. This is\",\n",
    "        \"Grass is green. This is\",\n",
    "        \"Snow is black. This is\",\n",
    "        \"The sun is bright. This is\",\n",
    "        \"Night is dark. This is\",\n",
    "    ],\n",
    "    'complete_sentence': [\n",
    "        \"The weather today is\",\n",
    "        \"My favorite color is\",\n",
    "        \"The best food is\",\n",
    "        \"Life is\",\n",
    "        \"The world is\",\n",
    "        \"Music is\",\n",
    "        \"Love is\",\n",
    "        \"Time is\",\n",
    "    ],\n",
    "    'provide_fact': [\n",
    "        \"The capital of France is\",\n",
    "        \"The largest planet is\",\n",
    "        \"The chemical formula for water is\",\n",
    "        \"The speed of light is\",\n",
    "        \"The first president of the USA is\",\n",
    "        \"The tallest mountain is\",\n",
    "        \"The deepest ocean is\",\n",
    "        \"The longest river is\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "ACTION_COLORS = {\n",
    "    'compute_number': '#3498db',   # blue\n",
    "    'answer_yesno': '#e74c3c',     # red\n",
    "    'complete_sentence': '#2ecc71', # green\n",
    "    'provide_fact': '#9b59b6',     # purple\n",
    "}\n",
    "\n",
    "print(f\"Action probes: {len(ACTION_PROBES)} categories\")\n",
    "print(f\"Total probes: {sum(len(v) for v in ACTION_PROBES.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action discrimination experiment\n",
    "action_activations = {layer: [] for layer in config.layers_to_probe}\n",
    "action_labels = []\n",
    "action_texts = []\n",
    "\n",
    "print(\"Running action discrimination probes...\")\n",
    "\n",
    "for category, prompts in ACTION_PROBES.items():\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        capture.clear()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        for layer_idx in config.layers_to_probe:\n",
    "            act = capture.get_last_token_activation(layer_idx)\n",
    "            action_activations[layer_idx].append(act)\n",
    "        \n",
    "        action_labels.append(category)\n",
    "        action_texts.append(prompt)\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    action_activations[layer_idx] = np.array(action_activations[layer_idx])\n",
    "\n",
    "print(f\"Collected {len(action_labels)} activations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze action results\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT B: ACTION DISCRIMINATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQuestion: Do different REQUIRED ACTIONS produce different AQ patterns?\\n\")\n",
    "\n",
    "action_metrics = {}\n",
    "action_pca = {}\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    print(f\"--- Layer {layer_idx} ---\")\n",
    "    \n",
    "    acts = action_activations[layer_idx]\n",
    "    metrics = compute_metrics(acts, action_labels)\n",
    "    action_metrics[layer_idx] = metrics\n",
    "    \n",
    "    print(f\"Silhouette: {metrics['silhouette']:.4f}\")\n",
    "    print(f\"Distance ratio: {metrics['distance_ratio']:.4f}\")\n",
    "    \n",
    "    pca_2d, explained = run_pca(acts)\n",
    "    action_pca[layer_idx] = pca_2d\n",
    "    print(f\"PCA variance explained: {explained:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize action results\n",
    "plot_scatter(action_pca[final_layer], action_labels, ACTION_COLORS,\n",
    "             f\"Action Discrimination - Layer {final_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity_heatmap(action_activations[final_layer], action_labels,\n",
    "                        f\"Action Discrimination Similarity - Layer {final_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_progression(action_metrics, \"Action Discrimination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPERIMENT C: Disambiguation at Critical Token\n",
    "\n",
    "## The Test\n",
    "\n",
    "Some sentences have ambiguity that gets resolved at a specific token.\n",
    "The AQ should crystallize differently based on how ambiguity resolves.\n",
    "\n",
    "Example:\n",
    "- \"I saw her duck\" - could be the animal or the action\n",
    "- \"I saw her duck under the fence\" - action resolved\n",
    "- \"I saw her duck swimming in the pond\" - animal resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disambiguation probes\n",
    "DISAMBIGUATION_PROBES = {\n",
    "    'flying_planes_people': [\n",
    "        \"Flying planes can be dangerous for pilots\",\n",
    "        \"Flying planes require skilled operators\",\n",
    "        \"Flying planes involves risk for the crew\",\n",
    "        \"Flying planes takes training and practice\",\n",
    "    ],\n",
    "    'flying_planes_aircraft': [\n",
    "        \"Flying planes can be seen from the ground\",\n",
    "        \"Flying planes leave contrails in the sky\",\n",
    "        \"Flying planes are visible overhead\",\n",
    "        \"Flying planes cross the ocean daily\",\n",
    "    ],\n",
    "    'time_flies_speed': [\n",
    "        \"Time flies when you are having fun\",\n",
    "        \"Time flies during enjoyable activities\",\n",
    "        \"Time flies while playing games\",\n",
    "        \"Time flies on vacation\",\n",
    "    ],\n",
    "    'time_flies_insects': [\n",
    "        \"Time flies like fruit and sweet things\",\n",
    "        \"Time flies are attracted to rotting food\",\n",
    "        \"Time flies buzz around the garbage\",\n",
    "        \"Time flies are a type of small insect\",\n",
    "    ],\n",
    "    'visiting_relatives_action': [\n",
    "        \"Visiting relatives can be tiring for hosts\",\n",
    "        \"Visiting relatives requires hospitality\",\n",
    "        \"Visiting relatives means preparing guest rooms\",\n",
    "        \"Visiting relatives is a social obligation\",\n",
    "    ],\n",
    "    'visiting_relatives_people': [\n",
    "        \"Visiting relatives bring gifts and stories\",\n",
    "        \"Visiting relatives arrived from out of town\",\n",
    "        \"Visiting relatives stayed for the weekend\",\n",
    "        \"Visiting relatives gathered for the holiday\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "DISAMBIGUATION_COLORS = {\n",
    "    'flying_planes_people': '#3498db',\n",
    "    'flying_planes_aircraft': '#2ecc71',\n",
    "    'time_flies_speed': '#e74c3c',\n",
    "    'time_flies_insects': '#f39c12',\n",
    "    'visiting_relatives_action': '#9b59b6',\n",
    "    'visiting_relatives_people': '#1abc9c',\n",
    "}\n",
    "\n",
    "print(f\"Disambiguation probes: {len(DISAMBIGUATION_PROBES)} categories\")\n",
    "print(f\"Total probes: {sum(len(v) for v in DISAMBIGUATION_PROBES.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run disambiguation experiment\n",
    "disambig_activations = {layer: [] for layer in config.layers_to_probe}\n",
    "disambig_labels = []\n",
    "disambig_texts = []\n",
    "\n",
    "print(\"Running disambiguation probes...\")\n",
    "\n",
    "for category, prompts in DISAMBIGUATION_PROBES.items():\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        capture.clear()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        for layer_idx in config.layers_to_probe:\n",
    "            act = capture.get_last_token_activation(layer_idx)\n",
    "            disambig_activations[layer_idx].append(act)\n",
    "        \n",
    "        disambig_labels.append(category)\n",
    "        disambig_texts.append(prompt)\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    disambig_activations[layer_idx] = np.array(disambig_activations[layer_idx])\n",
    "\n",
    "print(f\"Collected {len(disambig_labels)} activations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze disambiguation results\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT C: DISAMBIGUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQuestion: Do ambiguous phrases produce different patterns\")\n",
    "print(\"          based on how they are RESOLVED?\\n\")\n",
    "\n",
    "disambig_metrics = {}\n",
    "disambig_pca = {}\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    print(f\"--- Layer {layer_idx} ---\")\n",
    "    \n",
    "    acts = disambig_activations[layer_idx]\n",
    "    metrics = compute_metrics(acts, disambig_labels)\n",
    "    disambig_metrics[layer_idx] = metrics\n",
    "    \n",
    "    print(f\"Silhouette: {metrics['silhouette']:.4f}\")\n",
    "    print(f\"Distance ratio: {metrics['distance_ratio']:.4f}\")\n",
    "    \n",
    "    pca_2d, explained = run_pca(acts)\n",
    "    disambig_pca[layer_idx] = pca_2d\n",
    "    print(f\"PCA variance explained: {explained:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disambiguation results\n",
    "plot_scatter(disambig_pca[final_layer], disambig_labels, DISAMBIGUATION_COLORS,\n",
    "             f\"Disambiguation - Layer {final_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_progression(disambig_metrics, \"Disambiguation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT 035B EXTENDED - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_layer = config.layers_to_probe[-1]\n",
    "\n",
    "experiments = [\n",
    "    (\"A: Polysemous Words\", polysemy_metrics[final_layer]),\n",
    "    (\"B: Action Discrimination\", action_metrics[final_layer]),\n",
    "    (\"C: Disambiguation\", disambig_metrics[final_layer]),\n",
    "]\n",
    "\n",
    "print(f\"\\nFinal Layer: {final_layer}\")\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"{'Experiment':<25} | {'Silhouette':>12} | {'Distance Ratio':>15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_evidence = 0\n",
    "for name, metrics in experiments:\n",
    "    sil = metrics['silhouette']\n",
    "    ratio = metrics['distance_ratio']\n",
    "    print(f\"{name:<25} | {sil:>12.3f} | {ratio:>15.3f}\")\n",
    "    \n",
    "    if sil > 0.1:\n",
    "        total_evidence += 1\n",
    "    if ratio > 1.3:\n",
    "        total_evidence += 1\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nTotal Evidence Score: {total_evidence}/6\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "These experiments test AQ theory more rigorously:\n",
    "\n",
    "A. POLYSEMOUS WORDS: Same token, different meaning\n",
    "   - \"bank\" (financial) vs \"bank\" (river)\n",
    "   - If AQ theory is correct: cluster by MEANING, not token\n",
    "\n",
    "B. ACTION DISCRIMINATION: Different required outputs\n",
    "   - Compute number vs Answer yes/no vs Complete sentence\n",
    "   - If AQ theory is correct: cluster by ACTION TYPE\n",
    "\n",
    "C. DISAMBIGUATION: Same ambiguous phrase, different resolution\n",
    "   - \"Flying planes\" as activity vs aircraft\n",
    "   - If AQ theory is correct: cluster by RESOLVED MEANING\n",
    "\n",
    "KEY INSIGHT:\n",
    "   We measure at the LAST TOKEN (decision point) because that's\n",
    "   where the AQ must crystallize to enable correct action.\n",
    "   \n",
    "   The previous 035B failed because \"The answer is\" is structural\n",
    "   scaffolding - it doesn't discriminate actions.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "capture.remove_hooks()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 035B EXTENDED COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Relation to AQ Theory\n",
    "\n",
    "From `ACTION_QUANTA.md`:\n",
    "\n",
    "```\n",
    "AQ (pattern) -> enables DISCRIMINATION -> enables ACTION\n",
    "\n",
    "STRUCTURAL: AQ = Minimum PATTERN (what it IS)\n",
    "FUNCTIONAL: Discrimination = ATOMIC ABSTRACTION (what it DOES)\n",
    "```\n",
    "\n",
    "These experiments test whether:\n",
    "1. The same surface form (\"bank\") produces different AQ based on discrimination required\n",
    "2. Different action types (compute vs classify) produce different AQ patterns\n",
    "3. Ambiguity resolution crystallizes different AQ\n",
    "\n",
    "From `LANGUAGE_ACTION_CONTEXT.md`:\n",
    "\n",
    "```\n",
    "Linguistic signal + Experiential context -> AQ crystallizes -> Action\n",
    "\n",
    "Language provides the TRIGGER.\n",
    "Context provides the DISCRIMINATION SPACE.\n",
    "AQ is what enables the ACTION CHOICE.\n",
    "```\n",
    "\n",
    "The extended experiments test this by varying:\n",
    "- Same trigger (\"bank\"), different discrimination space\n",
    "- Different triggers, same structure (\"...is\"), different required action\n",
    "- Ambiguous trigger, different resolution\n",
    "\n",
    "---\n",
    "\n",
    "**AKIRA Project - Experiment 035B Extended**  \n",
    "Oscar Goldman - Shogu Research Group @ Datamutant.ai"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
