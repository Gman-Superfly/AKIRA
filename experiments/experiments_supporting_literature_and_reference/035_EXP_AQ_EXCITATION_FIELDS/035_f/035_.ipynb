{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035F: Compositional Bonding with Controls\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "Strengthen 035D findings with proper controls and explain the Layer 0 result.\n",
    "\n",
    "From 035D findings:\n",
    "- Complex action discriminations decompose into simpler AQ components (p=9.9e-65)\n",
    "- Best decomposition signal was at Layer 0\n",
    "- Component/control ratio was 1.18x\n",
    "\n",
    "This experiment addresses:\n",
    "1. **Controls**: Are results specific to action composition, or just word co-occurrence?\n",
    "2. **Layer dynamics**: Test the \"early decomposition, late fusion\" hypothesis\n",
    "3. **Sample size**: 50 samples per combination (vs 8-10 in 035D)\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "If AQ bonding is real:\n",
    "1. Bonded states should be MORE similar to components than to shuffled controls\n",
    "2. Bonded states should be MORE similar to components than to length-matched non-action prompts\n",
    "3. Bonded states should be MORE similar to components than to semantic-only controls\n",
    "4. Decomposition score should DECREASE with layer depth (fusion hypothesis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# !pip install transformers torch numpy scikit-learn matplotlib seaborn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import permutation_test\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for compositional controls experiment.\"\"\"\n",
    "    \n",
    "    # Models to test\n",
    "    models: Dict[str, str] = field(default_factory=lambda: {\n",
    "        \"gpt2-medium\": \"gpt2-medium\",\n",
    "        \"pythia-1.4b\": \"EleutherAI/pythia-1.4b\",\n",
    "        # \"gemma-2b\": \"google/gemma-2b\",  # Uncomment if you have access\n",
    "    })\n",
    "    \n",
    "    # Samples per bond level\n",
    "    samples_per_level: int = 50\n",
    "    \n",
    "    # Bond levels to test\n",
    "    bond_levels: List[int] = field(default_factory=lambda: [1, 2, 3, 4])\n",
    "    \n",
    "    # Statistical parameters\n",
    "    n_permutations: int = 10000\n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Models to test: {list(config.models.keys())}\")\n",
    "print(f\"Bond levels: {config.bond_levels}\")\n",
    "print(f\"Samples per level: {config.samples_per_level}\")\n",
    "print(f\"Permutations for significance: {config.n_permutations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Generation with Controls\n",
    "\n",
    "For each bonded state prompt, we generate three types of controls:\n",
    "\n",
    "1. **Shuffled control**: Same words, scrambled order (tests if word ORDER matters)\n",
    "2. **Length-matched control**: Non-action prompts of same token length (tests if action content matters)\n",
    "3. **Semantic-only control**: Action words without action context (tests if action CONTEXT matters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQ Components for compositional testing\n",
    "AQ_COMPONENTS = {\n",
    "    \"threat\": {\n",
    "        \"keywords\": [\"dangerous\", \"fire\", \"snake\", \"attack\", \"hazard\", \"bomb\", \"collapse\", \"poison\"],\n",
    "        \"templates\": [\n",
    "            \"A {adj} {noun} threatens you. You should\",\n",
    "            \"There is a {adj} {noun} ahead. You should\",\n",
    "            \"You face a {adj} {noun}. You should\",\n",
    "            \"A {adj} {noun} appears. You should\",\n",
    "            \"Danger from a {adj} {noun}. You should\",\n",
    "        ],\n",
    "        \"adjectives\": [\"deadly\", \"dangerous\", \"lethal\", \"harmful\", \"menacing\"],\n",
    "        \"nouns\": [\"fire\", \"snake\", \"attacker\", \"gas leak\", \"predator\", \"explosion\"]\n",
    "    },\n",
    "    \"urgency\": {\n",
    "        \"keywords\": [\"immediately\", \"now\", \"seconds\", \"instant\", \"fast\", \"quick\", \"hurry\"],\n",
    "        \"templates\": [\n",
    "            \"You have {time} to act. You should\",\n",
    "            \"Time is running out, {time} left. You should\",\n",
    "            \"Act within {time} or fail. You should\",\n",
    "            \"Only {time} remain. You should\",\n",
    "            \"The deadline is {time}. You should\",\n",
    "        ],\n",
    "        \"times\": [\"5 seconds\", \"3 seconds\", \"moments\", \"an instant\", \"10 seconds\"]\n",
    "    },\n",
    "    \"direction\": {\n",
    "        \"keywords\": [\"left\", \"right\", \"behind\", \"ahead\", \"above\", \"below\", \"north\", \"south\"],\n",
    "        \"templates\": [\n",
    "            \"The path is to your {dir}. You should\",\n",
    "            \"Move {dir} for safety. You should\",\n",
    "            \"The exit is {dir}. You should\",\n",
    "            \"Go {dir} to escape. You should\",\n",
    "            \"Safety lies {dir}. You should\",\n",
    "        ],\n",
    "        \"directions\": [\"left\", \"right\", \"behind you\", \"straight ahead\", \"to the north\"]\n",
    "    },\n",
    "    \"proximity\": {\n",
    "        \"keywords\": [\"close\", \"near\", \"inches\", \"feet\", \"steps\", \"reach\", \"beside\"],\n",
    "        \"templates\": [\n",
    "            \"It is {dist} away. You should\",\n",
    "            \"Only {dist} separates you. You should\",\n",
    "            \"The distance is {dist}. You should\",\n",
    "            \"You are {dist} from it. You should\",\n",
    "            \"Within {dist} of you. You should\",\n",
    "        ],\n",
    "        \"distances\": [\"inches\", \"a few feet\", \"arm's reach\", \"steps\", \"touching distance\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"AQ components: {list(AQ_COMPONENTS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_aq_prompts(component: str, n: int = 50) -> List[str]:\n",
    "    \"\"\"Generate prompts for a single AQ component.\n",
    "    \n",
    "    Args:\n",
    "        component: Name of AQ component\n",
    "        n: Number of prompts to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of prompts\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    comp_data = AQ_COMPONENTS[component]\n",
    "    \n",
    "    for i in range(n):\n",
    "        template = comp_data[\"templates\"][i % len(comp_data[\"templates\"])]\n",
    "        \n",
    "        if component == \"threat\":\n",
    "            adj = comp_data[\"adjectives\"][i % len(comp_data[\"adjectives\"])]\n",
    "            noun = comp_data[\"nouns\"][i % len(comp_data[\"nouns\"])]\n",
    "            prompts.append(template.format(adj=adj, noun=noun))\n",
    "        elif component == \"urgency\":\n",
    "            time = comp_data[\"times\"][i % len(comp_data[\"times\"])]\n",
    "            prompts.append(template.format(time=time))\n",
    "        elif component == \"direction\":\n",
    "            dir = comp_data[\"directions\"][i % len(comp_data[\"directions\"])]\n",
    "            prompts.append(template.format(dir=dir))\n",
    "        elif component == \"proximity\":\n",
    "            dist = comp_data[\"distances\"][i % len(comp_data[\"distances\"])]\n",
    "            prompts.append(template.format(dist=dist))\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "\n",
    "def generate_bonded_prompts(components: List[str], n: int = 50) -> List[str]:\n",
    "    \"\"\"Generate prompts combining multiple AQ components.\n",
    "    \n",
    "    Args:\n",
    "        components: List of component names to combine\n",
    "        n: Number of prompts to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of bonded prompts\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    # Templates for different bond levels\n",
    "    if len(components) == 2:\n",
    "        templates = [\n",
    "            \"{c1} and {c2}. You should\",\n",
    "            \"{c1}, plus {c2}. You should\",\n",
    "            \"Facing {c1} while {c2}. You should\",\n",
    "            \"{c1}. Additionally, {c2}. You should\",\n",
    "            \"With {c1} and {c2}. You should\",\n",
    "        ]\n",
    "    elif len(components) == 3:\n",
    "        templates = [\n",
    "            \"{c1}, {c2}, and {c3}. You should\",\n",
    "            \"{c1}. Also {c2}. Furthermore {c3}. You should\",\n",
    "            \"Confronting {c1} with {c2} and {c3}. You should\",\n",
    "            \"{c1} combines with {c2} and {c3}. You should\",\n",
    "            \"The situation: {c1}, {c2}, {c3}. You should\",\n",
    "        ]\n",
    "    elif len(components) == 4:\n",
    "        templates = [\n",
    "            \"{c1}, {c2}, {c3}, and {c4}. You should\",\n",
    "            \"{c1} with {c2}. Plus {c3} and {c4}. You should\",\n",
    "            \"Critical: {c1}, {c2}, {c3}, {c4}. You should\",\n",
    "            \"All at once: {c1}, {c2}, {c3}, {c4}. You should\",\n",
    "            \"Situation: {c1}. {c2}. {c3}. {c4}. You should\",\n",
    "        ]\n",
    "    else:\n",
    "        return generate_single_aq_prompts(components[0], n)\n",
    "    \n",
    "    # Generate component snippets\n",
    "    component_snippets = {}\n",
    "    for comp in components:\n",
    "        snippets = []\n",
    "        comp_data = AQ_COMPONENTS[comp]\n",
    "        if comp == \"threat\":\n",
    "            for adj, noun in zip(comp_data[\"adjectives\"], comp_data[\"nouns\"]):\n",
    "                snippets.append(f\"a {adj} {noun}\")\n",
    "        elif comp == \"urgency\":\n",
    "            snippets = [f\"only {t} to act\" for t in comp_data[\"times\"]]\n",
    "        elif comp == \"direction\":\n",
    "            snippets = [f\"escape is {d}\" for d in comp_data[\"directions\"]]\n",
    "        elif comp == \"proximity\":\n",
    "            snippets = [f\"{d} away\" for d in comp_data[\"distances\"]]\n",
    "        component_snippets[comp] = snippets\n",
    "    \n",
    "    for i in range(n):\n",
    "        template = templates[i % len(templates)]\n",
    "        \n",
    "        # Get snippets for each component\n",
    "        c_snippets = []\n",
    "        for j, comp in enumerate(components):\n",
    "            snippets = component_snippets[comp]\n",
    "            c_snippets.append(snippets[(i + j) % len(snippets)])\n",
    "        \n",
    "        if len(components) == 2:\n",
    "            prompts.append(template.format(c1=c_snippets[0], c2=c_snippets[1]))\n",
    "        elif len(components) == 3:\n",
    "            prompts.append(template.format(c1=c_snippets[0], c2=c_snippets[1], c3=c_snippets[2]))\n",
    "        elif len(components) == 4:\n",
    "            prompts.append(template.format(c1=c_snippets[0], c2=c_snippets[1], \n",
    "                                           c3=c_snippets[2], c4=c_snippets[3]))\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "\n",
    "print(\"Prompt generation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shuffled_control(prompt: str) -> str:\n",
    "    \"\"\"Create shuffled control by randomizing word order.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Original prompt\n",
    "        \n",
    "    Returns:\n",
    "        Shuffled version of prompt\n",
    "    \"\"\"\n",
    "    # Split into words, shuffle, rejoin\n",
    "    words = prompt.split()\n",
    "    \n",
    "    # Keep \"You should\" at end for consistency\n",
    "    if len(words) >= 2 and words[-2:] == [\"You\", \"should\"]:\n",
    "        main_words = words[:-2]\n",
    "        random.shuffle(main_words)\n",
    "        return \" \".join(main_words + [\"You\", \"should\"])\n",
    "    else:\n",
    "        random.shuffle(words)\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "def generate_length_matched_control(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Create non-action prompt of same token length.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Original prompt\n",
    "        tokenizer: Tokenizer for measuring length\n",
    "        \n",
    "    Returns:\n",
    "        Non-action prompt of similar length\n",
    "    \"\"\"\n",
    "    target_length = len(tokenizer.encode(prompt))\n",
    "    \n",
    "    # Pool of neutral/descriptive sentences\n",
    "    neutral_templates = [\n",
    "        \"The weather today is quite pleasant with clear skies and mild temperatures.\",\n",
    "        \"Mathematics involves the study of numbers, quantities, and shapes.\",\n",
    "        \"The library contains many books on various topics and subjects.\",\n",
    "        \"Trees provide oxygen and shade during the warm summer months.\",\n",
    "        \"Music has been part of human culture for thousands of years.\",\n",
    "        \"The ocean covers more than seventy percent of the Earth's surface.\",\n",
    "        \"Science helps us understand the natural world around us.\",\n",
    "        \"Art can express emotions and ideas in visual form.\",\n",
    "        \"History teaches us about events from the past.\",\n",
    "        \"Language allows humans to communicate complex thoughts and ideas.\",\n",
    "        \"The moon orbits the Earth approximately every twenty-eight days.\",\n",
    "        \"Plants require sunlight, water, and nutrients to grow properly.\",\n",
    "        \"Architecture combines art and engineering to create buildings.\",\n",
    "        \"Literature includes novels, poems, plays, and short stories.\",\n",
    "        \"Geography studies the physical features of the Earth.\",\n",
    "    ]\n",
    "    \n",
    "    # Find closest match\n",
    "    best_match = neutral_templates[0]\n",
    "    best_diff = abs(len(tokenizer.encode(best_match)) - target_length)\n",
    "    \n",
    "    for template in neutral_templates:\n",
    "        length = len(tokenizer.encode(template))\n",
    "        diff = abs(length - target_length)\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_match = template\n",
    "    \n",
    "    # Adjust length if needed by adding/removing words\n",
    "    return best_match + \" It is\"\n",
    "\n",
    "\n",
    "def generate_semantic_only_control(components: List[str]) -> str:\n",
    "    \"\"\"Create prompt with action words but no action context.\n",
    "    \n",
    "    Args:\n",
    "        components: List of AQ components used\n",
    "        \n",
    "    Returns:\n",
    "        Semantic-only control prompt\n",
    "    \"\"\"\n",
    "    # Use action words in a definitional/descriptive context\n",
    "    definitions = {\n",
    "        \"threat\": \"The word 'danger' refers to potential harm or risk. It is\",\n",
    "        \"urgency\": \"The concept of 'immediately' means without delay. It is\",\n",
    "        \"direction\": \"The term 'left' describes a spatial position. It is\",\n",
    "        \"proximity\": \"The word 'close' indicates nearness in distance. It is\",\n",
    "    }\n",
    "    \n",
    "    # Combine definitions for multi-component cases\n",
    "    if len(components) == 1:\n",
    "        return definitions.get(components[0], \"Words have meanings. They are\")\n",
    "    else:\n",
    "        words = []\n",
    "        for comp in components:\n",
    "            keywords = AQ_COMPONENTS[comp][\"keywords\"]\n",
    "            words.append(keywords[0])\n",
    "        word_list = \", \".join(words[:-1]) + \" and \" + words[-1]\n",
    "        return f\"The words {word_list} are vocabulary items. They are\"\n",
    "\n",
    "\n",
    "print(\"Control generation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all prompts\n",
    "print(\"Generating prompts and controls...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "all_components = list(AQ_COMPONENTS.keys())\n",
    "\n",
    "# Structure: {bond_level: {combination_name: {\"bonded\": [...], \"shuffled\": [...], etc.}}}\n",
    "PROMPTS = {}\n",
    "\n",
    "# Level 1: Single AQ\n",
    "PROMPTS[1] = {}\n",
    "for comp in all_components:\n",
    "    bonded = generate_single_aq_prompts(comp, config.samples_per_level)\n",
    "    PROMPTS[1][comp] = {\n",
    "        \"bonded\": bonded,\n",
    "        \"components\": [comp]\n",
    "    }\n",
    "    print(f\"Level 1 - {comp}: {len(bonded)} prompts\")\n",
    "\n",
    "# Level 2: Two-bond\n",
    "PROMPTS[2] = {}\n",
    "for combo in combinations(all_components, 2):\n",
    "    name = \"_\".join(combo)\n",
    "    bonded = generate_bonded_prompts(list(combo), config.samples_per_level)\n",
    "    PROMPTS[2][name] = {\n",
    "        \"bonded\": bonded,\n",
    "        \"components\": list(combo)\n",
    "    }\n",
    "    print(f\"Level 2 - {name}: {len(bonded)} prompts\")\n",
    "\n",
    "# Level 3: Three-bond\n",
    "PROMPTS[3] = {}\n",
    "for combo in combinations(all_components, 3):\n",
    "    name = \"_\".join(combo)\n",
    "    bonded = generate_bonded_prompts(list(combo), config.samples_per_level)\n",
    "    PROMPTS[3][name] = {\n",
    "        \"bonded\": bonded,\n",
    "        \"components\": list(combo)\n",
    "    }\n",
    "    print(f\"Level 3 - {name}: {len(bonded)} prompts\")\n",
    "\n",
    "# Level 4: Four-bond (all components)\n",
    "PROMPTS[4] = {}\n",
    "name = \"_\".join(all_components)\n",
    "bonded = generate_bonded_prompts(all_components, config.samples_per_level)\n",
    "PROMPTS[4][name] = {\n",
    "    \"bonded\": bonded,\n",
    "    \"components\": all_components\n",
    "}\n",
    "print(f\"Level 4 - {name}: {len(bonded)} prompts\")\n",
    "\n",
    "print(f\"\\nTotal prompt sets: {sum(len(PROMPTS[l]) for l in PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Activation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layers(model_name: str) -> List[int]:\n",
    "    \"\"\"Get all layer indices for a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name/path of the model\n",
    "        \n",
    "    Returns:\n",
    "        List of all layer indices\n",
    "    \"\"\"\n",
    "    if \"gpt2-medium\" in model_name:\n",
    "        return list(range(24))  # All 24 layers for fusion analysis\n",
    "    elif \"pythia-1.4b\" in model_name:\n",
    "        return list(range(24))\n",
    "    elif \"gemma-2b\" in model_name:\n",
    "        return list(range(18))\n",
    "    else:\n",
    "        return list(range(24))\n",
    "\n",
    "\n",
    "def load_model(model_path: str) -> Tuple[nn.Module, AutoTokenizer]:\n",
    "    \"\"\"Load model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path: HuggingFace model path\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_path}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    print(f\"  Loaded: {n_params:.1f}M parameters\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_activation(prompt: str, model: nn.Module, tokenizer: AutoTokenizer, \n",
    "                   layers: List[int]) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"Get last token activation at specified layers.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        layers: List of layer indices\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping layer index to activation vector\n",
    "    \"\"\"\n",
    "    assert prompt is not None and len(prompt) > 0, \"Prompt required\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    activations = {}\n",
    "    for layer_idx in layers:\n",
    "        h = outputs.hidden_states[layer_idx][0, -1, :].cpu().float().numpy()\n",
    "        activations[layer_idx] = h\n",
    "    \n",
    "    return activations\n",
    "\n",
    "\n",
    "def get_category_activations(prompts: List[str], model: nn.Module, \n",
    "                             tokenizer: AutoTokenizer, layers: List[int]) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"Get averaged activation for a category of prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompts\n",
    "        model: The model\n",
    "        tokenizer: The tokenizer\n",
    "        layers: List of layer indices\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping layer index to averaged activation vector\n",
    "    \"\"\"\n",
    "    all_activations = {layer: [] for layer in layers}\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        acts = get_activation(prompt, model, tokenizer, layers)\n",
    "        for layer in layers:\n",
    "            all_activations[layer].append(acts[layer])\n",
    "    \n",
    "    averaged = {}\n",
    "    for layer in layers:\n",
    "        averaged[layer] = np.mean(all_activations[layer], axis=0)\n",
    "    \n",
    "    return averaged\n",
    "\n",
    "\n",
    "print(\"Model loading functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decomposition Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_decomposition_score(bonded_act: np.ndarray, \n",
    "                                component_acts: List[np.ndarray],\n",
    "                                control_act: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute how well bonded state decomposes into components vs control.\n",
    "    \n",
    "    Args:\n",
    "        bonded_act: Activation of bonded state\n",
    "        component_acts: List of component activations\n",
    "        control_act: Control activation\n",
    "        \n",
    "    Returns:\n",
    "        Dict with decomposition metrics\n",
    "    \"\"\"\n",
    "    # Component similarities\n",
    "    component_sims = []\n",
    "    for comp_act in component_acts:\n",
    "        sim = cosine_similarity([bonded_act], [comp_act])[0, 0]\n",
    "        component_sims.append(sim)\n",
    "    \n",
    "    mean_component_sim = np.mean(component_sims)\n",
    "    \n",
    "    # Control similarity\n",
    "    control_sim = cosine_similarity([bonded_act], [control_act])[0, 0]\n",
    "    \n",
    "    # Ratio\n",
    "    ratio = mean_component_sim / control_sim if control_sim != 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        \"component_sims\": component_sims,\n",
    "        \"mean_component_sim\": mean_component_sim,\n",
    "        \"control_sim\": control_sim,\n",
    "        \"ratio\": ratio\n",
    "    }\n",
    "\n",
    "\n",
    "def permutation_test_ratio(component_sims: List[float], control_sims: List[float],\n",
    "                           n_permutations: int = 10000) -> Tuple[float, float]:\n",
    "    \"\"\"Permutation test for component vs control similarity.\n",
    "    \n",
    "    Args:\n",
    "        component_sims: List of component similarities\n",
    "        control_sims: List of control similarities\n",
    "        n_permutations: Number of permutations\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (observed_diff, p_value)\n",
    "    \"\"\"\n",
    "    observed_diff = np.mean(component_sims) - np.mean(control_sims)\n",
    "    \n",
    "    all_values = component_sims + control_sims\n",
    "    n_component = len(component_sims)\n",
    "    \n",
    "    count_extreme = 0\n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(all_values)\n",
    "        perm_component = all_values[:n_component]\n",
    "        perm_control = all_values[n_component:]\n",
    "        perm_diff = np.mean(perm_component) - np.mean(perm_control)\n",
    "        if perm_diff >= observed_diff:\n",
    "            count_extreme += 1\n",
    "    \n",
    "    p_value = (count_extreme + 1) / (n_permutations + 1)\n",
    "    \n",
    "    return observed_diff, p_value\n",
    "\n",
    "\n",
    "def analyze_layer_fusion(decomposition_scores_by_layer: Dict[int, float]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze if decomposition decreases with layer depth (fusion hypothesis).\n",
    "    \n",
    "    Args:\n",
    "        decomposition_scores_by_layer: Dict mapping layer to decomposition score\n",
    "        \n",
    "    Returns:\n",
    "        Dict with regression results\n",
    "    \"\"\"\n",
    "    layers = np.array(list(decomposition_scores_by_layer.keys())).reshape(-1, 1)\n",
    "    scores = np.array(list(decomposition_scores_by_layer.values()))\n",
    "    \n",
    "    # Linear regression\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(layers, scores)\n",
    "    \n",
    "    # Correlation\n",
    "    correlation, p_value = stats.pearsonr(layers.flatten(), scores)\n",
    "    \n",
    "    return {\n",
    "        \"slope\": float(reg.coef_[0]),\n",
    "        \"intercept\": float(reg.intercept_),\n",
    "        \"r_squared\": float(reg.score(layers, scores)),\n",
    "        \"correlation\": float(correlation),\n",
    "        \"p_value\": float(p_value),\n",
    "        \"fusion_supported\": reg.coef_[0] < 0 and p_value < 0.05\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Decomposition analysis functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_for_model(model_name: str, model_path: str,\n",
    "                             prompts: Dict, config: ExperimentConfig) -> Dict[str, Any]:\n",
    "    \"\"\"Run full experiment for a single model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model_path: HuggingFace path\n",
    "        prompts: Prompt dictionary\n",
    "        config: Experiment configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing all results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    try:\n",
    "        model, tokenizer = load_model(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {model_name}: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    layers = get_model_layers(model_path)\n",
    "    print(f\"Analyzing {len(layers)} layers\")\n",
    "    \n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"model_path\": model_path,\n",
    "        \"n_layers\": len(layers),\n",
    "        \"level_results\": {},\n",
    "        \"fusion_analysis\": {}\n",
    "    }\n",
    "    \n",
    "    # First, get activations for all single components\n",
    "    print(\"\\nGetting single AQ activations...\")\n",
    "    single_aq_acts = {}\n",
    "    for comp in AQ_COMPONENTS.keys():\n",
    "        comp_prompts = prompts[1][comp][\"bonded\"]\n",
    "        single_aq_acts[comp] = get_category_activations(comp_prompts, model, tokenizer, layers)\n",
    "        print(f\"  {comp}: done\")\n",
    "    \n",
    "    # Analyze each bond level\n",
    "    all_component_sims = []\n",
    "    all_control_sims = []\n",
    "    decomposition_by_layer = {l: [] for l in layers}\n",
    "    \n",
    "    for level in [2, 3, 4]:  # Skip level 1 (no composition)\n",
    "        print(f\"\\nAnalyzing Level {level} bonds...\")\n",
    "        results[\"level_results\"][level] = {}\n",
    "        \n",
    "        for combo_name, combo_data in prompts[level].items():\n",
    "            bonded_prompts = combo_data[\"bonded\"]\n",
    "            components = combo_data[\"components\"]\n",
    "            \n",
    "            # Get bonded activations\n",
    "            bonded_acts = get_category_activations(bonded_prompts, model, tokenizer, layers)\n",
    "            \n",
    "            # Generate and get control activations\n",
    "            # 1. Shuffled control\n",
    "            shuffled_prompts = [generate_shuffled_control(p) for p in bonded_prompts]\n",
    "            shuffled_acts = get_category_activations(shuffled_prompts, model, tokenizer, layers)\n",
    "            \n",
    "            # 2. Length-matched control\n",
    "            length_matched_prompts = [generate_length_matched_control(p, tokenizer) for p in bonded_prompts]\n",
    "            length_matched_acts = get_category_activations(length_matched_prompts, model, tokenizer, layers)\n",
    "            \n",
    "            # 3. Semantic-only control\n",
    "            semantic_prompt = generate_semantic_only_control(components)\n",
    "            semantic_acts = get_category_activations([semantic_prompt] * len(bonded_prompts), \n",
    "                                                     model, tokenizer, layers)\n",
    "            \n",
    "            # Analyze per layer\n",
    "            combo_results = {\"layers\": {}}\n",
    "            \n",
    "            for layer in layers:\n",
    "                bonded_act = bonded_acts[layer]\n",
    "                component_acts = [single_aq_acts[c][layer] for c in components]\n",
    "                \n",
    "                # Compute scores vs each control type\n",
    "                vs_shuffled = compute_decomposition_score(bonded_act, component_acts, shuffled_acts[layer])\n",
    "                vs_length = compute_decomposition_score(bonded_act, component_acts, length_matched_acts[layer])\n",
    "                vs_semantic = compute_decomposition_score(bonded_act, component_acts, semantic_acts[layer])\n",
    "                \n",
    "                combo_results[\"layers\"][layer] = {\n",
    "                    \"vs_shuffled\": vs_shuffled,\n",
    "                    \"vs_length_matched\": vs_length,\n",
    "                    \"vs_semantic_only\": vs_semantic\n",
    "                }\n",
    "                \n",
    "                # Collect for overall analysis\n",
    "                all_component_sims.extend(vs_shuffled[\"component_sims\"])\n",
    "                all_control_sims.append(vs_shuffled[\"control_sim\"])\n",
    "                decomposition_by_layer[layer].append(vs_shuffled[\"mean_component_sim\"])\n",
    "            \n",
    "            results[\"level_results\"][level][combo_name] = combo_results\n",
    "            print(f\"  {combo_name}: done\")\n",
    "    \n",
    "    # Permutation test for overall effect\n",
    "    print(\"\\nRunning permutation test...\")\n",
    "    observed_diff, p_value = permutation_test_ratio(\n",
    "        all_component_sims, all_control_sims, config.n_permutations\n",
    "    )\n",
    "    results[\"overall_test\"] = {\n",
    "        \"observed_diff\": float(observed_diff),\n",
    "        \"p_value\": float(p_value),\n",
    "        \"mean_component_sim\": float(np.mean(all_component_sims)),\n",
    "        \"mean_control_sim\": float(np.mean(all_control_sims)),\n",
    "        \"ratio\": float(np.mean(all_component_sims) / np.mean(all_control_sims))\n",
    "    }\n",
    "    print(f\"  Component vs Control: ratio={results['overall_test']['ratio']:.3f}, p={p_value:.6f}\")\n",
    "    \n",
    "    # Fusion analysis\n",
    "    print(\"\\nAnalyzing layer fusion hypothesis...\")\n",
    "    mean_decomp_by_layer = {l: np.mean(scores) for l, scores in decomposition_by_layer.items()}\n",
    "    fusion_results = analyze_layer_fusion(mean_decomp_by_layer)\n",
    "    results[\"fusion_analysis\"] = fusion_results\n",
    "    print(f\"  Slope: {fusion_results['slope']:.6f}\")\n",
    "    print(f\"  Correlation: {fusion_results['correlation']:.3f} (p={fusion_results['p_value']:.6f})\")\n",
    "    print(f\"  Fusion hypothesis supported: {fusion_results['fusion_supported']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Experiment runner ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment for all models\n",
    "ALL_RESULTS = {}\n",
    "\n",
    "for model_name, model_path in config.models.items():\n",
    "    results = run_experiment_for_model(model_name, model_path, PROMPTS, config)\n",
    "    ALL_RESULTS[model_name] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decomposition by layer for fusion analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Decomposition score by layer\n",
    "ax = axes[0]\n",
    "for model_name, results in ALL_RESULTS.items():\n",
    "    if \"error\" in results:\n",
    "        continue\n",
    "    \n",
    "    # Collect mean decomposition scores per layer\n",
    "    layers = []\n",
    "    scores = []\n",
    "    \n",
    "    for level in [2, 3, 4]:\n",
    "        for combo_name, combo_results in results[\"level_results\"][level].items():\n",
    "            for layer, layer_data in combo_results[\"layers\"].items():\n",
    "                if layer not in layers:\n",
    "                    layers.append(layer)\n",
    "    \n",
    "    layers = sorted(set(layers))\n",
    "    mean_scores = []\n",
    "    for layer in layers:\n",
    "        layer_scores = []\n",
    "        for level in [2, 3, 4]:\n",
    "            for combo_results in results[\"level_results\"][level].values():\n",
    "                layer_scores.append(combo_results[\"layers\"][layer][\"vs_shuffled\"][\"mean_component_sim\"])\n",
    "        mean_scores.append(np.mean(layer_scores))\n",
    "    \n",
    "    ax.plot(layers, mean_scores, 'o-', label=model_name, markersize=4)\n",
    "\n",
    "ax.set_xlabel(\"Layer\")\n",
    "ax.set_ylabel(\"Mean Component Similarity\")\n",
    "ax.set_title(\"Decomposition Score by Layer\\n(Higher = components more detectable)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Control comparison bar chart\n",
    "ax = axes[1]\n",
    "x_labels = []\n",
    "component_ratios = []\n",
    "colors = []\n",
    "\n",
    "for model_name, results in ALL_RESULTS.items():\n",
    "    if \"error\" in results:\n",
    "        continue\n",
    "    x_labels.append(model_name)\n",
    "    component_ratios.append(results[\"overall_test\"][\"ratio\"])\n",
    "\n",
    "x = np.arange(len(x_labels))\n",
    "ax.bar(x, component_ratios, color='steelblue', alpha=0.7)\n",
    "ax.axhline(y=1.0, color='r', linestyle='--', label='No difference (ratio=1)', alpha=0.7)\n",
    "ax.axhline(y=1.1, color='orange', linestyle='--', label='Target (ratio=1.1)', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Component/Control Ratio\")\n",
    "ax.set_title(\"Overall Component vs Control Ratio\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(\"035F: Compositional Controls Analysis\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"035F_compositional_controls.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 035F_compositional_controls.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 035F: COMPOSITIONAL CONTROLS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nExperiment Configuration:\")\n",
    "print(f\"  Models tested: {list(ALL_RESULTS.keys())}\")\n",
    "print(f\"  Bond levels: {config.bond_levels}\")\n",
    "print(f\"  Samples per level: {config.samples_per_level}\")\n",
    "print(f\"  Permutations: {config.n_permutations}\")\n",
    "\n",
    "print(f\"\\nControl Types:\")\n",
    "print(f\"  1. Shuffled: Same words, scrambled order\")\n",
    "print(f\"  2. Length-matched: Non-action prompts of same length\")\n",
    "print(f\"  3. Semantic-only: Action words without action context\")\n",
    "\n",
    "print(f\"\\nKey Results per Model:\")\n",
    "models_passing = 0\n",
    "fusion_supported_count = 0\n",
    "\n",
    "for model_name, results in ALL_RESULTS.items():\n",
    "    if \"error\" in results:\n",
    "        print(f\"  {model_name}: ERROR\")\n",
    "        continue\n",
    "    \n",
    "    overall = results[\"overall_test\"]\n",
    "    fusion = results[\"fusion_analysis\"]\n",
    "    \n",
    "    print(f\"\\n  {model_name}:\")\n",
    "    print(f\"    Component/Control ratio: {overall['ratio']:.3f}\")\n",
    "    print(f\"    Permutation test p-value: {overall['p_value']:.6f}\")\n",
    "    print(f\"    Fusion slope: {fusion['slope']:.6f}\")\n",
    "    print(f\"    Fusion supported: {fusion['fusion_supported']}\")\n",
    "    \n",
    "    if overall['ratio'] > 1.1 and overall['p_value'] < 0.01:\n",
    "        models_passing += 1\n",
    "        print(f\"    STATUS: PASS (ratio > 1.1, p < 0.01)\")\n",
    "    else:\n",
    "        print(f\"    STATUS: FAIL\")\n",
    "    \n",
    "    if fusion['fusion_supported']:\n",
    "        fusion_supported_count += 1\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"CONCLUSIONS:\")\n",
    "print(f\"  Models passing threshold: {models_passing}/{len([r for r in ALL_RESULTS.values() if 'error' not in r])}\")\n",
    "print(f\"  Fusion hypothesis supported in: {fusion_supported_count} models\")\n",
    "\n",
    "if models_passing >= len(ALL_RESULTS) - 1:  # Allow 1 failure\n",
    "    print(f\"\\n  CONCLUSION: Evidence SUPPORTS compositional bonding.\")\n",
    "    print(f\"  Bonded states contain component signatures beyond word co-occurrence.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: Evidence does NOT strongly support compositional bonding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "def make_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "results_output = {\n",
    "    \"config\": {\n",
    "        \"models\": list(config.models.keys()),\n",
    "        \"samples_per_level\": config.samples_per_level,\n",
    "        \"n_permutations\": config.n_permutations,\n",
    "        \"random_seed\": config.random_seed\n",
    "    },\n",
    "    \"results\": make_serializable(ALL_RESULTS),\n",
    "    \"summary\": {\n",
    "        \"models_passing\": models_passing,\n",
    "        \"fusion_supported_count\": fusion_supported_count,\n",
    "        \"conclusion\": \"SUPPORTS\" if models_passing >= len(ALL_RESULTS) - 1 else \"DOES NOT SUPPORT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"035F_results.json\", \"w\") as f:\n",
    "    json.dump(results_output, f, indent=2)\n",
    "\n",
    "print(\"Results saved to 035F_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
