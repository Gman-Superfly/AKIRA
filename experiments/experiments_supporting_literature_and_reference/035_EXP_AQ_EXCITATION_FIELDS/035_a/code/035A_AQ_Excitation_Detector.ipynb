{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 035A: AQ Excitation Pattern Detection\n",
    "\n",
    "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Experiment Tests\n",
    "\n",
    "Action Quanta (AQ) are hypothesized to be **quasiparticle field excitations** stored in LLM weights. They manifest when context resonates with the weight structure.\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "```\n",
    "WEIGHTS = Field (crystallized AQ structure)\n",
    "CONTEXT = Perturbation (selects resonance)\n",
    "ACTIVATIONS = Excitation patterns (observable AQ)\n",
    "```\n",
    "\n",
    "### Predictions\n",
    "\n",
    "If AQ theory is correct:\n",
    "- Same discrimination type should produce similar activation patterns\n",
    "- Different types should produce different patterns\n",
    "- Later layers should show cleaner separation (crystallization)\n",
    "\n",
    "### What We Measure\n",
    "\n",
    "1. **Silhouette Score**: Do same-category probes cluster together?\n",
    "2. **Distance Ratio**: Do different categories separate?\n",
    "3. **Layer Progression**: Does separation increase with depth?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "!pip install transformers torch numpy scikit-learn matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Choose your model and experiment parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for AQ Excitation Detection experiment.\n",
    "    \n",
    "    Attributes:\n",
    "        model_name: HuggingFace model identifier\n",
    "        layers_to_probe: Which layers to capture activations from\n",
    "        n_pca_components: Dimensions for PCA reduction\n",
    "        n_clusters: Number of clusters for K-means (matches probe categories)\n",
    "        random_seed: For reproducibility\n",
    "    \"\"\"\n",
    "    model_name: str = \"gpt2\"  # Options: \"gpt2\", \"EleutherAI/pythia-70m\", \"EleutherAI/pythia-160m\"\n",
    "    layers_to_probe: List[int] = field(default_factory=list)\n",
    "    n_pca_components: int = 20  # Must be <= number of probes (30)\n",
    "    n_clusters: int = 5  # Matches number of probe categories\n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Set layer indices based on model architecture.\"\"\"\n",
    "        if not self.layers_to_probe:\n",
    "            if \"gpt2\" in self.model_name.lower():\n",
    "                # GPT-2 has 12 layers (0-11)\n",
    "                self.layers_to_probe = [0, 3, 6, 9, 11]\n",
    "            elif \"pythia-70m\" in self.model_name.lower():\n",
    "                # Pythia-70M has 6 layers\n",
    "                self.layers_to_probe = [0, 1, 3, 5]\n",
    "            elif \"pythia-160m\" in self.model_name.lower():\n",
    "                # Pythia-160M has 12 layers\n",
    "                self.layers_to_probe = [0, 3, 6, 9, 11]\n",
    "            else:\n",
    "                # Default - assume 12 layers\n",
    "                self.layers_to_probe = [0, 3, 6, 9, 11]\n",
    "        \n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = ExperimentConfig()\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Layers to probe: {config.layers_to_probe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discrimination Probes\n",
    "\n",
    "These prompts test whether different **discrimination types** produce different activation patterns.\n",
    "\n",
    "- **Sentiment Positive**: Prompts expecting positive emotional continuation\n",
    "- **Sentiment Negative**: Prompts expecting negative emotional continuation\n",
    "- **Math**: Arithmetic problems\n",
    "- **Factual Geography**: Capital city questions\n",
    "- **Factual Science**: Scientific fact questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBES: Dict[str, List[str]] = {\n",
    "    'sentiment_positive': [\n",
    "        \"The movie was great, I felt\",\n",
    "        \"What a wonderful day, I am feeling\",\n",
    "        \"This food is delicious, it makes me\",\n",
    "        \"I love this song, it makes me feel\",\n",
    "        \"The vacation was amazing, I felt so\",\n",
    "        \"My friend is wonderful, they make me feel\",\n",
    "    ],\n",
    "    'sentiment_negative': [\n",
    "        \"The movie was terrible, I felt\",\n",
    "        \"What an awful day, I am feeling\",\n",
    "        \"This food is disgusting, it makes me\",\n",
    "        \"I hate this song, it makes me feel\",\n",
    "        \"The vacation was horrible, I felt so\",\n",
    "        \"My enemy is cruel, they make me feel\",\n",
    "    ],\n",
    "    'math': [\n",
    "        \"2 + 2 =\",\n",
    "        \"3 + 1 =\",\n",
    "        \"5 - 1 =\",\n",
    "        \"10 / 2 =\",\n",
    "        \"7 + 3 =\",\n",
    "        \"8 - 4 =\",\n",
    "    ],\n",
    "    'factual_geography': [\n",
    "        \"The capital of France is\",\n",
    "        \"The capital of Japan is\",\n",
    "        \"The capital of Germany is\",\n",
    "        \"The capital of Italy is\",\n",
    "        \"The capital of Spain is\",\n",
    "        \"The capital of Brazil is\",\n",
    "    ],\n",
    "    'factual_science': [\n",
    "        \"Water freezes at\",\n",
    "        \"The speed of light is\",\n",
    "        \"Gravity on Earth is\",\n",
    "        \"The chemical formula for water is\",\n",
    "        \"The boiling point of water is\",\n",
    "        \"The atomic number of carbon is\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Category colors for visualization\n",
    "CATEGORY_COLORS: Dict[str, str] = {\n",
    "    'sentiment_positive': '#2ecc71',  # green\n",
    "    'sentiment_negative': '#e74c3c',  # red\n",
    "    'math': '#3498db',                # blue\n",
    "    'factual_geography': '#9b59b6',   # purple\n",
    "    'factual_science': '#f39c12',     # orange\n",
    "}\n",
    "\n",
    "print(f\"Number of categories: {len(PROBES)}\")\n",
    "print(f\"Total probes: {sum(len(v) for v in PROBES.values())}\")\n",
    "for cat, prompts in PROBES.items():\n",
    "    print(f\"  {cat}: {len(prompts)} probes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Capture Class\n",
    "\n",
    "This class registers forward hooks on transformer layers to intercept activations during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCapture:\n",
    "    \"\"\"Captures activations from specified layers using forward hooks.\n",
    "    \n",
    "    This class registers hooks on transformer layers to intercept\n",
    "    intermediate activations during the forward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, layer_indices: List[int], model_type: str = \"gpt2\") -> None:\n",
    "        \"\"\"Initialize activation capture with hooks on specified layers.\"\"\"\n",
    "        assert len(layer_indices) > 0, \"Must specify at least one layer to probe\"\n",
    "        \n",
    "        self.activations: Dict[int, torch.Tensor] = {}\n",
    "        self.hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "        self.layer_indices = layer_indices\n",
    "        \n",
    "        # Get the transformer blocks based on model architecture\n",
    "        if hasattr(model, 'transformer'):\n",
    "            # GPT-2 style\n",
    "            layers = model.transformer.h\n",
    "        elif hasattr(model, 'gpt_neox'):\n",
    "            # Pythia style\n",
    "            layers = model.gpt_neox.layers\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model architecture: {type(model)}\")\n",
    "        \n",
    "        assert len(layers) > max(layer_indices), \\\n",
    "            f\"Model has {len(layers)} layers but requested layer {max(layer_indices)}\"\n",
    "        \n",
    "        # Register hooks on each specified layer\n",
    "        for idx in layer_indices:\n",
    "            layer = layers[idx]\n",
    "            hook = layer.register_forward_hook(self._make_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "        \n",
    "        print(f\"Registered hooks on layers: {layer_indices}\")\n",
    "    \n",
    "    def _make_hook(self, layer_idx: int):\n",
    "        \"\"\"Create a hook function for a specific layer.\"\"\"\n",
    "        def hook(module: nn.Module, input: Tuple, output: Tuple) -> None:\n",
    "            if isinstance(output, tuple):\n",
    "                hidden_states = output[0]\n",
    "            else:\n",
    "                hidden_states = output\n",
    "            self.activations[layer_idx] = hidden_states.detach()\n",
    "        return hook\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = {}\n",
    "    \n",
    "    def remove_hooks(self) -> None:\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"Removed all hooks\")\n",
    "    \n",
    "    def get_last_token_activation(self, layer_idx: int) -> np.ndarray:\n",
    "        \"\"\"Get activation for the last token position at a specific layer.\"\"\"\n",
    "        assert layer_idx in self.activations, f\"Layer {layer_idx} not captured\"\n",
    "        act = self.activations[layer_idx]\n",
    "        assert act is not None and act.numel() > 0, \"Empty activation\"\n",
    "        \n",
    "        # Get last token: [batch=0, seq=-1, hidden_dim]\n",
    "        last_token_act = act[0, -1, :].cpu().numpy()\n",
    "        return last_token_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_category_distances(\n",
    "    activations: np.ndarray,\n",
    "    labels: List[str]\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"Compute within-category and between-category distances.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (within_distance, between_distance, ratio)\n",
    "        ratio > 1 means categories are separating well\n",
    "    \"\"\"\n",
    "    assert len(activations) == len(labels), \"Activation/label count mismatch\"\n",
    "    \n",
    "    within_distances = []\n",
    "    between_distances = []\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    distances = pairwise_distances(activations, metric='euclidean')\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] == labels[j]:\n",
    "                within_distances.append(distances[i, j])\n",
    "            else:\n",
    "                between_distances.append(distances[i, j])\n",
    "    \n",
    "    within_mean = np.mean(within_distances) if within_distances else 0\n",
    "    between_mean = np.mean(between_distances) if between_distances else 0\n",
    "    \n",
    "    ratio = between_mean / within_mean if within_mean > 0 else float('inf')\n",
    "    \n",
    "    return within_mean, between_mean, ratio\n",
    "\n",
    "\n",
    "def compute_silhouette(\n",
    "    activations: np.ndarray,\n",
    "    labels: List[str]\n",
    ") -> float:\n",
    "    \"\"\"Compute silhouette score for clustering quality.\n",
    "    \n",
    "    Silhouette score ranges from -1 to 1:\n",
    "    - 1: Perfect clustering\n",
    "    - 0: Overlapping clusters\n",
    "    - -1: Wrong clustering\n",
    "    \"\"\"\n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "    int_labels = [label_to_int[label] for label in labels]\n",
    "    \n",
    "    if len(set(int_labels)) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    return silhouette_score(activations, int_labels)\n",
    "\n",
    "\n",
    "def run_pca_analysis(\n",
    "    activations: np.ndarray,\n",
    "    labels: List[str],\n",
    "    n_components: int = 2,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[np.ndarray, PCA]:\n",
    "    \"\"\"Apply PCA to reduce activation dimensionality.\"\"\"\n",
    "    assert activations.shape[0] == len(labels), \"Sample count mismatch\"\n",
    "    assert n_components <= activations.shape[1], \"Too many components requested\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    activations_scaled = scaler.fit_transform(activations)\n",
    "    \n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced = pca.fit_transform(activations_scaled)\n",
    "    \n",
    "    if verbose:\n",
    "        explained_var = sum(pca.explained_variance_ratio_) * 100\n",
    "        print(f\"PCA: {n_components} components explain {explained_var:.1f}% variance\")\n",
    "    \n",
    "    return reduced, pca\n",
    "\n",
    "\n",
    "def run_kmeans_clustering(\n",
    "    activations: np.ndarray,\n",
    "    n_clusters: int\n",
    ") -> Tuple[np.ndarray, KMeans]:\n",
    "    \"\"\"Apply K-means clustering to activations.\"\"\"\n",
    "    assert n_clusters > 0, \"Need positive number of clusters\"\n",
    "    assert activations.shape[0] >= n_clusters, \"More clusters than samples\"\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(activations)\n",
    "    \n",
    "    return cluster_labels, kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_scatter(\n",
    "    activations_2d: np.ndarray,\n",
    "    labels: List[str],\n",
    "    layer_idx: int,\n",
    "    texts: List[str]\n",
    ") -> None:\n",
    "    \"\"\"Create 2D scatter plot of activations colored by category.\"\"\"\n",
    "    assert activations_2d.shape[1] == 2, \"Need 2D activations for scatter\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for category in CATEGORY_COLORS:\n",
    "        mask = [l == category for l in labels]\n",
    "        if any(mask):\n",
    "            indices = [i for i, m in enumerate(mask) if m]\n",
    "            plt.scatter(\n",
    "                activations_2d[indices, 0],\n",
    "                activations_2d[indices, 1],\n",
    "                c=CATEGORY_COLORS[category],\n",
    "                label=category.replace('_', ' '),\n",
    "                alpha=0.7,\n",
    "                s=100,\n",
    "                edgecolors='white',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('PC1', fontsize=12)\n",
    "    plt.ylabel('PC2', fontsize=12)\n",
    "    plt.title(f'AQ Excitation Patterns - Layer {layer_idx}', fontsize=14)\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_similarity_matrix(\n",
    "    activations: np.ndarray,\n",
    "    labels: List[str],\n",
    "    layer_idx: int\n",
    ") -> None:\n",
    "    \"\"\"Create heatmap of pairwise activation similarities.\"\"\"\n",
    "    # Compute cosine similarity\n",
    "    norms = np.linalg.norm(activations, axis=1, keepdims=True)\n",
    "    normalized = activations / (norms + 1e-8)\n",
    "    similarity = normalized @ normalized.T\n",
    "    \n",
    "    # Sort by category for cleaner visualization\n",
    "    sorted_indices = sorted(range(len(labels)), key=lambda i: labels[i])\n",
    "    similarity_sorted = similarity[sorted_indices][:, sorted_indices]\n",
    "    labels_sorted = [labels[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        similarity_sorted,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Cosine Similarity'}\n",
    "    )\n",
    "    \n",
    "    # Add category boundaries\n",
    "    unique_labels = []\n",
    "    boundaries = [0]\n",
    "    for i, label in enumerate(labels_sorted):\n",
    "        if label not in unique_labels:\n",
    "            unique_labels.append(label)\n",
    "            if i > 0:\n",
    "                boundaries.append(i)\n",
    "    boundaries.append(len(labels_sorted))\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        plt.axhline(y=b, color='black', linewidth=2)\n",
    "        plt.axvline(x=b, color='black', linewidth=2)\n",
    "    \n",
    "    plt.title(f'Activation Similarity Matrix - Layer {layer_idx}', fontsize=14)\n",
    "    plt.xlabel('Probe Index (sorted by category)')\n",
    "    plt.ylabel('Probe Index (sorted by category)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_layer_comparison(\n",
    "    metrics_by_layer: Dict[int, Dict[str, float]]\n",
    ") -> None:\n",
    "    \"\"\"Plot how metrics change across layers.\"\"\"\n",
    "    layers = sorted(metrics_by_layer.keys())\n",
    "    \n",
    "    silhouettes = [metrics_by_layer[l]['silhouette'] for l in layers]\n",
    "    ratios = [metrics_by_layer[l]['distance_ratio'] for l in layers]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Silhouette score by layer\n",
    "    axes[0].plot(layers, silhouettes, 'o-', color='#3498db', linewidth=2, markersize=10)\n",
    "    axes[0].set_xlabel('Layer Index', fontsize=12)\n",
    "    axes[0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "    axes[0].set_title('Clustering Quality by Layer', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(-0.2, 1.0)\n",
    "    \n",
    "    # Distance ratio by layer\n",
    "    axes[1].plot(layers, ratios, 'o-', color='#e74c3c', linewidth=2, markersize=10)\n",
    "    axes[1].set_xlabel('Layer Index', fontsize=12)\n",
    "    axes[1].set_ylabel('Between/Within Distance Ratio', fontsize=12)\n",
    "    axes[1].set_title('Category Separation by Layer', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Model\n",
    "\n",
    "Load the pretrained model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Determine model type for hook placement\n",
    "if hasattr(model, 'transformer'):\n",
    "    model_type = \"gpt2\"\n",
    "elif hasattr(model, 'gpt_neox'):\n",
    "    model_type = \"pythia\"\n",
    "else:\n",
    "    model_type = \"unknown\"\n",
    "\n",
    "print(f\"Model loaded on {DEVICE} (type: {model_type})\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Probes and Capture Activations\n",
    "\n",
    "Run all discrimination probes through the model and capture activations at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up activation capture\n",
    "capture = ActivationCapture(model, config.layers_to_probe, model_type)\n",
    "\n",
    "# Storage for results\n",
    "all_activations: Dict[int, List[np.ndarray]] = {\n",
    "    layer: [] for layer in config.layers_to_probe\n",
    "}\n",
    "all_labels: List[str] = []\n",
    "all_texts: List[str] = []\n",
    "\n",
    "print(\"Running discrimination probes...\")\n",
    "\n",
    "for category, prompts in PROBES.items():\n",
    "    for prompt in prompts:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        # Forward pass (captures activations via hooks)\n",
    "        capture.clear()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Store activations for last token position\n",
    "        for layer_idx in config.layers_to_probe:\n",
    "            act = capture.get_last_token_activation(layer_idx)\n",
    "            all_activations[layer_idx].append(act)\n",
    "        \n",
    "        all_labels.append(category)\n",
    "        all_texts.append(prompt)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    all_activations[layer_idx] = np.array(all_activations[layer_idx])\n",
    "\n",
    "print(f\"Collected activations for {len(all_labels)} probes\")\n",
    "print(f\"Activation shape per layer: {all_activations[config.layers_to_probe[0]].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Layer-by-Layer Analysis\n",
    "\n",
    "Compute metrics for each layer to see how AQ patterns develop through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LAYER-BY-LAYER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metrics_by_layer: Dict[int, Dict[str, float]] = {}\n",
    "pca_results_by_layer: Dict[int, np.ndarray] = {}\n",
    "\n",
    "for layer_idx in config.layers_to_probe:\n",
    "    print(f\"\\n--- Layer {layer_idx} ---\")\n",
    "    \n",
    "    activations = all_activations[layer_idx]\n",
    "    \n",
    "    # Compute distance metrics\n",
    "    within_dist, between_dist, ratio = compute_category_distances(\n",
    "        activations, all_labels\n",
    "    )\n",
    "    print(f\"Within-category distance:  {within_dist:.4f}\")\n",
    "    print(f\"Between-category distance: {between_dist:.4f}\")\n",
    "    print(f\"Distance ratio (higher = better separation): {ratio:.4f}\")\n",
    "    \n",
    "    # Compute silhouette score\n",
    "    silhouette = compute_silhouette(activations, all_labels)\n",
    "    print(f\"Silhouette score: {silhouette:.4f}\")\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics_by_layer[layer_idx] = {\n",
    "        'within_distance': within_dist,\n",
    "        'between_distance': between_dist,\n",
    "        'distance_ratio': ratio,\n",
    "        'silhouette': silhouette\n",
    "    }\n",
    "    \n",
    "    # PCA reduction for visualization\n",
    "    pca_2d, _ = run_pca_analysis(activations, all_labels, n_components=2)\n",
    "    pca_results_by_layer[layer_idx] = pca_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. K-Means Clustering Analysis\n",
    "\n",
    "Test whether unsupervised clustering recovers the probe categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use the final layer for clustering analysis\n",
    "final_layer = config.layers_to_probe[-1]\n",
    "final_activations = all_activations[final_layer]\n",
    "\n",
    "# Apply PCA first (for better clustering)\n",
    "pca_for_clustering, _ = run_pca_analysis(\n",
    "    final_activations, all_labels, n_components=config.n_pca_components\n",
    ")\n",
    "\n",
    "# K-means clustering\n",
    "cluster_labels, kmeans = run_kmeans_clustering(\n",
    "    pca_for_clustering, config.n_clusters\n",
    ")\n",
    "\n",
    "# Compare clusters to true categories\n",
    "print(f\"\\nK-means with k={config.n_clusters} clusters on Layer {final_layer}\")\n",
    "print(\"\\nCluster composition:\")\n",
    "\n",
    "for cluster_id in range(config.n_clusters):\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_categories = [all_labels[i] for i in range(len(all_labels)) if cluster_mask[i]]\n",
    "    category_counts = {}\n",
    "    for cat in cluster_categories:\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"  Cluster {cluster_id}: {category_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations\n",
    "\n",
    "### 11.1 Scatter Plots by Layer\n",
    "\n",
    "Visualize how activation patterns cluster in 2D PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in config.layers_to_probe:\n",
    "    print(f\"\\nScatter plot for Layer {layer_idx}:\")\n",
    "    plot_activation_scatter(\n",
    "        pca_results_by_layer[layer_idx],\n",
    "        all_labels,\n",
    "        layer_idx,\n",
    "        all_texts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Similarity Matrix\n",
    "\n",
    "Visualize pairwise cosine similarity between all probes. If AQ theory is correct, we should see block-diagonal structure where same-category probes are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSimilarity matrix for Layer {final_layer}:\")\n",
    "plot_similarity_matrix(\n",
    "    all_activations[final_layer],\n",
    "    all_labels,\n",
    "    final_layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Layer Comparison\n",
    "\n",
    "Do later layers show cleaner separation? This tests the \"crystallization\" hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLayer-wise metric comparison:\")\n",
    "plot_layer_comparison(metrics_by_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Assessment\n",
    "\n",
    "Evaluate the evidence for AQ excitation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check AQ theory predictions\n",
    "final_silhouette = metrics_by_layer[final_layer]['silhouette']\n",
    "final_ratio = metrics_by_layer[final_layer]['distance_ratio']\n",
    "\n",
    "# Does separation increase with depth?\n",
    "silhouettes = [metrics_by_layer[l]['silhouette'] for l in config.layers_to_probe]\n",
    "ratios = [metrics_by_layer[l]['distance_ratio'] for l in config.layers_to_probe]\n",
    "\n",
    "silhouette_increases = silhouettes[-1] > silhouettes[0]\n",
    "ratio_increases = ratios[-1] > ratios[0]\n",
    "\n",
    "print(\"\\nAQ THEORY PREDICTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n1. Do same-type probes cluster together?\")\n",
    "if final_silhouette > 0.1:\n",
    "    print(f\"   YES - Silhouette score {final_silhouette:.3f} > 0.1\")\n",
    "else:\n",
    "    print(f\"   UNCLEAR - Silhouette score {final_silhouette:.3f} is low\")\n",
    "\n",
    "print(f\"\\n2. Do different types separate?\")\n",
    "if final_ratio > 1.2:\n",
    "    print(f\"   YES - Distance ratio {final_ratio:.3f} > 1.2\")\n",
    "else:\n",
    "    print(f\"   UNCLEAR - Distance ratio {final_ratio:.3f} is low\")\n",
    "\n",
    "print(f\"\\n3. Does separation increase in later layers (crystallization)?\")\n",
    "if silhouette_increases and ratio_increases:\n",
    "    print(f\"   YES - Both silhouette and ratio increase with depth\")\n",
    "    print(f\"   Layer 0 -> Layer {final_layer}:\")\n",
    "    print(f\"     Silhouette: {silhouettes[0]:.3f} -> {silhouettes[-1]:.3f}\")\n",
    "    print(f\"     Ratio: {ratios[0]:.3f} -> {ratios[-1]:.3f}\")\n",
    "elif silhouette_increases or ratio_increases:\n",
    "    print(f\"   PARTIAL - One metric increases, one doesn't\")\n",
    "else:\n",
    "    print(f\"   NO - Metrics don't consistently increase with depth\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"OVERALL ASSESSMENT:\")\n",
    "\n",
    "evidence_score = 0\n",
    "if final_silhouette > 0.1:\n",
    "    evidence_score += 1\n",
    "if final_ratio > 1.2:\n",
    "    evidence_score += 1\n",
    "if silhouette_increases:\n",
    "    evidence_score += 1\n",
    "if ratio_increases:\n",
    "    evidence_score += 1\n",
    "\n",
    "if evidence_score >= 3:\n",
    "    print(\"STRONG EVIDENCE for AQ excitation patterns\")\n",
    "elif evidence_score >= 2:\n",
    "    print(\"MODERATE EVIDENCE for AQ excitation patterns\")\n",
    "elif evidence_score >= 1:\n",
    "    print(\"WEAK EVIDENCE for AQ excitation patterns\")\n",
    "else:\n",
    "    print(\"NO CLEAR EVIDENCE for AQ excitation patterns\")\n",
    "\n",
    "print(f\"(Evidence score: {evidence_score}/4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL METRICS BY LAYER:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Layer':>6} | {'Silhouette':>12} | {'Distance Ratio':>15} | {'Within Dist':>12} | {'Between Dist':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for layer_idx, metrics in metrics_by_layer.items():\n",
    "    print(f\"{layer_idx:>6} | {metrics['silhouette']:>12.3f} | {metrics['distance_ratio']:>15.3f} | {metrics['within_distance']:>12.3f} | {metrics['between_distance']:>12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hooks\n",
    "capture.remove_hooks()\n",
    "\n",
    "# Store results for further analysis\n",
    "results = {\n",
    "    'config': config,\n",
    "    'activations': all_activations,\n",
    "    'labels': all_labels,\n",
    "    'texts': all_texts,\n",
    "    'metrics_by_layer': metrics_by_layer,\n",
    "    'pca_results': pca_results_by_layer,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'evidence_score': evidence_score\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpretation Guide\n",
    "\n",
    "### What the Results Mean\n",
    "\n",
    "**If evidence score is 3-4/4:**\n",
    "- Strong support for AQ theory\n",
    "- Activations show stable patterns corresponding to discrimination types\n",
    "- These patterns crystallize (sharpen) in later layers\n",
    "- This is consistent with AQ as quasiparticle excitations in the weight field\n",
    "\n",
    "**If evidence score is 1-2/4:**\n",
    "- Partial support for AQ theory\n",
    "- Some structure exists but is not fully consistent\n",
    "- May need more probes or different model\n",
    "\n",
    "**If evidence score is 0/4:**\n",
    "- No clear support for AQ theory\n",
    "- Activations don't cluster by discrimination type\n",
    "- Either AQ theory needs revision or this experiment doesn't capture the right patterns\n",
    "\n",
    "### What to Look For in Visualizations\n",
    "\n",
    "1. **Scatter plots**: Look for distinct color clusters. Overlap means poor separation.\n",
    "2. **Similarity matrix**: Look for bright blocks along diagonal (same category = high similarity).\n",
    "3. **Layer comparison**: Upward trend = crystallization hypothesis supported.\n",
    "\n",
    "---\n",
    "\n",
    "**AKIRA Project - Experiment 035**  \n",
    "Oscar Goldman - Shogu Research Group @ Datamutant.ai"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
