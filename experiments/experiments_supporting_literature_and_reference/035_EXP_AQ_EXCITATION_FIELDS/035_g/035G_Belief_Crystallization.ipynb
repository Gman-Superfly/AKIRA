{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 035G: Belief Crystallization Strength\n",
        "\n",
        "**AKIRA Project - Oscar Goldman - Shogu Research Group @ Datamutant.ai**\n",
        "\n",
        "---\n",
        "\n",
        "## Core Insight\n",
        "\n",
        "AQ are **crystallized beliefs from data patterns**. They emerge when sufficient\n",
        "consistent evidence accumulates:\n",
        "\n",
        "```\n",
        "3 cups sugar + 1/2 gallon milk + 2 eggs = CAKE\n",
        "late night + noise closer + 8 people approaching = THREAT\n",
        "cold + looks like rain = GO_HOME\n",
        "```\n",
        "\n",
        "The key is: **if X is X and X is doing X then = X**\n",
        "\n",
        "When patterns are:\n",
        "- **Complete and consistent** -> Strong crystallization, clear AQ\n",
        "- **Incomplete** -> Weak crystallization, uncertain AQ\n",
        "- **Contradictory** -> No crystallization, interference/hallucination\n",
        "\n",
        "---\n",
        "\n",
        "## The Question\n",
        "\n",
        "**How do I X if X isn't X?**\n",
        "\n",
        "When the pattern is ambiguous or contradictory, the belief cannot crystallize.\n",
        "The model should show:\n",
        "- Phase spreading (not tight clustering)\n",
        "- Lower confidence\n",
        "- Higher activation variance\n",
        "- Potential hallucination\n",
        "\n",
        "---\n",
        "\n",
        "## Experimental Design\n",
        "\n",
        "Test belief crystallization with varying **pattern completeness**:\n",
        "\n",
        "| Level | Pattern State | Example | Expected |\n",
        "|:------|:--------------|:--------|:---------|\n",
        "| 5 | Complete + Consistent | All components present, all point same way | Strong crystallization |\n",
        "| 4 | Mostly complete | Missing 1 component | Good crystallization |\n",
        "| 3 | Partial | Missing 2+ components | Weak crystallization |\n",
        "| 2 | Ambiguous | Components present but conflicting | Interference |\n",
        "| 1 | Contradictory | Direct contradictions | No crystallization |\n",
        "| 0 | Impossible | Logical impossibility | Failure/hallucination |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch numpy matplotlib seaborn scipy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from scipy import stats\n",
        "from scipy.stats import circvar, circmean\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Belief Pattern Definitions\n",
        "\n",
        "Define patterns that should crystallize into clear beliefs vs patterns that\n",
        "should create interference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Belief domains - each has components that together crystallize a belief\n",
        "\n",
        "BELIEF_PATTERNS = {\n",
        "    \"THREAT_FLEE\": {\n",
        "        \"description\": \"Threat situation requiring escape\",\n",
        "        \"expected_belief\": \"FLEE\",\n",
        "        \"components\": {\n",
        "            \"danger_source\": [\"predator\", \"attacker\", \"fire\", \"flood\", \"armed person\", \"collapsing structure\"],\n",
        "            \"proximity\": [\"approaching\", \"nearby\", \"close\", \"getting closer\", \"meters away\"],\n",
        "            \"urgency\": [\"now\", \"immediately\", \"quickly\", \"right now\", \"urgent\"],\n",
        "            \"direction\": [\"coming toward you\", \"heading your way\", \"approaching from behind\"],\n",
        "            \"capability\": [\"you can run\", \"exit available\", \"path clear\", \"you are mobile\"]\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"RECIPE_CAKE\": {\n",
        "        \"description\": \"Recipe pattern for making cake\",\n",
        "        \"expected_belief\": \"MAKE_CAKE\",\n",
        "        \"components\": {\n",
        "            \"flour\": [\"2 cups flour\", \"flour sifted\", \"all-purpose flour\"],\n",
        "            \"sugar\": [\"1 cup sugar\", \"granulated sugar\", \"sugar measured\"],\n",
        "            \"eggs\": [\"2 eggs\", \"eggs beaten\", \"large eggs\"],\n",
        "            \"butter\": [\"half cup butter\", \"butter softened\", \"melted butter\"],\n",
        "            \"method\": [\"mix together\", \"combine ingredients\", \"bake at 350\"]\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"WEATHER_SHELTER\": {\n",
        "        \"description\": \"Weather pattern requiring shelter\",\n",
        "        \"expected_belief\": \"GO_INSIDE\",\n",
        "        \"components\": {\n",
        "            \"temperature\": [\"cold\", \"freezing\", \"temperature dropping\"],\n",
        "            \"precipitation\": [\"rain coming\", \"storm approaching\", \"dark clouds\"],\n",
        "            \"wind\": [\"strong wind\", \"gusty\", \"wind picking up\"],\n",
        "            \"time\": [\"getting dark\", \"evening\", \"late\"],\n",
        "            \"location\": [\"outside\", \"in the open\", \"exposed\"]\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"TRUST_PERSON\": {\n",
        "        \"description\": \"Pattern indicating trustworthy person\",\n",
        "        \"expected_belief\": \"TRUST\",\n",
        "        \"components\": {\n",
        "            \"history\": [\"known for years\", \"long friendship\", \"proven reliable\"],\n",
        "            \"behavior\": [\"always honest\", \"keeps promises\", \"never lied\"],\n",
        "            \"reputation\": [\"respected by others\", \"good reputation\", \"trusted by community\"],\n",
        "            \"alignment\": [\"shares your values\", \"similar goals\", \"mutual benefit\"],\n",
        "            \"vulnerability\": [\"has trusted you\", \"shared secrets\", \"been vulnerable\"]\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"WEALTH_PATH\": {\n",
        "        \"description\": \"Pattern for achieving wealth - inherently ambiguous\",\n",
        "        \"expected_belief\": \"UNCERTAIN\",\n",
        "        \"components\": {\n",
        "            \"education\": [\"studied hard\", \"got degree\", \"learned skills\"],\n",
        "            \"work\": [\"work hard\", \"put in hours\", \"dedicated effort\"],\n",
        "            \"saving\": [\"save money\", \"invest wisely\", \"budget carefully\"],\n",
        "            \"opportunity\": [\"right place\", \"good timing\", \"lucky break\"],\n",
        "            \"connections\": [\"know right people\", \"network\", \"mentors\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"Defined {len(BELIEF_PATTERNS)} belief patterns\")\n",
        "for name, pattern in BELIEF_PATTERNS.items():\n",
        "    print(f\"  {name}: {len(pattern['components'])} components -> {pattern['expected_belief']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prompt Generation by Crystallization Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_crystallization_prompts(pattern_name: str, n_per_level: int = 50) -> Dict[int, List[str]]:\n",
        "    \"\"\"Generate prompts at different crystallization levels for a belief pattern.\n",
        "    \n",
        "    Level 5: All components present, consistent\n",
        "    Level 4: 4 of 5 components\n",
        "    Level 3: 3 of 5 components  \n",
        "    Level 2: Components present but with ambiguity markers\n",
        "    Level 1: Components with contradictions\n",
        "    Level 0: Logical impossibility\n",
        "    \n",
        "    Args:\n",
        "        pattern_name: Name of the belief pattern\n",
        "        n_per_level: Number of prompts per crystallization level\n",
        "        \n",
        "    Returns:\n",
        "        Dict mapping level to list of prompts\n",
        "    \"\"\"\n",
        "    assert pattern_name in BELIEF_PATTERNS, f\"Unknown pattern: {pattern_name}\"\n",
        "    \n",
        "    pattern = BELIEF_PATTERNS[pattern_name]\n",
        "    components = pattern[\"components\"]\n",
        "    component_names = list(components.keys())\n",
        "    \n",
        "    prompts = {level: [] for level in range(6)}\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    for i in range(n_per_level):\n",
        "        # Level 5: All components\n",
        "        parts = []\n",
        "        for comp_name in component_names:\n",
        "            options = components[comp_name]\n",
        "            parts.append(options[i % len(options)])\n",
        "        prompt_5 = \"Situation: \" + \", \".join(parts) + \". What should you do?\"\n",
        "        prompts[5].append(prompt_5)\n",
        "        \n",
        "        # Level 4: 4 of 5 components\n",
        "        subset = np.random.choice(component_names, 4, replace=False)\n",
        "        parts = [components[c][i % len(components[c])] for c in subset]\n",
        "        prompt_4 = \"Situation: \" + \", \".join(parts) + \". What should you do?\"\n",
        "        prompts[4].append(prompt_4)\n",
        "        \n",
        "        # Level 3: 3 of 5 components\n",
        "        subset = np.random.choice(component_names, 3, replace=False)\n",
        "        parts = [components[c][i % len(components[c])] for c in subset]\n",
        "        prompt_3 = \"Situation: \" + \", \".join(parts) + \". What should you do?\"\n",
        "        prompts[3].append(prompt_3)\n",
        "        \n",
        "        # Level 2: All components but with ambiguity markers\n",
        "        ambiguity_markers = [\"maybe\", \"possibly\", \"not sure if\", \"might be\", \"could be\"]\n",
        "        parts = []\n",
        "        for j, comp_name in enumerate(component_names):\n",
        "            options = components[comp_name]\n",
        "            marker = ambiguity_markers[j % len(ambiguity_markers)]\n",
        "            parts.append(f\"{marker} {options[i % len(options)]}\")\n",
        "        prompt_2 = \"Situation: \" + \", \".join(parts) + \". What should you do?\"\n",
        "        prompts[2].append(prompt_2)\n",
        "        \n",
        "        # Level 1: Components with contradictions\n",
        "        parts = []\n",
        "        contradictions = {\n",
        "            \"THREAT_FLEE\": [\"but also safe\", \"but friendly\", \"but not dangerous\", \"but harmless\"],\n",
        "            \"RECIPE_CAKE\": [\"but no oven\", \"but making soup\", \"but without heat\", \"but raw\"],\n",
        "            \"WEATHER_SHELTER\": [\"but sunny\", \"but warm\", \"but clear skies\", \"but pleasant\"],\n",
        "            \"TRUST_PERSON\": [\"but lied before\", \"but unreliable\", \"but suspicious\", \"but secretive\"],\n",
        "            \"WEALTH_PATH\": [\"but economy crashed\", \"but no jobs\", \"but skills outdated\", \"but unlucky\"]\n",
        "        }\n",
        "        for j, comp_name in enumerate(component_names[:3]):\n",
        "            options = components[comp_name]\n",
        "            parts.append(options[i % len(options)])\n",
        "        contras = contradictions.get(pattern_name, [\"but opposite\", \"but not really\"])\n",
        "        parts.append(contras[i % len(contras)])\n",
        "        prompt_1 = \"Situation: \" + \", \".join(parts) + \". What should you do?\"\n",
        "        prompts[1].append(prompt_1)\n",
        "        \n",
        "        # Level 0: Logical impossibility\n",
        "        impossibilities = {\n",
        "            \"THREAT_FLEE\": [\n",
        "                \"A friendly predator is safely attacking you with harmless danger. What should you do?\",\n",
        "                \"The approaching threat has already passed before it arrives. What should you do?\",\n",
        "                \"You must flee from safety toward the danger to escape. What should you do?\"\n",
        "            ],\n",
        "            \"RECIPE_CAKE\": [\n",
        "                \"To make this cake, first remove the ingredients after baking. What should you do?\",\n",
        "                \"Mix the unbaked batter that was already eaten. What should you do?\",\n",
        "                \"The cake is done before you start making it. What should you do?\"\n",
        "            ],\n",
        "            \"WEATHER_SHELTER\": [\n",
        "                \"The dry rain is making the sunny storm wet with warmth. What should you do?\",\n",
        "                \"Go inside the outside where the indoor rain is dry. What should you do?\",\n",
        "                \"The freezing heat is warmly cold. What should you do?\"\n",
        "            ],\n",
        "            \"TRUST_PERSON\": [\n",
        "                \"This honest liar always tells truthful lies. What should you do?\",\n",
        "                \"Trust the untrustworthy person who reliably betrays. What should you do?\",\n",
        "                \"Their proven dishonesty demonstrates integrity. What should you do?\"\n",
        "            ],\n",
        "            \"WEALTH_PATH\": [\n",
        "                \"To become wealthy, first spend all your earnings on poverty. What should you do?\",\n",
        "                \"Achieve success by failing at everything successfully. What should you do?\",\n",
        "                \"The guaranteed path to wealth requires losing all money. What should you do?\"\n",
        "            ]\n",
        "        }\n",
        "        impossible_list = impossibilities.get(pattern_name, [\"Impossible situation. What should you do?\"])\n",
        "        prompts[0].append(impossible_list[i % len(impossible_list)])\n",
        "    \n",
        "    return prompts\n",
        "\n",
        "\n",
        "# Test prompt generation\n",
        "test_prompts = generate_crystallization_prompts(\"THREAT_FLEE\", n_per_level=3)\n",
        "print(\"\\n=== THREAT_FLEE Examples ===\")\n",
        "for level in range(5, -1, -1):\n",
        "    print(f\"\\nLevel {level}:\")\n",
        "    print(f\"  {test_prompts[level][0][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Loading and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Configuration for belief crystallization experiment.\"\"\"\n",
        "    \n",
        "    models: Dict[str, str] = field(default_factory=lambda: {\n",
        "        \"gpt2-medium\": \"gpt2-medium\",\n",
        "        \"gpt2-large\": \"gpt2-large\",\n",
        "    })\n",
        "    \n",
        "    prompts_per_level: int = 100\n",
        "    max_new_tokens: int = 50\n",
        "    random_seed: int = 42\n",
        "    \n",
        "    # Layers to analyze (skip layer 0 - it's just embeddings)\n",
        "    layer_ratios: List[float] = field(default_factory=lambda: [0.15, 0.3, 0.5, 0.7, 0.85, 1.0])\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        np.random.seed(self.random_seed)\n",
        "        torch.manual_seed(self.random_seed)\n",
        "\n",
        "\n",
        "config = ExperimentConfig()\n",
        "print(f\"Models: {list(config.models.keys())}\")\n",
        "print(f\"Prompts per level: {config.prompts_per_level}\")\n",
        "print(f\"Total prompts per pattern: {config.prompts_per_level * 6}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_name: str) -> Tuple:\n",
        "    \"\"\"Load model and tokenizer.\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "        output_hidden_states=True,\n",
        "        output_attentions=True\n",
        "    )\n",
        "    model.eval()\n",
        "    \n",
        "    if hasattr(model.config, 'n_layer'):\n",
        "        n_layers = model.config.n_layer\n",
        "    elif hasattr(model.config, 'num_hidden_layers'):\n",
        "        n_layers = model.config.num_hidden_layers\n",
        "    else:\n",
        "        n_layers = 24\n",
        "    \n",
        "    print(f\"  Layers: {n_layers}, Hidden: {model.config.hidden_size}\")\n",
        "    return model, tokenizer, n_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CrystallizationMetrics:\n",
        "    \"\"\"Metrics for measuring belief crystallization strength.\"\"\"\n",
        "    \n",
        "    # Phase metrics (from activations)\n",
        "    phase_variance: float = 0.0  # Lower = more crystallized\n",
        "    phase_coherence: float = 0.0  # Higher = more crystallized (Rayleigh R)\n",
        "    \n",
        "    # Activation metrics\n",
        "    activation_magnitude: float = 0.0\n",
        "    activation_variance: float = 0.0  # Higher variance = less crystallized\n",
        "    \n",
        "    # Cross-layer coherence\n",
        "    layer_agreement: float = 0.0  # Do layers agree? Higher = more crystallized\n",
        "    \n",
        "    # Output metrics\n",
        "    confidence: float = 0.0  # Token probability\n",
        "    entropy: float = 0.0  # Lower entropy = more certain\n",
        "    \n",
        "    # Response analysis\n",
        "    response_text: str = \"\"\n",
        "    contains_hedging: bool = False  # \"maybe\", \"possibly\", \"not sure\"\n",
        "    contains_action: bool = False  # Clear action verb\n",
        "\n",
        "\n",
        "def compute_phase_from_activations(activations: np.ndarray) -> Tuple[float, float]:\n",
        "    \"\"\"Compute phase metrics from activation vectors.\n",
        "    \n",
        "    Uses PCA to find dominant directions, then computes phase angles.\n",
        "    \n",
        "    Args:\n",
        "        activations: Array of shape (n_samples, hidden_size)\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (circular_variance, rayleigh_r)\n",
        "    \"\"\"\n",
        "    if len(activations) < 2:\n",
        "        return 0.0, 1.0\n",
        "    \n",
        "    # Center the activations\n",
        "    centered = activations - np.mean(activations, axis=0)\n",
        "    \n",
        "    # Use first two principal components to define phase\n",
        "    U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
        "    \n",
        "    # Project onto first two components\n",
        "    proj = centered @ Vt[:2].T  # (n_samples, 2)\n",
        "    \n",
        "    # Compute phase angles\n",
        "    phases = np.arctan2(proj[:, 1], proj[:, 0])\n",
        "    \n",
        "    # Circular variance (0 = perfectly aligned, 1 = uniform)\n",
        "    circ_var = circvar(phases)\n",
        "    \n",
        "    # Rayleigh R (mean resultant length, 1 = perfectly aligned, 0 = uniform)\n",
        "    mean_cos = np.mean(np.cos(phases))\n",
        "    mean_sin = np.mean(np.sin(phases))\n",
        "    rayleigh_r = np.sqrt(mean_cos**2 + mean_sin**2)\n",
        "    \n",
        "    return float(circ_var), float(rayleigh_r)\n",
        "\n",
        "\n",
        "def analyze_prompt(model, tokenizer, prompt: str, layers: List[int]) -> CrystallizationMetrics:\n",
        "    \"\"\"Analyze a single prompt for crystallization metrics.\"\"\"\n",
        "    metrics = CrystallizationMetrics()\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get hidden states\n",
        "        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "        \n",
        "        # Collect activations from specified layers (last token)\n",
        "        layer_activations = []\n",
        "        for layer_idx in layers:\n",
        "            if layer_idx < len(hidden_states):\n",
        "                act = hidden_states[layer_idx][0, -1, :].cpu().numpy().astype(np.float32)\n",
        "                layer_activations.append(act)\n",
        "        \n",
        "        if layer_activations:\n",
        "            layer_activations = np.array(layer_activations)\n",
        "            \n",
        "            # Activation magnitude (mean across layers)\n",
        "            metrics.activation_magnitude = float(np.mean(np.linalg.norm(layer_activations, axis=1)))\n",
        "            \n",
        "            # Activation variance across layers\n",
        "            metrics.activation_variance = float(np.var(layer_activations))\n",
        "            \n",
        "            # Layer agreement (cosine similarity between consecutive layers)\n",
        "            if len(layer_activations) > 1:\n",
        "                agreements = []\n",
        "                for i in range(len(layer_activations) - 1):\n",
        "                    a, b = layer_activations[i], layer_activations[i+1]\n",
        "                    sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)\n",
        "                    agreements.append(sim)\n",
        "                metrics.layer_agreement = float(np.mean(agreements))\n",
        "        \n",
        "        # Generate response\n",
        "        gen_outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=config.max_new_tokens,\n",
        "            do_sample=False,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        \n",
        "        # Decode response\n",
        "        generated_ids = gen_outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "        metrics.response_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "        \n",
        "        # Confidence (mean probability of generated tokens)\n",
        "        if gen_outputs.scores:\n",
        "            probs = []\n",
        "            for i, score in enumerate(gen_outputs.scores):\n",
        "                if i < len(generated_ids):\n",
        "                    token_id = generated_ids[i].item()\n",
        "                    prob = torch.softmax(score[0], dim=-1)[token_id].item()\n",
        "                    probs.append(prob)\n",
        "            if probs:\n",
        "                metrics.confidence = float(np.mean(probs))\n",
        "                \n",
        "                # Entropy of first token distribution\n",
        "                first_probs = torch.softmax(gen_outputs.scores[0][0], dim=-1)\n",
        "                entropy = -torch.sum(first_probs * torch.log(first_probs + 1e-10)).item()\n",
        "                metrics.entropy = entropy\n",
        "    \n",
        "    # Analyze response text\n",
        "    response_lower = metrics.response_text.lower()\n",
        "    hedging_words = [\"maybe\", \"possibly\", \"perhaps\", \"might\", \"could\", \"not sure\", \n",
        "                    \"uncertain\", \"depends\", \"it depends\", \"hard to say\"]\n",
        "    action_words = [\"run\", \"flee\", \"go\", \"leave\", \"stay\", \"do\", \"make\", \"take\",\n",
        "                   \"should\", \"must\", \"need to\", \"have to\", \"will\"]\n",
        "    \n",
        "    metrics.contains_hedging = any(w in response_lower for w in hedging_words)\n",
        "    metrics.contains_action = any(w in response_lower for w in action_words)\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_crystallization_experiment(model_name: str, model_path: str) -> Dict:\n",
        "    \"\"\"Run crystallization experiment for one model.\"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model, tokenizer, n_layers = load_model(model_path)\n",
        "    \n",
        "    # Select layers (skip layer 0)\n",
        "    layers = [max(1, int(r * n_layers)) for r in config.layer_ratios]\n",
        "    layers = sorted(set(layers))  # Remove duplicates\n",
        "    print(f\"Analyzing layers: {layers}\")\n",
        "    \n",
        "    results = {\n",
        "        \"model\": model_name,\n",
        "        \"layers\": layers,\n",
        "        \"patterns\": {}\n",
        "    }\n",
        "    \n",
        "    for pattern_name in BELIEF_PATTERNS.keys():\n",
        "        print(f\"\\nPattern: {pattern_name}\")\n",
        "        \n",
        "        # Generate prompts\n",
        "        prompts_by_level = generate_crystallization_prompts(\n",
        "            pattern_name, \n",
        "            n_per_level=config.prompts_per_level\n",
        "        )\n",
        "        \n",
        "        pattern_results = {level: [] for level in range(6)}\n",
        "        \n",
        "        for level in range(5, -1, -1):\n",
        "            print(f\"  Level {level}...\", end=\" \")\n",
        "            \n",
        "            level_metrics = []\n",
        "            for prompt in tqdm(prompts_by_level[level], desc=f\"L{level}\", leave=False):\n",
        "                metrics = analyze_prompt(model, tokenizer, prompt, layers)\n",
        "                level_metrics.append(metrics)\n",
        "            \n",
        "            pattern_results[level] = level_metrics\n",
        "            \n",
        "            # Quick summary\n",
        "            avg_conf = np.mean([m.confidence for m in level_metrics])\n",
        "            avg_layer_agree = np.mean([m.layer_agreement for m in level_metrics])\n",
        "            hedge_rate = np.mean([m.contains_hedging for m in level_metrics])\n",
        "            print(f\"conf={avg_conf:.3f}, agree={avg_layer_agree:.3f}, hedge={hedge_rate:.2f}\")\n",
        "        \n",
        "        results[\"patterns\"][pattern_name] = pattern_results\n",
        "    \n",
        "    # Cleanup\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Run experiment\n",
        "all_results = {}\n",
        "for model_name, model_path in config.models.items():\n",
        "    try:\n",
        "        all_results[model_name] = run_crystallization_experiment(model_name, model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {model_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_crystallization_results(results: Dict) -> None:\n",
        "    \"\"\"Plot crystallization metrics by level.\"\"\"\n",
        "    \n",
        "    for model_name, model_results in results.items():\n",
        "        n_patterns = len(model_results[\"patterns\"])\n",
        "        fig, axes = plt.subplots(n_patterns, 4, figsize=(16, 4 * n_patterns))\n",
        "        \n",
        "        for row, (pattern_name, pattern_data) in enumerate(model_results[\"patterns\"].items()):\n",
        "            levels = list(range(6))\n",
        "            \n",
        "            # Collect metrics by level\n",
        "            confidences = [np.mean([m.confidence for m in pattern_data[l]]) for l in levels]\n",
        "            conf_stds = [np.std([m.confidence for m in pattern_data[l]]) for l in levels]\n",
        "            \n",
        "            layer_agrees = [np.mean([m.layer_agreement for m in pattern_data[l]]) for l in levels]\n",
        "            agree_stds = [np.std([m.layer_agreement for m in pattern_data[l]]) for l in levels]\n",
        "            \n",
        "            entropies = [np.mean([m.entropy for m in pattern_data[l]]) for l in levels]\n",
        "            \n",
        "            hedge_rates = [np.mean([m.contains_hedging for m in pattern_data[l]]) for l in levels]\n",
        "            action_rates = [np.mean([m.contains_action for m in pattern_data[l]]) for l in levels]\n",
        "            \n",
        "            # Plot confidence\n",
        "            ax = axes[row, 0] if n_patterns > 1 else axes[0]\n",
        "            ax.errorbar(levels, confidences, yerr=conf_stds, marker='o', capsize=5)\n",
        "            ax.set_xlabel('Crystallization Level')\n",
        "            ax.set_ylabel('Confidence')\n",
        "            ax.set_title(f'{pattern_name}\\nConfidence by Level')\n",
        "            ax.set_xticks(levels)\n",
        "            ax.set_xticklabels(['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete'])\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Plot layer agreement\n",
        "            ax = axes[row, 1] if n_patterns > 1 else axes[1]\n",
        "            ax.errorbar(levels, layer_agrees, yerr=agree_stds, marker='s', capsize=5, color='green')\n",
        "            ax.set_xlabel('Crystallization Level')\n",
        "            ax.set_ylabel('Layer Agreement')\n",
        "            ax.set_title(f'{pattern_name}\\nLayer Agreement (Coherence)')\n",
        "            ax.set_xticks(levels)\n",
        "            ax.set_xticklabels(['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete'])\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Plot entropy\n",
        "            ax = axes[row, 2] if n_patterns > 1 else axes[2]\n",
        "            ax.bar(levels, entropies, color='purple', alpha=0.7)\n",
        "            ax.set_xlabel('Crystallization Level')\n",
        "            ax.set_ylabel('Entropy')\n",
        "            ax.set_title(f'{pattern_name}\\nOutput Entropy (Lower = More Certain)')\n",
        "            ax.set_xticks(levels)\n",
        "            ax.set_xticklabels(['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete'])\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Plot hedging vs action rates\n",
        "            ax = axes[row, 3] if n_patterns > 1 else axes[3]\n",
        "            x = np.arange(len(levels))\n",
        "            width = 0.35\n",
        "            ax.bar(x - width/2, hedge_rates, width, label='Hedging', color='red', alpha=0.7)\n",
        "            ax.bar(x + width/2, action_rates, width, label='Action', color='blue', alpha=0.7)\n",
        "            ax.set_xlabel('Crystallization Level')\n",
        "            ax.set_ylabel('Rate')\n",
        "            ax.set_title(f'{pattern_name}\\nHedging vs Action in Response')\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete'])\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.suptitle(f'{model_name}: Belief Crystallization Analysis', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'035G_crystallization_{model_name}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if all_results:\n",
        "    plot_crystallization_results(all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_crystallization_summary(results: Dict) -> None:\n",
        "    \"\"\"Summary plot across all patterns.\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(1, len(results), figsize=(7 * len(results), 5))\n",
        "    if len(results) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ax, (model_name, model_results) in zip(axes, results.items()):\n",
        "        levels = list(range(6))\n",
        "        \n",
        "        # Average across all patterns\n",
        "        all_confidences = {l: [] for l in levels}\n",
        "        all_agreements = {l: [] for l in levels}\n",
        "        \n",
        "        for pattern_data in model_results[\"patterns\"].values():\n",
        "            for l in levels:\n",
        "                all_confidences[l].extend([m.confidence for m in pattern_data[l]])\n",
        "                all_agreements[l].extend([m.layer_agreement for m in pattern_data[l]])\n",
        "        \n",
        "        conf_means = [np.mean(all_confidences[l]) for l in levels]\n",
        "        agree_means = [np.mean(all_agreements[l]) for l in levels]\n",
        "        \n",
        "        ax.plot(levels, conf_means, 'b-o', label='Confidence', linewidth=2, markersize=8)\n",
        "        ax.plot(levels, agree_means, 'g-s', label='Layer Agreement', linewidth=2, markersize=8)\n",
        "        \n",
        "        ax.set_xlabel('Crystallization Level', fontsize=12)\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_title(f'{model_name}\\nCrystallization Metrics Summary', fontsize=14)\n",
        "        ax.set_xticks(levels)\n",
        "        ax.set_xticklabels(['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete'])\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('035G_crystallization_summary.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if all_results:\n",
        "    plot_crystallization_summary(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def statistical_summary(results: Dict) -> None:\n",
        "    \"\"\"Compute and print statistical summary.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STATISTICAL SUMMARY: BELIEF CRYSTALLIZATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for model_name, model_results in results.items():\n",
        "        print(f\"\\n### {model_name} ###\")\n",
        "        \n",
        "        # Collect all metrics by level\n",
        "        all_by_level = {l: {\"conf\": [], \"agree\": [], \"entropy\": [], \"hedge\": [], \"action\": []} \n",
        "                       for l in range(6)}\n",
        "        \n",
        "        for pattern_data in model_results[\"patterns\"].values():\n",
        "            for l in range(6):\n",
        "                for m in pattern_data[l]:\n",
        "                    all_by_level[l][\"conf\"].append(m.confidence)\n",
        "                    all_by_level[l][\"agree\"].append(m.layer_agreement)\n",
        "                    all_by_level[l][\"entropy\"].append(m.entropy)\n",
        "                    all_by_level[l][\"hedge\"].append(m.contains_hedging)\n",
        "                    all_by_level[l][\"action\"].append(m.contains_action)\n",
        "        \n",
        "        print(\"\\nMetrics by Crystallization Level:\")\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"{'Level':<12} {'Confidence':>12} {'LayerAgree':>12} {'Entropy':>12} {'Hedge%':>10} {'Action%':>10}\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        level_names = ['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete']\n",
        "        for l in range(6):\n",
        "            conf = np.mean(all_by_level[l][\"conf\"])\n",
        "            agree = np.mean(all_by_level[l][\"agree\"])\n",
        "            ent = np.mean(all_by_level[l][\"entropy\"])\n",
        "            hedge = np.mean(all_by_level[l][\"hedge\"]) * 100\n",
        "            action = np.mean(all_by_level[l][\"action\"]) * 100\n",
        "            print(f\"{level_names[l]:<12} {conf:>12.4f} {agree:>12.4f} {ent:>12.2f} {hedge:>9.1f}% {action:>9.1f}%\")\n",
        "        \n",
        "        # Correlation analysis\n",
        "        print(\"\\nCorrelation with Crystallization Level:\")\n",
        "        \n",
        "        all_levels = []\n",
        "        all_confs = []\n",
        "        all_agrees = []\n",
        "        \n",
        "        for l in range(6):\n",
        "            for conf, agree in zip(all_by_level[l][\"conf\"], all_by_level[l][\"agree\"]):\n",
        "                all_levels.append(l)\n",
        "                all_confs.append(conf)\n",
        "                all_agrees.append(agree)\n",
        "        \n",
        "        r_conf, p_conf = stats.pearsonr(all_levels, all_confs)\n",
        "        r_agree, p_agree = stats.pearsonr(all_levels, all_agrees)\n",
        "        \n",
        "        print(f\"  Confidence: r = {r_conf:.3f}, p = {p_conf:.6f}\")\n",
        "        print(f\"  Layer Agreement: r = {r_agree:.3f}, p = {p_agree:.6f}\")\n",
        "        \n",
        "        # Compare extremes (Level 0 vs Level 5)\n",
        "        print(\"\\nComparing Extremes (Impossible vs Complete):\")\n",
        "        \n",
        "        t_conf, p_conf = stats.ttest_ind(all_by_level[0][\"conf\"], all_by_level[5][\"conf\"])\n",
        "        t_agree, p_agree = stats.ttest_ind(all_by_level[0][\"agree\"], all_by_level[5][\"agree\"])\n",
        "        \n",
        "        # Cohen's d\n",
        "        def cohens_d(a, b):\n",
        "            pooled_std = np.sqrt((np.std(a)**2 + np.std(b)**2) / 2)\n",
        "            return (np.mean(b) - np.mean(a)) / pooled_std if pooled_std > 0 else 0\n",
        "        \n",
        "        d_conf = cohens_d(all_by_level[0][\"conf\"], all_by_level[5][\"conf\"])\n",
        "        d_agree = cohens_d(all_by_level[0][\"agree\"], all_by_level[5][\"agree\"])\n",
        "        \n",
        "        print(f\"  Confidence: t = {t_conf:.3f}, p = {p_conf:.6f}, Cohen's d = {d_conf:.3f}\")\n",
        "        print(f\"  Layer Agreement: t = {t_agree:.3f}, p = {p_agree:.6f}, Cohen's d = {d_agree:.3f}\")\n",
        "        \n",
        "        # Interpretation\n",
        "        print(\"\\nInterpretation:\")\n",
        "        if r_conf > 0.1 and p_conf < 0.05:\n",
        "            print(f\"  Confidence INCREASES with crystallization level (r={r_conf:.3f})\")\n",
        "        elif r_conf < -0.1 and p_conf < 0.05:\n",
        "            print(f\"  Confidence DECREASES with crystallization level (r={r_conf:.3f})\")\n",
        "        else:\n",
        "            print(f\"  No significant relationship between confidence and crystallization\")\n",
        "        \n",
        "        if r_agree > 0.1 and p_agree < 0.05:\n",
        "            print(f\"  Layer agreement INCREASES with crystallization level (r={r_agree:.3f})\")\n",
        "            print(f\"  -> SUPPORTS belief crystallization hypothesis\")\n",
        "        elif r_agree < -0.1 and p_agree < 0.05:\n",
        "            print(f\"  Layer agreement DECREASES with crystallization level (r={r_agree:.3f})\")\n",
        "        else:\n",
        "            print(f\"  No significant relationship between layer agreement and crystallization\")\n",
        "\n",
        "\n",
        "if all_results:\n",
        "    statistical_summary(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Example Responses Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_example_responses(results: Dict, n_examples: int = 3) -> None:\n",
        "    \"\"\"Show example responses at different crystallization levels.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXAMPLE RESPONSES BY CRYSTALLIZATION LEVEL\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for model_name, model_results in results.items():\n",
        "        print(f\"\\n### {model_name} ###\")\n",
        "        \n",
        "        # Pick one pattern to show\n",
        "        pattern_name = \"THREAT_FLEE\"\n",
        "        pattern_data = model_results[\"patterns\"][pattern_name]\n",
        "        \n",
        "        level_names = ['Impossible', 'Contradict', 'Ambiguous', 'Partial', 'Mostly', 'Complete']\n",
        "        \n",
        "        for level in [5, 3, 1, 0]:  # Show key levels\n",
        "            print(f\"\\n--- Level {level} ({level_names[level]}) ---\")\n",
        "            \n",
        "            for i in range(min(n_examples, len(pattern_data[level]))):\n",
        "                m = pattern_data[level][i]\n",
        "                print(f\"\\nResponse {i+1}:\")\n",
        "                print(f\"  Text: {m.response_text[:150]}...\" if len(m.response_text) > 150 else f\"  Text: {m.response_text}\")\n",
        "                print(f\"  Confidence: {m.confidence:.3f}\")\n",
        "                print(f\"  Hedging: {m.contains_hedging}, Action: {m.contains_action}\")\n",
        "\n",
        "\n",
        "if all_results:\n",
        "    show_example_responses(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusions\n",
        "\n",
        "This experiment tests **belief crystallization strength** - the idea that AQ emerge\n",
        "when sufficient consistent pattern evidence accumulates.\n",
        "\n",
        "**Key predictions:**\n",
        "\n",
        "1. **Complete patterns (Level 5)** should show:\n",
        "   - High confidence\n",
        "   - High layer agreement (crystallized belief)\n",
        "   - Clear action in response\n",
        "   - Low hedging\n",
        "\n",
        "2. **Impossible patterns (Level 0)** should show:\n",
        "   - Lower confidence (or overconfident hallucination)\n",
        "   - Lower layer agreement (no crystallization)\n",
        "   - Hedging or nonsensical responses\n",
        "\n",
        "3. **Gradient between levels** should show:\n",
        "   - Monotonic relationship between pattern completeness and crystallization metrics\n",
        "\n",
        "**Connection to AKIRA theory:**\n",
        "\n",
        "- AQ crystallize from **signal + context**\n",
        "- Incomplete context = incomplete crystallization\n",
        "- Contradictory context = interference, no crystallization\n",
        "- The model needs sufficient consistent AQ to construct coherent response\n",
        "\n",
        "**The question \"How do I X if X isn't X?\"** captures the failure mode:\n",
        "when patterns contradict, belief cannot crystallize, and the model either\n",
        "hallucinates, hedges, or fails."
      ]
    }
  ]
}
