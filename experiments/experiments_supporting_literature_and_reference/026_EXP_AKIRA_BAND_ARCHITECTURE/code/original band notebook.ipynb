{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c44ee567e914cbd8d8b5233d59a77da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1645a02bfc9d4d7188db90f136324a20",
              "IPY_MODEL_f358706bf3884acea7a5e494d6ddfcec",
              "IPY_MODEL_b7210bdd1a254a5bb27bd5219c685d9b"
            ],
            "layout": "IPY_MODEL_207fcee708074b968428b4878ccc778a"
          }
        },
        "1645a02bfc9d4d7188db90f136324a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a596354a6941e7bf04bf48ec1706d4",
            "placeholder": "​",
            "style": "IPY_MODEL_04313bf3e7494ac79555c5c229aa3882",
            "value": "Map: 100%"
          }
        },
        "f358706bf3884acea7a5e494d6ddfcec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dcc0a03cbcc45b2b261cb3b3d554be5",
            "max": 1165029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0941f1beeb74e7e8f83a0b0f1962b53",
            "value": 1165029
          }
        },
        "b7210bdd1a254a5bb27bd5219c685d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5e1e0d90534837b27d61c68246f2dc",
            "placeholder": "​",
            "style": "IPY_MODEL_140dac182a384e09a761af8ef7f73a77",
            "value": " 1165029/1165029 [16:02&lt;00:00, 1188.74 examples/s]"
          }
        },
        "207fcee708074b968428b4878ccc778a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a596354a6941e7bf04bf48ec1706d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04313bf3e7494ac79555c5c229aa3882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dcc0a03cbcc45b2b261cb3b3d554be5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0941f1beeb74e7e8f83a0b0f1962b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b5e1e0d90534837b27d61c68246f2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140dac182a384e09a761af8ef7f73a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "498b7eba2ad742328cf7205ae47d8354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7000be65a114443a677a5b8120fa68b",
              "IPY_MODEL_404e7f16de26493688bd6a682cba6c22",
              "IPY_MODEL_c3d6453559394b49adc52e3730d9340b"
            ],
            "layout": "IPY_MODEL_9c624420158149f9a41a74b0d0426683"
          }
        },
        "e7000be65a114443a677a5b8120fa68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a592aab42bd747369444139ceffe8b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_1d0af4ecd25a43868eb1f1b27f0a00f0",
            "value": "Map: 100%"
          }
        },
        "404e7f16de26493688bd6a682cba6c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_376233e958734ac6bb90bc3f363b62c2",
            "max": 2461,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8558686527c148188b38c877800184a7",
            "value": 2461
          }
        },
        "c3d6453559394b49adc52e3730d9340b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e7008fef2624645b267092498d81e81",
            "placeholder": "​",
            "style": "IPY_MODEL_47114dffea6a43bc93f71f0b837c6608",
            "value": " 2461/2461 [00:02&lt;00:00, 960.48 examples/s]"
          }
        },
        "9c624420158149f9a41a74b0d0426683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a592aab42bd747369444139ceffe8b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0af4ecd25a43868eb1f1b27f0a00f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "376233e958734ac6bb90bc3f363b62c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8558686527c148188b38c877800184a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e7008fef2624645b267092498d81e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47114dffea6a43bc93f71f0b837c6608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2530ace9958d4799b29e1b22173ae742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_328e5c5edd0741cfbf1da6ff1a892cd7",
              "IPY_MODEL_f1e8fda5e2434fd48abb479ecd0f93b5",
              "IPY_MODEL_bad47e841a4b47ce8a4d681f7b23afb1"
            ],
            "layout": "IPY_MODEL_c7575e82f105450c83e9de5ddea2a9b8"
          }
        },
        "328e5c5edd0741cfbf1da6ff1a892cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac21a5f378484840b22518d0be8cca09",
            "placeholder": "​",
            "style": "IPY_MODEL_a637255537e1414d98f65d9a70b680e1",
            "value": "Map: 100%"
          }
        },
        "f1e8fda5e2434fd48abb479ecd0f93b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86329401fa704ba2aec44264170ceba1",
            "max": 1165029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0141ca9c3df4e37b7d03eb965adbb88",
            "value": 1165029
          }
        },
        "bad47e841a4b47ce8a4d681f7b23afb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e558eb9a685a438dbceb0561eda2ccc6",
            "placeholder": "​",
            "style": "IPY_MODEL_89f9492fb3264dc6a99b51ee68dbedad",
            "value": " 1165029/1165029 [15:55&lt;00:00, 1266.11 examples/s]"
          }
        },
        "c7575e82f105450c83e9de5ddea2a9b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac21a5f378484840b22518d0be8cca09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a637255537e1414d98f65d9a70b680e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86329401fa704ba2aec44264170ceba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0141ca9c3df4e37b7d03eb965adbb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e558eb9a685a438dbceb0561eda2ccc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f9492fb3264dc6a99b51ee68dbedad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b524afb918d4cb19ec57a1ca2758c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d462dd58ddd4ef8891c78a9fae2804b",
              "IPY_MODEL_ae7c786cd47a4de7beb0ca89ad06d177",
              "IPY_MODEL_03ad5e1e2ffd47a7bb3f738526c8dfc5"
            ],
            "layout": "IPY_MODEL_8239acdc08784542a05a17bfed76f907"
          }
        },
        "4d462dd58ddd4ef8891c78a9fae2804b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d12332b6cd645fea549cf38a8409f4d",
            "placeholder": "​",
            "style": "IPY_MODEL_8607387920914f6f90c64fa18a818934",
            "value": "Map: 100%"
          }
        },
        "ae7c786cd47a4de7beb0ca89ad06d177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7991daf38754832a0f882a78ea2775c",
            "max": 2461,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_609c139658ef4355858787a254e79928",
            "value": 2461
          }
        },
        "03ad5e1e2ffd47a7bb3f738526c8dfc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65bc8d5486c744e4babe4dbd403523b3",
            "placeholder": "​",
            "style": "IPY_MODEL_d762989a192f47918a47d38b872064b6",
            "value": " 2461/2461 [00:02&lt;00:00, 941.58 examples/s]"
          }
        },
        "8239acdc08784542a05a17bfed76f907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d12332b6cd645fea549cf38a8409f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8607387920914f6f90c64fa18a818934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7991daf38754832a0f882a78ea2775c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "609c139658ef4355858787a254e79928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65bc8d5486c744e4babe4dbd403523b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d762989a192f47918a47d38b872064b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy scipy tqdm transformer_lens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "YMPDYRvm1ioO",
        "outputId": "8b71dbd3-4bb9-4381-c16f-2853e643749a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.12.0)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.8.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.57.2)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.46.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer_lens) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.0/192.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading jaxtyping-0.3.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: transformers-stream-generator\n",
            "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=ad8f4c9c0e799a6f9f17986953ae52dd75b861602e7eb2c78dcf8753f256fcff\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n",
            "Successfully built transformers-stream-generator\n",
            "Installing collected packages: better-abc, wadler-lindig, numpy, fancy-einsum, beartype, jaxtyping, transformers-stream-generator, transformer_lens\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: beartype\n",
            "    Found existing installation: beartype 0.22.6\n",
            "    Uninstalling beartype-0.22.6:\n",
            "      Successfully uninstalled beartype-0.22.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "plum-dispatch 2.6.0 requires beartype>=0.16.2, but you have beartype 0.14.1 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 fancy-einsum-0.0.3 jaxtyping-0.3.3 numpy-1.26.4 transformer_lens-2.16.1 transformers-stream-generator-0.0.5 wadler-lindig-0.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "698c6f4645fc42c08b0cf28569fa49c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-w8mi5N96P7",
        "outputId": "e2480664-8d22-4076-fa78-da63d8669b48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pot\n",
            "  Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.12/dist-packages (from pot) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.12/dist-packages (from pot) (1.16.3)\n",
            "Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pot\n",
            "Successfully installed pot-0.9.6.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install typing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "zvxAO6gJ_gxr",
        "outputId": "6e8ac9cd-90dd-45b6-86b1-f97c415b48e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=4b9c4a733f9a9ca0dd385d1afcc8d7c165f0ba043f8f51e8ca42835a67ba8f5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/98/52/2bffe242a9a487f00886e43b8ed8dac46456702e11a0d6abef\n",
            "Successfully built typing\n",
            "Installing collected packages: typing\n",
            "Successfully installed typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              },
              "id": "22c6385341b846d9aec46f48ef06dfaa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install math\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOj5y5gEG2cV",
        "outputId": "3defa9e3-859e-43eb-f327-69c2d3574a7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement math (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for math\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AKIRA Band Transformer Implementation\n",
        "=====================================\n",
        "\n",
        "This implements the core AKIRA architectural innovation: explicit spectral band structure\n",
        "with differential learning rates.\n",
        "\n",
        "Key differences from standard transformer:\n",
        "1. Single MLP replaced with 7 parallel band MLPs\n",
        "2. Each band has its own learning rate (slow for low-freq, fast for high-freq)\n",
        "3. Wormhole attention enables cross-band communication\n",
        "4. Band dimensions follow AKIRA's pattern (larger for low-freq)\n",
        "\n",
        "AKIRA Project - Experiment 026\n",
        "Oscar Goldman - Shogu Research Group @ Datamutant.ai\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AKIRAConfig:\n",
        "    \"\"\"\n",
        "    Configuration for AKIRA Band Transformer.\n",
        "\n",
        "    Band structure follows AKIRA's 7-band architecture:\n",
        "    - Band 0: DC (lowest frequency, most stable features)\n",
        "    - Band 1-2: Low frequency\n",
        "    - Band 3: Mid frequency\n",
        "    - Band 4-5: High frequency\n",
        "    - Band 6: Highest frequency (most adaptive features)\n",
        "    \"\"\"\n",
        "    # Model dimensions\n",
        "    vocab_size: int = 50257  # GPT-2 vocab\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    max_seq_length: int = 512\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # Band configuration (7 bands)\n",
        "    num_bands: int = 7\n",
        "    band_dims: Tuple[int, ...] = (128, 96, 80, 64, 64, 48, 32)  # Sum = 512\n",
        "\n",
        "    # Learning rates per band (from slow to fast)\n",
        "    band_learning_rates: Tuple[float, ...] = (1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2)\n",
        "\n",
        "    # MLP expansion factor\n",
        "    mlp_expansion: int = 4\n",
        "\n",
        "    # Wormhole attention settings\n",
        "    use_wormhole: bool = True\n",
        "    wormhole_threshold: float = 0.5  # Activation threshold for cross-band communication\n",
        "\n",
        "    @property\n",
        "    def hidden_dim(self) -> int:\n",
        "        \"\"\"Total hidden dimension (sum of all bands).\"\"\"\n",
        "        return sum(self.band_dims)\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        \"\"\"Validate configuration consistency.\"\"\"\n",
        "        assert len(self.band_dims) == self.num_bands, \"band_dims must match num_bands\"\n",
        "        assert len(self.band_learning_rates) == self.num_bands, \"band_learning_rates must match num_bands\"\n",
        "        assert self.hidden_dim % self.num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "        return True\n",
        "\n",
        "\n",
        "class BandMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP for a single frequency band.\n",
        "\n",
        "    Each band has its own MLP with dimension specific to that band.\n",
        "    Low-frequency bands are larger (more capacity for stable features).\n",
        "    High-frequency bands are smaller (less capacity, more adaptive).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, band_dim: int, expansion: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.band_dim = band_dim\n",
        "        hidden_dim = band_dim * expansion\n",
        "\n",
        "        self.fc1 = nn.Linear(band_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, band_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through band MLP.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, seq, band_dim]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape [batch, seq, band_dim]\n",
        "        \"\"\"\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WormholeAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Wormhole attention for cross-band communication.\n",
        "\n",
        "    Allows information to \"jump\" between frequency bands when needed.\n",
        "    This implements AKIRA's wormhole attention concept: when high-frequency\n",
        "    patterns need to inform low-frequency representations (or vice versa),\n",
        "    wormhole attention enables this communication.\n",
        "\n",
        "    The attention is sparse and gated - it only activates when the\n",
        "    cross-band relevance exceeds a threshold.\n",
        "\n",
        "    Uses a COMMON projection dimension for all cross-band queries/keys\n",
        "    to enable attention between bands of different sizes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, band_dims: Tuple[int, ...], threshold: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.band_dims = band_dims\n",
        "        self.num_bands = len(band_dims)\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # Common dimension for cross-band attention (use smallest band // 2)\n",
        "        self.cross_dim = min(band_dims) // 2  # = 16 for default config\n",
        "\n",
        "        # Projection matrices for cross-band queries and keys\n",
        "        # All project to the SAME cross_dim for compatibility\n",
        "        self.cross_queries = nn.ModuleList([\n",
        "            nn.Linear(dim, self.cross_dim) for dim in band_dims\n",
        "        ])\n",
        "        self.cross_keys = nn.ModuleList([\n",
        "            nn.Linear(dim, self.cross_dim) for dim in band_dims\n",
        "        ])\n",
        "\n",
        "        # Values project to target band dimension (for output)\n",
        "        # We need value projections from each source to each target\n",
        "        # Simplified: project to common dim, then expand per-target\n",
        "        self.cross_values = nn.ModuleList([\n",
        "            nn.Linear(dim, self.cross_dim * 2) for dim in band_dims\n",
        "        ])\n",
        "\n",
        "        # Output projection: from common value dim to target band dim\n",
        "        self.out_projs = nn.ModuleList([\n",
        "            nn.Linear(self.cross_dim * 2, dim) for dim in band_dims\n",
        "        ])\n",
        "\n",
        "        # Gate to control cross-band information flow\n",
        "        self.gate = nn.ModuleList([\n",
        "            nn.Linear(dim + dim, dim) for dim in band_dims  # band + projected output\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        bands: List[torch.Tensor],\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> List[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Apply wormhole attention across bands.\n",
        "\n",
        "        Args:\n",
        "            bands: List of tensors, one per band, each [batch, seq, band_dim]\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            List of updated band tensors\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = bands[0].shape[:2]\n",
        "        updated_bands = []\n",
        "\n",
        "        for i, band in enumerate(bands):\n",
        "            # Compute cross-band attention for this band\n",
        "            cross_info = torch.zeros(batch_size, seq_len, self.band_dims[i], device=band.device)\n",
        "\n",
        "            for j, other_band in enumerate(bands):\n",
        "                if i == j:\n",
        "                    continue  # Skip self\n",
        "\n",
        "                # Query from current band, key/value from other band\n",
        "                # All use common cross_dim for compatibility\n",
        "                q = self.cross_queries[i](band)  # [batch, seq, cross_dim]\n",
        "                k = self.cross_keys[j](other_band)  # [batch, seq, cross_dim]\n",
        "                v = self.cross_values[j](other_band)  # [batch, seq, cross_dim*2]\n",
        "\n",
        "                # Attention scores\n",
        "                scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.cross_dim)\n",
        "\n",
        "                if mask is not None:\n",
        "                    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "                attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "                # Apply attention to values\n",
        "                cross_output = torch.matmul(attn, v)  # [batch, seq, cross_dim*2]\n",
        "\n",
        "                # Project to target band dimension\n",
        "                cross_output = self.out_projs[i](cross_output)  # [batch, seq, band_dim[i]]\n",
        "\n",
        "                # Apply threshold gate\n",
        "                gate_input = torch.cat([band, cross_output], dim=-1)\n",
        "                gate_value = torch.sigmoid(self.gate[i](gate_input))\n",
        "\n",
        "                # Only allow cross-band info if gate exceeds threshold\n",
        "                gate_value = (gate_value > self.threshold).float() * gate_value\n",
        "\n",
        "                cross_info = cross_info + gate_value * cross_output\n",
        "\n",
        "            # Residual connection with cross-band info\n",
        "            updated_bands.append(band + cross_info / (self.num_bands - 1))\n",
        "\n",
        "        return updated_bands\n",
        "\n",
        "\n",
        "class AKIRAMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention that operates on concatenated bands.\n",
        "\n",
        "    Standard attention but aware of band structure for potential\n",
        "    band-specific attention patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: AKIRAConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = config.num_heads\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.head_dim = config.hidden_dim // config.num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.k_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.v_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Standard multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq, hidden_dim]\n",
        "            mask: Optional causal mask\n",
        "\n",
        "        Returns:\n",
        "            Output tensor [batch, seq, hidden_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class AKIRABandLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single AKIRA transformer layer with band structure.\n",
        "\n",
        "    Processing order:\n",
        "    1. Layer norm\n",
        "    2. Multi-head attention (on concatenated bands)\n",
        "    3. Split into bands\n",
        "    4. Per-band MLP processing\n",
        "    5. Wormhole attention (cross-band communication)\n",
        "    6. Concatenate bands\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: AKIRAConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # Pre-norm for attention\n",
        "        self.norm1 = nn.LayerNorm(config.hidden_dim)\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = AKIRAMultiHeadAttention(config)\n",
        "\n",
        "        # Pre-norm for MLPs\n",
        "        self.band_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(dim) for dim in config.band_dims\n",
        "        ])\n",
        "\n",
        "        # Per-band MLPs\n",
        "        self.band_mlps = nn.ModuleList([\n",
        "            BandMLP(dim, config.mlp_expansion, config.dropout)\n",
        "            for dim in config.band_dims\n",
        "        ])\n",
        "\n",
        "        # Wormhole attention (optional)\n",
        "        if config.use_wormhole:\n",
        "            self.wormhole = WormholeAttention(config.band_dims, config.wormhole_threshold)\n",
        "        else:\n",
        "            self.wormhole = None\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def split_bands(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        \"\"\"Split concatenated representation into bands.\"\"\"\n",
        "        bands = []\n",
        "        start = 0\n",
        "        for dim in self.config.band_dims:\n",
        "            bands.append(x[..., start:start + dim])\n",
        "            start += dim\n",
        "        return bands\n",
        "\n",
        "    def concat_bands(self, bands: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Concatenate bands into single representation.\"\"\"\n",
        "        return torch.cat(bands, dim=-1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through AKIRA layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq, hidden_dim]\n",
        "            mask: Optional causal mask\n",
        "\n",
        "        Returns:\n",
        "            Output tensor [batch, seq, hidden_dim]\n",
        "        \"\"\"\n",
        "        # Attention block\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(x, mask)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        # Split into bands\n",
        "        bands = self.split_bands(x)\n",
        "\n",
        "        # Per-band MLP\n",
        "        processed_bands = []\n",
        "        for i, (band, norm, mlp) in enumerate(zip(bands, self.band_norms, self.band_mlps)):\n",
        "            residual = band\n",
        "            band = norm(band)\n",
        "            band = mlp(band)\n",
        "            band = residual + band\n",
        "            processed_bands.append(band)\n",
        "\n",
        "        # Wormhole attention (cross-band communication)\n",
        "        if self.wormhole is not None:\n",
        "            processed_bands = self.wormhole(processed_bands, mask)\n",
        "\n",
        "        # Concatenate bands\n",
        "        x = self.concat_bands(processed_bands)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AKIRABandTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Full AKIRA Band Transformer model.\n",
        "\n",
        "    This is the main model class that implements AKIRA's spectral band architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: AKIRAConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        config.validate()\n",
        "        self.config = config\n",
        "\n",
        "        # Token and position embeddings\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "        self.position_embedding = nn.Embedding(config.max_seq_length, config.hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # AKIRA layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            AKIRABandLayer(config) for _ in range(config.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.final_norm = nn.LayerNorm(config.hidden_dim)\n",
        "\n",
        "        # Output projection (tied with token embedding by default)\n",
        "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights following GPT-2 conventions.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def get_band_parameters(self) -> Dict[int, List[nn.Parameter]]:\n",
        "        \"\"\"\n",
        "        Get parameters grouped by band for differential learning rates.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping band index to list of parameters\n",
        "        \"\"\"\n",
        "        band_params = {i: [] for i in range(self.config.num_bands)}\n",
        "\n",
        "        for layer in self.layers:\n",
        "            for i, mlp in enumerate(layer.band_mlps):\n",
        "                band_params[i].extend(mlp.parameters())\n",
        "            for i, norm in enumerate(layer.band_norms):\n",
        "                band_params[i].extend(norm.parameters())\n",
        "\n",
        "        return band_params\n",
        "\n",
        "    def get_optimizer_param_groups(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get parameter groups with per-band learning rates for optimizer.\n",
        "\n",
        "        Returns:\n",
        "            List of param group dicts for optimizer\n",
        "        \"\"\"\n",
        "        band_params = self.get_band_parameters()\n",
        "\n",
        "        param_groups = []\n",
        "\n",
        "        # Band-specific parameters with differential LRs\n",
        "        for band_idx, params in band_params.items():\n",
        "            param_groups.append({\n",
        "                'params': params,\n",
        "                'lr': self.config.band_learning_rates[band_idx],\n",
        "                'name': f'band_{band_idx}'\n",
        "            })\n",
        "\n",
        "        # Non-band parameters (embeddings, attention, final norm) with base LR\n",
        "        non_band_params = []\n",
        "        non_band_params.extend(self.token_embedding.parameters())\n",
        "        non_band_params.extend(self.position_embedding.parameters())\n",
        "        non_band_params.extend(self.final_norm.parameters())\n",
        "        non_band_params.extend(self.lm_head.parameters())\n",
        "\n",
        "        for layer in self.layers:\n",
        "            non_band_params.extend(layer.norm1.parameters())\n",
        "            non_band_params.extend(layer.attention.parameters())\n",
        "            if layer.wormhole is not None:\n",
        "                non_band_params.extend(layer.wormhole.parameters())\n",
        "\n",
        "        param_groups.append({\n",
        "            'params': non_band_params,\n",
        "            'lr': self.config.band_learning_rates[3],  # Use mid-band LR as base\n",
        "            'name': 'shared'\n",
        "        })\n",
        "\n",
        "        return param_groups\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through AKIRA model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs [batch, seq]\n",
        "            attention_mask: Optional attention mask\n",
        "            labels: Optional labels for loss computation\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with 'logits' and optionally 'loss'\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Causal mask\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
        "        causal_mask = ~causal_mask  # Flip for attention\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            causal_mask = causal_mask & attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Process through layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, causal_mask)\n",
        "\n",
        "        # Final norm and output\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        output = {'logits': logits}\n",
        "\n",
        "        # Compute loss if labels provided\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, self.config.vocab_size),\n",
        "                shift_labels.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "            output['loss'] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "    def extract_band_activations(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        layer_idx: int\n",
        "    ) -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extract per-band activations for AQ analysis.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs [batch, seq]\n",
        "            layer_idx: Which layer to extract from\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping band index to activation tensor\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Forward pass up to target layer\n",
        "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
        "        causal_mask = ~causal_mask\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x, causal_mask)\n",
        "            if i == layer_idx:\n",
        "                break\n",
        "\n",
        "        # Split into bands\n",
        "        bands = self.layers[layer_idx].split_bands(x)\n",
        "\n",
        "        return {i: band.detach() for i, band in enumerate(bands)}\n",
        "\n",
        "\n",
        "# Utility functions\n",
        "\n",
        "def create_akira_model(\n",
        "    num_bands: int = 7,\n",
        "    hidden_dim: int = 512,\n",
        "    num_layers: int = 6,\n",
        "    num_heads: int = 8,\n",
        "    use_wormhole: bool = True\n",
        ") -> AKIRABandTransformer:\n",
        "    \"\"\"\n",
        "    Create an AKIRA model with default band structure.\n",
        "\n",
        "    Args:\n",
        "        num_bands: Number of frequency bands (default 7)\n",
        "        hidden_dim: Total hidden dimension (will be distributed across bands)\n",
        "        num_layers: Number of transformer layers\n",
        "        num_heads: Number of attention heads\n",
        "        use_wormhole: Whether to use wormhole attention\n",
        "\n",
        "    Returns:\n",
        "        Configured AKIRABandTransformer\n",
        "    \"\"\"\n",
        "    # Distribute dimensions across bands (larger for low-freq)\n",
        "    if num_bands == 7:\n",
        "        # Default AKIRA distribution\n",
        "        band_dims = (128, 96, 80, 64, 64, 48, 32)\n",
        "        band_lrs = (1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2)\n",
        "    else:\n",
        "        # Uniform distribution for other band counts\n",
        "        base_dim = hidden_dim // num_bands\n",
        "        band_dims = tuple([base_dim] * num_bands)\n",
        "        # Logarithmic LR spread\n",
        "        band_lrs = tuple([1e-5 * (10 ** (i * 3 / (num_bands - 1))) for i in range(num_bands)])\n",
        "\n",
        "    config = AKIRAConfig(\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_bands=num_bands,\n",
        "        band_dims=band_dims,\n",
        "        band_learning_rates=band_lrs,\n",
        "        use_wormhole=use_wormhole\n",
        "    )\n",
        "\n",
        "    return AKIRABandTransformer(config)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Quick test\n",
        "    print(\"Creating AKIRA Band Transformer...\")\n",
        "\n",
        "    config = AKIRAConfig()\n",
        "    model = AKIRABandTransformer(config)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Hidden dim: {config.hidden_dim}\")\n",
        "    print(f\"Band dims: {config.band_dims}\")\n",
        "    print(f\"Band LRs: {config.band_learning_rates}\")\n",
        "\n",
        "    # Test forward pass\n",
        "    batch_size, seq_len = 2, 64\n",
        "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "    labels = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    output = model(input_ids, labels=labels)\n",
        "    print(f\"Output logits shape: {output['logits'].shape}\")\n",
        "    print(f\"Loss: {output['loss'].item():.4f}\")\n",
        "\n",
        "    # Test band activation extraction\n",
        "    band_acts = model.extract_band_activations(input_ids, layer_idx=2)\n",
        "    for band_idx, acts in band_acts.items():\n",
        "        print(f\"Band {band_idx} activations: {acts.shape}\")\n",
        "\n",
        "    # Test optimizer param groups\n",
        "    param_groups = model.get_optimizer_param_groups()\n",
        "    for pg in param_groups:\n",
        "        print(f\"Param group '{pg['name']}': {sum(p.numel() for p in pg['params']):,} params, lr={pg['lr']}\")\n",
        "\n",
        "    print(\"\\nAKIRA Band Transformer test complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEiUI6G42tta",
        "outputId": "b529175e-c2ca-4944-da25-2b1ee6673300"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating AKIRA Band Transformer...\n",
            "Model parameters: 60,972,672\n",
            "Hidden dim: 512\n",
            "Band dims: (128, 96, 80, 64, 64, 48, 32)\n",
            "Band LRs: (1e-05, 3e-05, 0.0001, 0.0003, 0.001, 0.003, 0.01)\n",
            "Output logits shape: torch.Size([2, 64, 50257])\n",
            "Loss: 10.9072\n",
            "Band 0 activations: torch.Size([2, 64, 128])\n",
            "Band 1 activations: torch.Size([2, 64, 96])\n",
            "Band 2 activations: torch.Size([2, 64, 80])\n",
            "Band 3 activations: torch.Size([2, 64, 64])\n",
            "Band 4 activations: torch.Size([2, 64, 64])\n",
            "Band 5 activations: torch.Size([2, 64, 48])\n",
            "Band 6 activations: torch.Size([2, 64, 32])\n",
            "Param group 'band_0': 791,808 params, lr=1e-05\n",
            "Param group 'band_1': 446,400 params, lr=3e-05\n",
            "Param group 'band_2': 310,560 params, lr=0.0001\n",
            "Param group 'band_3': 199,296 params, lr=0.0003\n",
            "Param group 'band_4': 199,296 params, lr=0.001\n",
            "Param group 'band_5': 112,608 params, lr=0.003\n",
            "Param group 'band_6': 50,496 params, lr=0.01\n",
            "Param group 'shared': 58,862,208 params, lr=0.0003\n",
            "\n",
            "AKIRA Band Transformer test complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zM6GSMJ2tkK",
        "outputId": "1e983171-d4a1-4756-f0ed-1a43c062d3d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E949tnRHOmh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AKIRA Band Transformer - COLAB VERSION\n",
        "======================================\n",
        "\n",
        "AKIRA Project - Experiment 026\n",
        "Oscar Goldman - Shogu Research @ Datamutant.ai\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 1: INSTALL DEPENDENCIES\n",
        "# ==============================================================================\n",
        "\n",
        "!pip install transformers datasets tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaatiS2PQLT5",
        "outputId": "0ae9bbc0-3062-484e-e73b-c61f9d11b80b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 2: IMPORTS\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzqaw0XZPWT9",
        "outputId": "2a10dff2-4059-40bd-fcdc-d4501dc1de75"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Memory: 42.5 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 3: AKIRA CONFIG\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class AKIRAConfig:\n",
        "    \"\"\"AKIRA Band Transformer configuration.\"\"\"\n",
        "    vocab_size: int = 50257\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    max_seq_length: int = 512\n",
        "    dropout: float = 0.1\n",
        "    num_bands: int = 7\n",
        "    band_dims: Tuple[int, ...] = (128, 96, 80, 64, 64, 48, 32)\n",
        "    band_learning_rates: Tuple[float, ...] = (1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2)\n",
        "    mlp_expansion: int = 4\n",
        "    use_wormhole: bool = True\n",
        "    wormhole_threshold: float = 0.5\n",
        "\n",
        "    @property\n",
        "    def hidden_dim(self) -> int:\n",
        "        return sum(self.band_dims)\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        assert len(self.band_dims) == self.num_bands\n",
        "        assert len(self.band_learning_rates) == self.num_bands\n",
        "        assert self.hidden_dim % self.num_heads == 0\n",
        "        return True\n",
        "\n",
        "print(\"AKIRAConfig defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUaUGYVsPbzm",
        "outputId": "d56acf77-d147-4ba7-e55f-9cf8df2a43e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AKIRAConfig defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 4: BAND MLP\n",
        "# ==============================================================================\n",
        "\n",
        "class BandMLP(nn.Module):\n",
        "    \"\"\"MLP for a single frequency band.\"\"\"\n",
        "\n",
        "    def __init__(self, band_dim: int, expansion: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        hidden_dim = band_dim * expansion\n",
        "        self.fc1 = nn.Linear(band_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, band_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "print(\"BandMLP defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y4JlOPJPe_5",
        "outputId": "3dda40e7-7709-4771-b282-4ae16ad3ee98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BandMLP defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WormholeAttention(nn.Module):\n",
        "    \"\"\"Cross-band communication via gated attention.\"\"\"\n",
        "\n",
        "    def __init__(self, band_dims: Tuple[int, ...], threshold: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.band_dims = band_dims\n",
        "        self.num_bands = len(band_dims)\n",
        "        self.threshold = threshold\n",
        "        self.cross_dim = min(band_dims) // 2\n",
        "\n",
        "        self.cross_queries = nn.ModuleList([nn.Linear(dim, self.cross_dim) for dim in band_dims])\n",
        "        self.cross_keys = nn.ModuleList([nn.Linear(dim, self.cross_dim) for dim in band_dims])\n",
        "        self.cross_values = nn.ModuleList([nn.Linear(dim, self.cross_dim * 2) for dim in band_dims])\n",
        "        self.out_projs = nn.ModuleList([nn.Linear(self.cross_dim * 2, dim) for dim in band_dims])\n",
        "        self.gate = nn.ModuleList([nn.Linear(dim * 2, dim) for dim in band_dims])\n",
        "\n",
        "    def forward(self, bands: List[torch.Tensor], mask: Optional[torch.Tensor] = None) -> List[torch.Tensor]:\n",
        "        batch_size, seq_len = bands[0].shape[:2]\n",
        "        updated_bands = []\n",
        "\n",
        "        # Handle 4D mask from main attention (squeeze to 3D for our use)\n",
        "        if mask is not None and mask.dim() == 4:\n",
        "            mask = mask.squeeze(1)  # [batch, 1, seq, seq] -> [batch, seq, seq]\n",
        "\n",
        "        for i, band in enumerate(bands):\n",
        "            cross_info = torch.zeros(batch_size, seq_len, self.band_dims[i], device=band.device, dtype=band.dtype)\n",
        "\n",
        "            for j, other_band in enumerate(bands):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                q = self.cross_queries[i](band)\n",
        "                k = self.cross_keys[j](other_band)\n",
        "                v = self.cross_values[j](other_band)\n",
        "\n",
        "                scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.cross_dim)\n",
        "                if mask is not None:\n",
        "                    scores = scores.masked_fill(mask == 0, torch.finfo(scores.dtype).min)\n",
        "\n",
        "                attn = F.softmax(scores, dim=-1)\n",
        "                cross_output = torch.matmul(attn, v)\n",
        "                cross_output = self.out_projs[i](cross_output)\n",
        "\n",
        "                gate_input = torch.cat([band, cross_output], dim=-1)\n",
        "                gate_value = torch.sigmoid(self.gate[i](gate_input))\n",
        "                gate_value = (gate_value > self.threshold).float() * gate_value\n",
        "\n",
        "                cross_info = cross_info + gate_value * cross_output\n",
        "\n",
        "            updated_bands.append(band + cross_info / (self.num_bands - 1))\n",
        "\n",
        "        return updated_bands\n",
        "\n",
        "print(\"WormholeAttention defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk0P8SBEPijr",
        "outputId": "6bf9b161-971f-45ac-b094-402f6f66e878"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WormholeAttention defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 6: MULTI-HEAD ATTENTION\n",
        "# ==============================================================================\n",
        "\n",
        "class AKIRAMultiHeadAttention(nn.Module):\n",
        "    \"\"\"Standard multi-head attention.\"\"\"\n",
        "\n",
        "    def __init__(self, config: AKIRAConfig):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.head_dim = config.hidden_dim // config.num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.k_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.v_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, torch.finfo(scores.dtype).min)\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "print(\"AKIRAMultiHeadAttention defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCmyeZMnPmYk",
        "outputId": "76cdb258-25f2-47cb-da25-9af42c970884"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AKIRAMultiHeadAttention defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: AKIRA BAND LAYER\n",
        "# ==============================================================================\n",
        "\n",
        "class AKIRABandLayer(nn.Module):\n",
        "    \"\"\"Single AKIRA transformer layer with band structure.\"\"\"\n",
        "\n",
        "    def __init__(self, config: AKIRAConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(config.hidden_dim)\n",
        "        self.attention = AKIRAMultiHeadAttention(config)\n",
        "        self.band_norms = nn.ModuleList([nn.LayerNorm(dim) for dim in config.band_dims])\n",
        "        self.band_mlps = nn.ModuleList([BandMLP(dim, config.mlp_expansion, config.dropout) for dim in config.band_dims])\n",
        "\n",
        "        if config.use_wormhole:\n",
        "            self.wormhole = WormholeAttention(config.band_dims, config.wormhole_threshold)\n",
        "        else:\n",
        "            self.wormhole = None\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def split_bands(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        bands = []\n",
        "        start = 0\n",
        "        for dim in self.config.band_dims:\n",
        "            bands.append(x[..., start:start + dim])\n",
        "            start += dim\n",
        "        return bands\n",
        "\n",
        "    def concat_bands(self, bands: List[torch.Tensor]) -> torch.Tensor:\n",
        "        return torch.cat(bands, dim=-1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        # Attention\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(x, mask)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        # Split into bands\n",
        "        bands = self.split_bands(x)\n",
        "\n",
        "        # Per-band MLP\n",
        "        processed_bands = []\n",
        "        for i, (band, norm, mlp) in enumerate(zip(bands, self.band_norms, self.band_mlps)):\n",
        "            residual = band\n",
        "            band = norm(band)\n",
        "            band = mlp(band)\n",
        "            band = residual + band\n",
        "            processed_bands.append(band)\n",
        "\n",
        "        # Wormhole\n",
        "        if self.wormhole is not None:\n",
        "            processed_bands = self.wormhole(processed_bands, mask)\n",
        "\n",
        "        return self.concat_bands(processed_bands)\n",
        "\n",
        "print(\"AKIRABandLayer defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttjR8YWYPq-H",
        "outputId": "823c57e9-e099-42e9-9dc7-17f65083a8e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AKIRABandLayer defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 8: AKIRA BAND TRANSFORMER\n",
        "# ==============================================================================\n",
        "\n",
        "class AKIRABandTransformer(nn.Module):\n",
        "    \"\"\"Full AKIRA Band Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, config: AKIRAConfig):\n",
        "        super().__init__()\n",
        "        config.validate()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "        self.position_embedding = nn.Embedding(config.max_seq_length, config.hidden_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([AKIRABandLayer(config) for _ in range(config.num_layers)])\n",
        "        self.final_norm = nn.LayerNorm(config.hidden_dim)\n",
        "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def get_optimizer_param_groups(self) -> List[Dict]:\n",
        "        \"\"\"Get parameter groups with per-band learning rates.\"\"\"\n",
        "        band_params = {i: [] for i in range(self.config.num_bands)}\n",
        "\n",
        "        for layer in self.layers:\n",
        "            for i, mlp in enumerate(layer.band_mlps):\n",
        "                band_params[i].extend(mlp.parameters())\n",
        "            for i, norm in enumerate(layer.band_norms):\n",
        "                band_params[i].extend(norm.parameters())\n",
        "\n",
        "        param_groups = []\n",
        "        for band_idx, params in band_params.items():\n",
        "            param_groups.append({\n",
        "                'params': params,\n",
        "                'lr': self.config.band_learning_rates[band_idx],\n",
        "                'name': f'band_{band_idx}'\n",
        "            })\n",
        "\n",
        "        # Non-band parameters\n",
        "        non_band_params = []\n",
        "        non_band_params.extend(self.token_embedding.parameters())\n",
        "        non_band_params.extend(self.position_embedding.parameters())\n",
        "        non_band_params.extend(self.final_norm.parameters())\n",
        "        non_band_params.extend(self.lm_head.parameters())\n",
        "\n",
        "        for layer in self.layers:\n",
        "            non_band_params.extend(layer.norm1.parameters())\n",
        "            non_band_params.extend(layer.attention.parameters())\n",
        "            if layer.wormhole is not None:\n",
        "                non_band_params.extend(layer.wormhole.parameters())\n",
        "\n",
        "        param_groups.append({\n",
        "            'params': non_band_params,\n",
        "            'lr': self.config.band_learning_rates[3],\n",
        "            'name': 'shared'\n",
        "        })\n",
        "\n",
        "        return param_groups\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
        "        causal_mask = ~causal_mask\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            causal_mask = causal_mask & attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, causal_mask)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        output = {'logits': logits}\n",
        "\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, self.config.vocab_size),\n",
        "                shift_labels.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "            output['loss'] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"AKIRABandTransformer defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYCfUcROPviC",
        "outputId": "e285e319-da0a-46c0-f91e-0b765e6d2fec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AKIRABandTransformer defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 9: TEST MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING AKIRA MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "config = AKIRAConfig()\n",
        "model = AKIRABandTransformer(config)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Hidden dim: {config.hidden_dim}\")\n",
        "print(f\"Band dims: {config.band_dims}\")\n",
        "\n",
        "# Test forward pass\n",
        "batch_size, seq_len = 2, 64\n",
        "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "labels = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "\n",
        "output = model(input_ids, labels=labels)\n",
        "print(f\"Output shape: {output['logits'].shape}\")\n",
        "print(f\"Loss: {output['loss'].item():.4f}\")\n",
        "\n",
        "# Show param groups\n",
        "param_groups = model.get_optimizer_param_groups()\n",
        "for pg in param_groups:\n",
        "    print(f\"  {pg['name']}: {sum(p.numel() for p in pg['params']):,} params, lr={pg['lr']}\")\n",
        "\n",
        "print(\"\\nModel test complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jNdqMbAPykI",
        "outputId": "e4eb0669-edd5-4c56-8a7f-9e4c421844ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING AKIRA MODEL\n",
            "============================================================\n",
            "Model parameters: 60,972,672\n",
            "Hidden dim: 512\n",
            "Band dims: (128, 96, 80, 64, 64, 48, 32)\n",
            "Output shape: torch.Size([2, 64, 50257])\n",
            "Loss: 10.9278\n",
            "  band_0: 791,808 params, lr=1e-05\n",
            "  band_1: 446,400 params, lr=3e-05\n",
            "  band_2: 310,560 params, lr=0.0001\n",
            "  band_3: 199,296 params, lr=0.0003\n",
            "  band_4: 199,296 params, lr=0.001\n",
            "  band_5: 112,608 params, lr=0.003\n",
            "  band_6: 50,496 params, lr=0.01\n",
            "  shared: 58,862,208 params, lr=0.0003\n",
            "\n",
            "Model test complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 10: DATA LOADING\n",
        "# ==============================================================================\n",
        "\n",
        "def load_wikitext103(tokenizer, max_length: int = 512, split: str = \"train\"):\n",
        "    \"\"\"Load WikiText-103 dataset.\"\"\"\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    print(f\"Loading WikiText-103 {split}...\")\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=split)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "        return tokenized\n",
        "\n",
        "    dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
        "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "print(\"Data loading function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZtKbz35P1jf",
        "outputId": "8032ddd3-14bf-43b3-a020-11d46c606f2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 11: TRAINING LOOP\n",
        "# ==============================================================================\n",
        "\n",
        "def train_akira(\n",
        "    model,\n",
        "    train_loader,\n",
        "    eval_loader,\n",
        "    total_steps=10000,\n",
        "    eval_every=500,\n",
        "    log_every=50,\n",
        "    save_path=\"./akira_checkpoint\"\n",
        "):\n",
        "    \"\"\"Train AKIRA model.\"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer with per-band LRs\n",
        "    param_groups = model.get_optimizer_param_groups()\n",
        "    optimizer = AdamW(param_groups, weight_decay=0.01)\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-7)\n",
        "\n",
        "    # Mixed precision\n",
        "    scaler = torch.amp.GradScaler('cuda') if device == \"cuda\" else None\n",
        "\n",
        "    model.train()\n",
        "    train_iter = iter(train_loader)\n",
        "    train_losses = []\n",
        "    best_eval_loss = float('inf')\n",
        "\n",
        "    pbar = tqdm(range(total_steps), desc=\"Training\")\n",
        "\n",
        "    for step in pbar:\n",
        "        try:\n",
        "            batch = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(train_loader)\n",
        "            batch = next(train_iter)\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = output[\"loss\"]\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = output[\"loss\"]\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        if step % log_every == 0:\n",
        "            avg_loss = sum(train_losses[-log_every:]) / len(train_losses[-log_every:])\n",
        "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
        "\n",
        "        if step > 0 and step % eval_every == 0:\n",
        "            eval_loss = evaluate(model, eval_loader)\n",
        "            print(f\"\\nStep {step}: eval_loss = {eval_loss:.4f}, ppl = {math.exp(eval_loss):.2f}\")\n",
        "\n",
        "            if eval_loss < best_eval_loss:\n",
        "                best_eval_loss = eval_loss\n",
        "                torch.save(model.state_dict(), f\"{save_path}_best.pt\")\n",
        "                print(f\"  Saved best model!\")\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    torch.save(model.state_dict(), f\"{save_path}_final.pt\")\n",
        "    return model\n",
        "\n",
        "def evaluate(model, eval_loader, max_batches=50):\n",
        "    \"\"\"Evaluate model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            if num_batches >= max_batches:\n",
        "                break\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda') if device == \"cuda\" else torch.no_grad():\n",
        "                output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            total_loss += output[\"loss\"].item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "print(\"Training functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX5q9178P5Oo",
        "outputId": "60572a9e-6e66-431e-ddba-98e715caf790"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 12: RUN TRAINING\n",
        "# ==============================================================================\n",
        "\n",
        "# Load tokenizer\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load data (this takes a few minutes)\n",
        "train_dataset = load_wikitext103(tokenizer, max_length=256, split=\"train\")\n",
        "eval_dataset = load_wikitext103(tokenizer, max_length=256, split=\"validation\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "7c44ee567e914cbd8d8b5233d59a77da",
            "1645a02bfc9d4d7188db90f136324a20",
            "f358706bf3884acea7a5e494d6ddfcec",
            "b7210bdd1a254a5bb27bd5219c685d9b",
            "207fcee708074b968428b4878ccc778a",
            "b4a596354a6941e7bf04bf48ec1706d4",
            "04313bf3e7494ac79555c5c229aa3882",
            "2dcc0a03cbcc45b2b261cb3b3d554be5",
            "b0941f1beeb74e7e8f83a0b0f1962b53",
            "2b5e1e0d90534837b27d61c68246f2dc",
            "140dac182a384e09a761af8ef7f73a77",
            "498b7eba2ad742328cf7205ae47d8354",
            "e7000be65a114443a677a5b8120fa68b",
            "404e7f16de26493688bd6a682cba6c22",
            "c3d6453559394b49adc52e3730d9340b",
            "9c624420158149f9a41a74b0d0426683",
            "a592aab42bd747369444139ceffe8b8a",
            "1d0af4ecd25a43868eb1f1b27f0a00f0",
            "376233e958734ac6bb90bc3f363b62c2",
            "8558686527c148188b38c877800184a7",
            "3e7008fef2624645b267092498d81e81",
            "47114dffea6a43bc93f71f0b837c6608"
          ]
        },
        "id": "Y4zKz8VCP8Sd",
        "outputId": "2d1ac071-7d62-46cf-b0cb-b917709a601d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiText-103 train...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1165029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c44ee567e914cbd8d8b5233d59a77da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiText-103 validation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "498b7eba2ad742328cf7205ae47d8354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 1165029\n",
            "Eval samples: 2461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BASELINE TEST - Run this cell after AKIRA training\n",
        "# ==============================================================================\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class BaselineConfig:\n",
        "    vocab_size: int = 50257\n",
        "    num_layers: int = 6\n",
        "    hidden_dim: int = 512\n",
        "    num_heads: int = 8\n",
        "    mlp_expansion: int = 4\n",
        "    max_seq_length: int = 256\n",
        "    dropout: float = 0.1\n",
        "    def validate(self):\n",
        "        assert self.hidden_dim % self.num_heads == 0\n",
        "        return True\n",
        "\n",
        "class BaselineMHA(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.head_dim = config.hidden_dim // config.num_heads\n",
        "        self.q_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.k_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.v_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, S, _ = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, torch.finfo(scores.dtype).min)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, S, self.hidden_dim)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class BaselineMLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        h = config.hidden_dim * config.mlp_expansion\n",
        "        self.fc1 = nn.Linear(config.hidden_dim, h)\n",
        "        self.fc2 = nn.Linear(h, config.hidden_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.act = nn.GELU()\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fc2(self.dropout(self.act(self.fc1(x)))))\n",
        "\n",
        "class BaselineLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(config.hidden_dim)\n",
        "        self.attn = BaselineMHA(config)\n",
        "        self.norm2 = nn.LayerNorm(config.hidden_dim)\n",
        "        self.mlp = BaselineMLP(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class BaselineTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        config.validate()\n",
        "        self.config = config\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_length, config.hidden_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([BaselineLayer(config) for _ in range(config.num_layers)])\n",
        "        self.norm = nn.LayerNorm(config.hidden_dim)\n",
        "        self.head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
        "        self.apply(self._init)\n",
        "\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.zeros_(m.bias)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        B, S = input_ids.shape\n",
        "        pos = torch.arange(S, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "        x = self.dropout(self.tok_emb(input_ids) + self.pos_emb(pos))\n",
        "        mask = ~torch.triu(torch.ones(S, S, device=input_ids.device), diagonal=1).bool()\n",
        "        if attention_mask is not None:\n",
        "            mask = mask & attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        logits = self.head(self.norm(x))\n",
        "        out = {'logits': logits}\n",
        "        if labels is not None:\n",
        "            out['loss'] = F.cross_entropy(logits[..., :-1, :].reshape(-1, self.config.vocab_size),\n",
        "                                          labels[..., 1:].reshape(-1), ignore_index=-100)\n",
        "        return out\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE TRAINING - Compare with AKIRA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_config = BaselineConfig()\n",
        "baseline_model = BaselineTransformer(baseline_config)\n",
        "print(f\"Baseline params: {sum(p.numel() for p in baseline_model.parameters()):,}\")\n",
        "\n",
        "baseline_model = baseline_model.to(device)\n",
        "optimizer = AdamW(baseline_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=5000, eta_min=1e-7)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "baseline_model.train()\n",
        "train_iter = iter(train_loader)\n",
        "best_loss = float('inf')\n",
        "\n",
        "for step in tqdm(range(5000), desc=\"Baseline\"):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    ids = batch[\"input_ids\"].to(device)\n",
        "    mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        out = baseline_model(ids, attention_mask=mask, labels=labels)\n",
        "        loss = out[\"loss\"]\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(baseline_model.parameters(), 1.0)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    scheduler.step()\n",
        "\n",
        "    if step % 500 == 0 and step > 0:\n",
        "        baseline_model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i, eb in enumerate(eval_loader):\n",
        "                if i >= 50: break\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    eo = baseline_model(eb[\"input_ids\"].to(device), eb[\"attention_mask\"].to(device), eb[\"labels\"].to(device))\n",
        "                eval_loss += eo[\"loss\"].item()\n",
        "        eval_loss /= 50\n",
        "        print(f\"\\nStep {step}: eval_loss={eval_loss:.4f}, ppl={math.exp(eval_loss):.2f}\")\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "            print(\"  New best!\")\n",
        "        baseline_model.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"AKIRA best loss:    ~1.99 (ppl ~7.3)\")\n",
        "print(f\"Baseline best loss: {best_loss:.4f} (ppl {math.exp(best_loss):.2f})\")\n",
        "print(\"=\"*60)\n",
        "if best_loss > 1.99:\n",
        "    print(\"RESULT: AKIRA WINS - Band architecture helps!\")\n",
        "else:\n",
        "    print(\"RESULT: Baseline wins or tie - Need investigation\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pllsx67HPJO-",
        "outputId": "2bf35b11-accc-4b97-8dd4-f6ecc3e429ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASELINE TRAINING - Compare with AKIRA\n",
            "============================================================\n",
            "Baseline params: 70,509,568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  10%|█         | 503/5000 [00:21<10:22,  7.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 500: eval_loss=2.1135, ppl=8.28\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  20%|██        | 1004/5000 [00:43<07:17,  9.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1000: eval_loss=1.9898, ppl=7.31\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  30%|███       | 1505/5000 [01:05<06:31,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1500: eval_loss=1.9165, ppl=6.80\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  40%|████      | 2003/5000 [01:26<07:01,  7.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2000: eval_loss=1.8629, ppl=6.44\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  50%|█████     | 2504/5000 [01:48<04:33,  9.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2500: eval_loss=1.8189, ppl=6.17\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  60%|██████    | 3005/5000 [02:10<03:42,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3000: eval_loss=1.7772, ppl=5.91\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  70%|███████   | 3503/5000 [02:31<03:31,  7.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3500: eval_loss=1.7467, ppl=5.74\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  80%|████████  | 4004/5000 [02:53<01:49,  9.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4000: eval_loss=1.7308, ppl=5.64\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline:  90%|█████████ | 4505/5000 [03:14<00:54,  9.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4500: eval_loss=1.7230, ppl=5.60\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline: 100%|██████████| 5000/5000 [03:35<00:00, 23.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL COMPARISON\n",
            "============================================================\n",
            "AKIRA best loss:    ~1.99 (ppl ~7.3)\n",
            "Baseline best loss: 1.7230 (ppl 5.60)\n",
            "============================================================\n",
            "RESULT: Baseline wins or tie - Need investigation\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ABLATION: AKIRA WITHOUT WORMHOLE\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ABLATION: AKIRA with bands but NO wormhole attention\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Redefine config with wormhole disabled\n",
        "@dataclass\n",
        "class AKIRAConfigNoWormhole:\n",
        "    vocab_size: int = 50257\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    max_seq_length: int = 256\n",
        "    dropout: float = 0.1\n",
        "    num_bands: int = 7\n",
        "    band_dims: Tuple[int, ...] = (128, 96, 80, 64, 64, 48, 32)\n",
        "    band_learning_rates: Tuple[float, ...] = (1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2)\n",
        "    mlp_expansion: int = 4\n",
        "    use_wormhole: bool = False  # DISABLED\n",
        "    wormhole_threshold: float = 0.5\n",
        "\n",
        "    @property\n",
        "    def hidden_dim(self) -> int:\n",
        "        return sum(self.band_dims)\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        assert len(self.band_dims) == self.num_bands\n",
        "        assert len(self.band_learning_rates) == self.num_bands\n",
        "        assert self.hidden_dim % self.num_heads == 0\n",
        "        return True\n",
        "\n",
        "# Create model with NO wormhole\n",
        "ablation_config = AKIRAConfigNoWormhole()\n",
        "ablation_model = AKIRABandTransformer(ablation_config)\n",
        "print(f\"AKIRA (no wormhole) params: {sum(p.numel() for p in ablation_model.parameters()):,}\")\n",
        "\n",
        "ablation_model = ablation_model.to(device)\n",
        "param_groups = ablation_model.get_optimizer_param_groups()\n",
        "optimizer = AdamW(param_groups, weight_decay=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=5000, eta_min=1e-7)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "ablation_model.train()\n",
        "train_iter = iter(train_loader)\n",
        "best_loss = float('inf')\n",
        "\n",
        "for step in tqdm(range(5000), desc=\"AKIRA no-wormhole\"):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    ids = batch[\"input_ids\"].to(device)\n",
        "    mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        out = ablation_model(ids, attention_mask=mask, labels=labels)\n",
        "        loss = out[\"loss\"]\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(ablation_model.parameters(), 1.0)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    scheduler.step()\n",
        "\n",
        "    if step % 500 == 0 and step > 0:\n",
        "        ablation_model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i, eb in enumerate(eval_loader):\n",
        "                if i >= 50: break\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    eo = ablation_model(eb[\"input_ids\"].to(device), eb[\"attention_mask\"].to(device), eb[\"labels\"].to(device))\n",
        "                eval_loss += eo[\"loss\"].item()\n",
        "        eval_loss /= 50\n",
        "        print(f\"\\nStep {step}: eval_loss={eval_loss:.4f}, ppl={math.exp(eval_loss):.2f}\")\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "            print(\"  New best!\")\n",
        "        ablation_model.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ABLATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Baseline:              1.72 (ppl 5.60)\")\n",
        "print(f\"AKIRA + wormhole:      1.99 (ppl 7.30)\")\n",
        "print(f\"AKIRA no wormhole:     {best_loss:.4f} (ppl {math.exp(best_loss):.2f})\")\n",
        "print(\"=\"*60)\n",
        "if best_loss < 1.72:\n",
        "    print(\"Bands help! Wormhole was the problem.\")\n",
        "elif best_loss < 1.99:\n",
        "    print(\"Wormhole hurts, but bands still worse than baseline.\")\n",
        "else:\n",
        "    print(\"Bands themselves hurt performance.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpDpdz0xrd7I",
        "outputId": "0085b609-004c-45d3-9d57-b93188dd16da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ABLATION: AKIRA with bands but NO wormhole attention\n",
            "============================================================\n",
            "AKIRA (no wormhole) params: 60,015,616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  10%|█         | 502/5000 [00:41<23:30,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 500: eval_loss=2.1642, ppl=8.71\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  20%|██        | 1002/5000 [01:23<21:26,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1000: eval_loss=2.0257, ppl=7.58\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  30%|███       | 1502/5000 [02:05<18:33,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1500: eval_loss=1.9526, ppl=7.05\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  40%|████      | 2002/5000 [02:48<16:01,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2000: eval_loss=1.8967, ppl=6.66\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  50%|█████     | 2502/5000 [03:30<13:44,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2500: eval_loss=1.8562, ppl=6.40\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  60%|██████    | 3002/5000 [04:13<10:50,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3000: eval_loss=1.8184, ppl=6.16\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  70%|███████   | 3502/5000 [04:56<07:56,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3500: eval_loss=1.7945, ppl=6.02\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  80%|████████  | 4002/5000 [05:39<05:23,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4000: eval_loss=1.7778, ppl=5.92\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole:  90%|█████████ | 4502/5000 [06:22<02:36,  3.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4500: eval_loss=1.7713, ppl=5.88\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA no-wormhole: 100%|██████████| 5000/5000 [07:02<00:00, 11.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ABLATION RESULTS\n",
            "============================================================\n",
            "Baseline:              1.72 (ppl 5.60)\n",
            "AKIRA + wormhole:      1.99 (ppl 7.30)\n",
            "AKIRA no wormhole:     1.7713 (ppl 5.88)\n",
            "============================================================\n",
            "Wormhole hurts, but bands still worse than baseline.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ABLATION 2: AKIRA bands with UNIFORM learning rate\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ABLATION 2: AKIRA bands with UNIFORM LR (no wormhole)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@dataclass\n",
        "class AKIRAConfigUniformLR:\n",
        "    vocab_size: int = 50257\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    max_seq_length: int = 256\n",
        "    dropout: float = 0.1\n",
        "    num_bands: int = 7\n",
        "    band_dims: Tuple[int, ...] = (128, 96, 80, 64, 64, 48, 32)\n",
        "    band_learning_rates: Tuple[float, ...] = (3e-4, 3e-4, 3e-4, 3e-4, 3e-4, 3e-4, 3e-4)  # ALL SAME\n",
        "    mlp_expansion: int = 4\n",
        "    use_wormhole: bool = False\n",
        "    wormhole_threshold: float = 0.5\n",
        "\n",
        "    @property\n",
        "    def hidden_dim(self) -> int:\n",
        "        return sum(self.band_dims)\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        assert len(self.band_dims) == self.num_bands\n",
        "        assert len(self.band_learning_rates) == self.num_bands\n",
        "        assert self.hidden_dim % self.num_heads == 0\n",
        "        return True\n",
        "\n",
        "ablation2_config = AKIRAConfigUniformLR()\n",
        "ablation2_model = AKIRABandTransformer(ablation2_config)\n",
        "print(f\"AKIRA (uniform LR) params: {sum(p.numel() for p in ablation2_model.parameters()):,}\")\n",
        "\n",
        "ablation2_model = ablation2_model.to(device)\n",
        "param_groups = ablation2_model.get_optimizer_param_groups()\n",
        "optimizer = AdamW(param_groups, weight_decay=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=5000, eta_min=1e-7)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "ablation2_model.train()\n",
        "train_iter = iter(train_loader)\n",
        "best_loss = float('inf')\n",
        "\n",
        "for step in tqdm(range(5000), desc=\"AKIRA uniform-LR\"):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    ids = batch[\"input_ids\"].to(device)\n",
        "    mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        out = ablation2_model(ids, attention_mask=mask, labels=labels)\n",
        "        loss = out[\"loss\"]\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(ablation2_model.parameters(), 1.0)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    scheduler.step()\n",
        "\n",
        "    if step % 500 == 0 and step > 0:\n",
        "        ablation2_model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i, eb in enumerate(eval_loader):\n",
        "                if i >= 50: break\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    eo = ablation2_model(eb[\"input_ids\"].to(device), eb[\"attention_mask\"].to(device), eb[\"labels\"].to(device))\n",
        "                eval_loss += eo[\"loss\"].item()\n",
        "        eval_loss /= 50\n",
        "        print(f\"\\nStep {step}: eval_loss={eval_loss:.4f}, ppl={math.exp(eval_loss):.2f}\")\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "            print(\"  New best!\")\n",
        "        ablation2_model.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FULL ABLATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Baseline:                    1.72 (ppl 5.60)\")\n",
        "print(f\"AKIRA bands + uniform LR:    {best_loss:.4f} (ppl {math.exp(best_loss):.2f})\")\n",
        "print(f\"AKIRA bands + diff LR:       1.77 (ppl 5.88)\")\n",
        "print(f\"AKIRA bands + diff LR + wh:  1.99 (ppl 7.30)\")\n",
        "print(\"=\"*60)\n",
        "if best_loss < 1.72:\n",
        "    print(\"FINDING: Band structure helps with uniform LR!\")\n",
        "elif best_loss < 1.77:\n",
        "    print(\"FINDING: Differential LRs hurt! Uniform is better.\")\n",
        "else:\n",
        "    print(\"FINDING: Band structure itself is the issue.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LygOGeRYumxH",
        "outputId": "5a875061-2f64-412c-bf1c-03eeb3b25f8c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ABLATION 2: AKIRA bands with UNIFORM LR (no wormhole)\n",
            "============================================================\n",
            "AKIRA (uniform LR) params: 60,015,616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  10%|█         | 503/5000 [00:52<18:28,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 500: eval_loss=2.1533, ppl=8.61\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  20%|██        | 1003/5000 [01:35<16:48,  3.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1000: eval_loss=2.0275, ppl=7.60\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  30%|███       | 1503/5000 [02:17<14:24,  4.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1500: eval_loss=1.9527, ppl=7.05\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  40%|████      | 2003/5000 [02:59<12:24,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2000: eval_loss=1.9004, ppl=6.69\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  50%|█████     | 2503/5000 [03:40<10:15,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2500: eval_loss=1.8524, ppl=6.37\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  60%|██████    | 3003/5000 [04:22<08:10,  4.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3000: eval_loss=1.8194, ppl=6.17\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  70%|███████   | 3503/5000 [05:04<06:11,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3500: eval_loss=1.7963, ppl=6.03\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  80%|████████  | 4003/5000 [05:46<04:06,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4000: eval_loss=1.7832, ppl=5.95\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR:  90%|█████████ | 4503/5000 [06:28<02:02,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4500: eval_loss=1.7758, ppl=5.90\n",
            "  New best!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AKIRA uniform-LR: 100%|██████████| 5000/5000 [07:08<00:00, 11.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FULL ABLATION RESULTS\n",
            "============================================================\n",
            "Baseline:                    1.72 (ppl 5.60)\n",
            "AKIRA bands + uniform LR:    1.7758 (ppl 5.90)\n",
            "AKIRA bands + diff LR:       1.77 (ppl 5.88)\n",
            "AKIRA bands + diff LR + wh:  1.99 (ppl 7.30)\n",
            "============================================================\n",
            "FINDING: Band structure itself is the issue.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AKIRA Spectral Belief Machine - V2 CORRECT IMPLEMENTATION\n",
        "==========================================================\n",
        "\n",
        "This implements the ACTUAL theory from the architecture documents:\n",
        "1. Learnable bandpass filters for spectral decomposition\n",
        "2. 7 spectral bands + 1 temporal band = 8 total\n",
        "3. TEMPORAL wormhole with differential windows per band (Heisenberg)\n",
        "   - Band 0 sees 128 tokens back, Band 6 sees 4 tokens back\n",
        "   - Complementary pairs (0-6, 1-5, 2-4) exchange across time scales\n",
        "4. Differential processing modes per band (Geometric/Hybrid/Reactive)\n",
        "5. Proper temporal band with causal attention\n",
        "\n",
        "AKIRA Project - Experiment 026 v2\n",
        "Oscar Goldman - Shogu Research Group @ Datamutant.ai\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 1: INSTALL\n",
        "# ==============================================================================\n",
        "\n",
        "# !pip install transformers datasets tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 2: IMPORTS\n",
        "# ==============================================================================\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHcYmqRTwixG",
        "outputId": "fca59c03-a10b-46da-8170-71824d59ea1f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: SPECTRAL BELIEF MACHINE CONFIG\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SpectralConfig:\n",
        "    \"\"\"Configuration for Spectral Belief Machine.\"\"\"\n",
        "    vocab_size: int = 50257\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    embed_dim: int = 512  # Must be divisible by 8 for 8 bands\n",
        "    max_seq_length: int = 256\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # 7 spectral bands + 1 temporal = 8\n",
        "    num_spectral_bands: int = 7\n",
        "    num_total_bands: int = 8\n",
        "\n",
        "    # Differential learning rates per band (from theory)\n",
        "    # Band 0 (DC) slowest -> Band 6 fastest, Band 7 (temporal) medium\n",
        "    # Tightened to 100x range (was 1000x) for training stability\n",
        "    band_learning_rates: Tuple[float, ...] = (\n",
        "        3e-5,   # Band 0: DC - identity, eternal patterns\n",
        "        5e-5,   # Band 1: Coarse structure\n",
        "        1e-4,   # Band 2: Medium structure\n",
        "        3e-4,   # Band 3: Transitions (bridge band)\n",
        "        5e-4,   # Band 4: Fine structure\n",
        "        1e-3,   # Band 5: Textures\n",
        "        3e-3,   # Band 6: Edges, details\n",
        "        3e-4,   # Band 7: Temporal - medium rate\n",
        "    )\n",
        "\n",
        "    # Wormhole config\n",
        "    wormhole_top_k: int = 8  # Sparse selection\n",
        "    wormhole_threshold: float = 0.5\n",
        "\n",
        "    @property\n",
        "    def dim_per_band(self) -> int:\n",
        "        return self.embed_dim // self.num_total_bands  # 512/8 = 64\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        assert self.embed_dim % self.num_total_bands == 0, \"embed_dim must be divisible by 8\"\n",
        "        assert self.dim_per_band % 8 == 0, \"dim_per_band must be divisible by 8 for Tensor Cores\"\n",
        "        return True\n",
        "\n",
        "print(\"SpectralConfig defined\")\n",
        "print(f\"  embed_dim=512 -> dim_per_band={512//8}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teO-Imfb9Jpx",
        "outputId": "ff0ee2b3-060c-47eb-e69c-1f07986ec234"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectralConfig defined\n",
            "  embed_dim=512 -> dim_per_band=64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: SPECTRAL DECOMPOSER (Learnable Bandpass Filters)\n",
        "# ==============================================================================\n",
        "\n",
        "class SpectralDecomposer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decomposes input into 7 frequency bands using learnable bandpass-like filters.\n",
        "\n",
        "    Each band has a characteristic \"frequency response\" learned via convolutions\n",
        "    with different kernel sizes (small = high freq, large = low freq).\n",
        "\n",
        "    This captures the spirit of FFT decomposition in a differentiable way.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_bands: int = 7):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_bands = num_bands\n",
        "        self.dim_per_band = embed_dim // 8  # 64 for embed_dim=512\n",
        "\n",
        "        # Different \"kernel sizes\" for different frequency bands\n",
        "        # Band 0 (DC): largest kernel (captures global structure)\n",
        "        # Band 6 (high freq): smallest kernel (captures local details)\n",
        "        kernel_sizes = [15, 11, 9, 7, 5, 3, 1]  # Decreasing for higher bands\n",
        "\n",
        "        # Bandpass-inspired projections\n",
        "        # Each band uses: input projection + 1D conv (along embedding) + output projection\n",
        "        self.input_projs = nn.ModuleList([\n",
        "            nn.Linear(embed_dim, embed_dim) for _ in range(num_bands)\n",
        "        ])\n",
        "\n",
        "        # 1D convolutions along sequence dimension to capture different scales\n",
        "        self.band_convs = nn.ModuleList([\n",
        "            nn.Conv1d(embed_dim, embed_dim, kernel_size=k, padding=k//2, groups=embed_dim)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        # Output projections to band dimension\n",
        "        self.output_projs = nn.ModuleList([\n",
        "            nn.Linear(embed_dim, self.dim_per_band) for _ in range(num_bands)\n",
        "        ])\n",
        "\n",
        "        # Learnable band weights (for soft frequency selection)\n",
        "        self.band_gates = nn.ParameterList([\n",
        "            nn.Parameter(torch.ones(embed_dim) * (0.5 + 0.1 * i))  # Higher bands = more gate\n",
        "            for i in range(num_bands)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Decompose input into frequency bands.\n",
        "\n",
        "        Args:\n",
        "            x: (batch, seq_len, embed_dim)\n",
        "\n",
        "        Returns:\n",
        "            Dict of 7 band tensors, each (batch, seq_len, dim_per_band)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        bands = {}\n",
        "\n",
        "        for band_idx in range(self.num_bands):\n",
        "            # Input projection\n",
        "            h = self.input_projs[band_idx](x)\n",
        "\n",
        "            # Apply bandpass-like convolution (along sequence dimension)\n",
        "            # Transpose for conv1d: (B, T, D) -> (B, D, T)\n",
        "            h_t = h.transpose(1, 2)\n",
        "            h_conv = self.band_convs[band_idx](h_t)\n",
        "            h = h_conv.transpose(1, 2)  # Back to (B, T, D)\n",
        "\n",
        "            # Soft frequency gating\n",
        "            gate = torch.sigmoid(self.band_gates[band_idx])\n",
        "            h = h * gate.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            # For low bands: emphasize what remains after high-pass\n",
        "            # For high bands: emphasize the convolution output directly\n",
        "            if band_idx < 3:\n",
        "                # Low freq: residual connection (captures what conv smooths)\n",
        "                h = x - h + h  # Essentially: original minus high-freq + current\n",
        "\n",
        "            # Project to band dimension\n",
        "            bands[band_idx] = self.output_projs[band_idx](h)\n",
        "\n",
        "        return bands\n",
        "\n",
        "print(\"SpectralDecomposer defined (learnable bandpass filters)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzCP3OUW9GkA",
        "outputId": "86ed3b66-6ed5-41b3-b586-4bf77f3ee946"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectralDecomposer defined (learnable bandpass filters)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 5: SPECTRAL RECONSTRUCTOR\n",
        "# ==============================================================================\n",
        "\n",
        "class SpectralReconstructor(nn.Module):\n",
        "    \"\"\"Reconstructs signal from 7 spectral bands.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_bands: int = 7):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_bands = num_bands\n",
        "        self.dim_per_band = embed_dim // 8  # 64 for embed_dim=512\n",
        "\n",
        "        # Learnable reconstruction from each band\n",
        "        self.band_recon = nn.ModuleList([\n",
        "            nn.Linear(self.dim_per_band, embed_dim) for _ in range(num_bands)\n",
        "        ])\n",
        "\n",
        "        # Final mixing layer (7 bands * embed_dim -> embed_dim)\n",
        "        self.mix = nn.Linear(embed_dim * num_bands, embed_dim)\n",
        "\n",
        "    def forward(self, bands: Dict[int, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Reconstruct full signal from bands.\n",
        "\n",
        "        Args:\n",
        "            bands: Dict of 7 band tensors, each (batch, seq, dim_per_band)\n",
        "\n",
        "        Returns:\n",
        "            (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Project each band back to full dimension\n",
        "        reconstructed = []\n",
        "        for band_idx in range(self.num_bands):\n",
        "            if band_idx in bands:\n",
        "                reconstructed.append(self.band_recon[band_idx](bands[band_idx]))\n",
        "\n",
        "        # Concatenate and mix\n",
        "        if reconstructed:\n",
        "            concat = torch.cat(reconstructed, dim=-1)\n",
        "            return self.mix(concat)\n",
        "        else:\n",
        "            # Fallback\n",
        "            B, T, _ = list(bands.values())[0].shape\n",
        "            return torch.zeros(B, T, self.embed_dim, device=list(bands.values())[0].device)\n",
        "\n",
        "print(\"SpectralReconstructor defined\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyy03_o-9DUu",
        "outputId": "cfa31ed0-ab48-4119-c38f-91055977ae5a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectralReconstructor defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 6: BAND PROCESSORS (Different modes per band)\n",
        "# ==============================================================================\n",
        "\n",
        "class GeometricProcessor(nn.Module):\n",
        "    \"\"\"\n",
        "    Geometric/belief processing for low-frequency bands (0-2).\n",
        "    Slow, deliberate, structure-preserving.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # Smaller expansion, more residual\n",
        "        self.fc1 = nn.Linear(dim, dim * 2)\n",
        "        self.fc2 = nn.Linear(dim * 2, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gate = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        h = F.gelu(self.fc1(x))\n",
        "        h = self.dropout(h)\n",
        "        h = self.fc2(h)\n",
        "        # Gated residual - preserves structure\n",
        "        gate = torch.sigmoid(self.gate(residual))\n",
        "        return residual + gate * h\n",
        "\n",
        "\n",
        "class HybridProcessor(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid processing for mid-frequency bands (3-4).\n",
        "    Balanced between structure and adaptation.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, dim * 3)\n",
        "        self.fc2 = nn.Linear(dim * 3, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return residual + x\n",
        "\n",
        "\n",
        "class ReactiveProcessor(nn.Module):\n",
        "    \"\"\"\n",
        "    Reactive/energy processing for high-frequency bands (5-6).\n",
        "    Fast, adaptive, detail-sensitive.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # Larger expansion, more expressive\n",
        "        self.fc1 = nn.Linear(dim, dim * 4)\n",
        "        self.fc2 = nn.Linear(dim * 4, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return residual + x\n",
        "\n",
        "print(\"Band processors defined (Geometric/Hybrid/Reactive)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRKtVW_9AjR",
        "outputId": "f0922224-052f-4907-8df2-176917ded864"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Band processors defined (Geometric/Hybrid/Reactive)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: TEMPORAL WORMHOLE ATTENTION\n",
        "# ==============================================================================\n",
        "\n",
        "class TemporalWormhole(nn.Module):\n",
        "    \"\"\"\n",
        "    Wormhole attention with DIFFERENTIAL TEMPORAL WINDOWS.\n",
        "\n",
        "    Key insight from Heisenberg uncertainty:\n",
        "    - Low freq bands: good frequency resolution, poor time resolution -> see FAR back\n",
        "    - High freq bands: poor frequency resolution, good time resolution -> see RECENT only\n",
        "\n",
        "    Complementary pairs:\n",
        "    - Band 0 (sees t-128) <-> Band 6 (sees t-4)\n",
        "    - Band 1 (sees t-64)  <-> Band 5 (sees t-8)\n",
        "    - Band 2 (sees t-32)  <-> Band 4 (sees t-16)\n",
        "    - Band 3 (bridge) -> queries all with medium window\n",
        "\n",
        "    The wormhole lets slow patterns inform fast patterns ACROSS TIME.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, max_seq_len: int = 256):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Temporal windows per band (positions to look back)\n",
        "        # Band 0 sees far, Band 6 sees near\n",
        "        self.temporal_windows = [128, 64, 32, 16, 16, 8, 4]\n",
        "\n",
        "        # Complementary pairs: (low_band, high_band)\n",
        "        self.pairs = [(0, 6), (1, 5), (2, 4)]\n",
        "\n",
        "        # Per-band temporal attention (simplified: just Q, K, V per band)\n",
        "        self.q_projs = nn.ModuleList([nn.Linear(dim, dim) for _ in range(7)])\n",
        "        self.k_projs = nn.ModuleList([nn.Linear(dim, dim) for _ in range(7)])\n",
        "        self.v_projs = nn.ModuleList([nn.Linear(dim, dim) for _ in range(7)])\n",
        "\n",
        "        # Cross-band projection for wormhole transfer\n",
        "        self.cross_projs = nn.ModuleDict({\n",
        "            f\"{low}_{high}\": nn.Linear(dim, dim)\n",
        "            for low, high in self.pairs\n",
        "        })\n",
        "        self.cross_projs_rev = nn.ModuleDict({\n",
        "            f\"{low}_{high}\": nn.Linear(dim, dim)\n",
        "            for low, high in self.pairs\n",
        "        })\n",
        "\n",
        "        # Output gates per band\n",
        "        self.gates = nn.ModuleList([nn.Linear(dim * 2, dim) for _ in range(7)])\n",
        "\n",
        "        # Bridge band (3) projections\n",
        "        self.bridge_proj = nn.Linear(dim, dim)\n",
        "\n",
        "        # Precompute causal masks with different windows\n",
        "        self._precompute_masks(max_seq_len)\n",
        "\n",
        "    def _precompute_masks(self, max_len: int):\n",
        "        \"\"\"Create causal masks with different temporal windows per band.\"\"\"\n",
        "        for band_idx, window in enumerate(self.temporal_windows):\n",
        "            # Causal mask: can only see past positions within window\n",
        "            mask = torch.ones(max_len, max_len)\n",
        "            for i in range(max_len):\n",
        "                # Can see from max(0, i-window) to i (inclusive)\n",
        "                start = max(0, i - window + 1)\n",
        "                mask[i, start:i+1] = 0\n",
        "            # 1 = masked (can't attend), 0 = can attend\n",
        "            self.register_buffer(f\"mask_{band_idx}\", mask.bool())\n",
        "\n",
        "    def forward(self, bands: Dict[int, torch.Tensor]) -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Apply temporal wormhole attention.\n",
        "\n",
        "        Each band attends within its temporal window, then\n",
        "        complementary pairs exchange information across time scales.\n",
        "        \"\"\"\n",
        "        outputs = {k: v.clone() for k, v in bands.items()}\n",
        "        B, T, D = bands[0].shape\n",
        "\n",
        "        # Step 1: Each band does self-attention within its temporal window\n",
        "        band_contexts = {}\n",
        "        for band_idx in range(7):\n",
        "            band = bands[band_idx]\n",
        "            q = self.q_projs[band_idx](band)\n",
        "            k = self.k_projs[band_idx](band)\n",
        "            v = self.v_projs[band_idx](band)\n",
        "\n",
        "            # Attention with band-specific temporal mask\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(D)\n",
        "            mask = getattr(self, f\"mask_{band_idx}\")[:T, :T]\n",
        "            scores = scores.masked_fill(mask.unsqueeze(0), torch.finfo(scores.dtype).min)\n",
        "\n",
        "            attn = F.softmax(scores, dim=-1)\n",
        "            context = torch.matmul(attn, v)\n",
        "            band_contexts[band_idx] = context\n",
        "\n",
        "        # Step 2: Wormhole transfer between complementary pairs\n",
        "        for low, high in self.pairs:\n",
        "            key = f\"{low}_{high}\"\n",
        "\n",
        "            # Low band (far past) informs high band (recent)\n",
        "            # \"What patterns from the distant past are relevant now?\"\n",
        "            low_context = band_contexts[low]  # Has info from far back\n",
        "            high_band = bands[high]           # Current high-freq state\n",
        "\n",
        "            # Project low context to high band space\n",
        "            wormhole_to_high = self.cross_projs[key](low_context)\n",
        "\n",
        "            # High band (recent) informs low band\n",
        "            # \"What recent details should update the slow structure?\"\n",
        "            high_context = band_contexts[high]\n",
        "            wormhole_to_low = self.cross_projs_rev[key](high_context)\n",
        "\n",
        "            # Gated updates\n",
        "            gate_high = torch.sigmoid(self.gates[high](\n",
        "                torch.cat([high_band, wormhole_to_high], dim=-1)\n",
        "            ))\n",
        "            gate_low = torch.sigmoid(self.gates[low](\n",
        "                torch.cat([bands[low], wormhole_to_low], dim=-1)\n",
        "            ))\n",
        "\n",
        "            outputs[high] = outputs[high] + gate_high * wormhole_to_high\n",
        "            outputs[low] = outputs[low] + gate_low * wormhole_to_low\n",
        "\n",
        "        # Step 3: Bridge band (3) aggregates from all others\n",
        "        bridge = bands[3]\n",
        "        bridge_context = band_contexts[3]\n",
        "\n",
        "        # Average contexts from all other bands\n",
        "        other_contexts = torch.stack([\n",
        "            band_contexts[i] for i in range(7) if i != 3\n",
        "        ], dim=0).mean(dim=0)\n",
        "\n",
        "        bridge_info = self.bridge_proj(other_contexts)\n",
        "        gate_bridge = torch.sigmoid(self.gates[3](\n",
        "            torch.cat([bridge, bridge_info], dim=-1)\n",
        "        ))\n",
        "        outputs[3] = outputs[3] + gate_bridge * bridge_info\n",
        "\n",
        "        return outputs\n",
        "\n",
        "print(\"TemporalWormhole defined (differential temporal windows per band)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WmtyKTD89DY",
        "outputId": "86ab2fe7-c5ef-40de-c74d-c7e67fc71962"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TemporalWormhole defined (differential temporal windows per band)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 8: TEMPORAL BAND (Band 7 - Causal Attention)\n",
        "# ==============================================================================\n",
        "\n",
        "class TemporalBand(nn.Module):\n",
        "    \"\"\"\n",
        "    Band 7: Temporal attention across sequence positions.\n",
        "\n",
        "    Unlike spectral bands (which process features at one time step),\n",
        "    this band processes the sequence dimension with CAUSAL masking.\n",
        "\n",
        "    This is orthogonal to spectral processing (Heisenberg).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, max_seq_len: int, num_heads: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, dim)\n",
        "        self.v_proj = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Causal mask\n",
        "        self.register_buffer(\n",
        "            \"causal_mask\",\n",
        "            torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Causal temporal attention.\n",
        "\n",
        "        Args:\n",
        "            x: (batch, seq_len, dim) - aggregated from spectral bands\n",
        "\n",
        "        Returns:\n",
        "            (batch, seq_len, dim) - temporally contextualized\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Multi-head attention\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention over TIME (causal)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        scores = scores.masked_fill(self.causal_mask[:T, :T].unsqueeze(0).unsqueeze(0),\n",
        "                                     torch.finfo(scores.dtype).min)\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return residual + out\n",
        "\n",
        "print(\"TemporalBand defined (Band 7 - causal attention over time)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Ajj7K-84xZ",
        "outputId": "87c90e11-8021-4918-b842-6e41a3626cfe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TemporalBand defined (Band 7 - causal attention over time)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 9: SPECTRAL ATTENTION (For bands 0-6)\n",
        "# ==============================================================================\n",
        "\n",
        "class SpectralAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard attention for spectral bands (non-causal).\n",
        "\n",
        "    Unlike temporal attention, spectral attention can see all positions\n",
        "    because frequency content is inherently delocalized in time.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, num_heads: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, dim)\n",
        "        self.v_proj = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        B, T, D = x.shape\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, torch.finfo(scores.dtype).min)\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return residual + out\n",
        "\n",
        "print(\"SpectralAttention defined (non-causal for frequency bands)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc68fDzL810A",
        "outputId": "623566be-b0eb-46d0-d3fb-c0b22c79fe27"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectralAttention defined (non-causal for frequency bands)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 10: SPECTRAL BELIEF MACHINE LAYER\n",
        "# ==============================================================================\n",
        "\n",
        "class SpectralBeliefLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete 7+1 architecture layer.\n",
        "\n",
        "    7 spectral bands for spatial/feature processing.\n",
        "    1 temporal band for sequence processing.\n",
        "    Temporal wormhole attention with differential windows per band.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: SpectralConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        dim = config.dim_per_band\n",
        "\n",
        "        # Spectral decomposition\n",
        "        self.decomposer = SpectralDecomposer(config.embed_dim, config.num_spectral_bands)\n",
        "\n",
        "        # Per-band attention (spectral bands use non-causal)\n",
        "        self.band_attns = nn.ModuleList([\n",
        "            SpectralAttention(dim, num_heads=2, dropout=config.dropout)\n",
        "            for _ in range(7)\n",
        "        ])\n",
        "\n",
        "        # Per-band processors (different modes)\n",
        "        self.band_processors = nn.ModuleList([\n",
        "            GeometricProcessor(dim, config.dropout),   # Band 0: DC\n",
        "            GeometricProcessor(dim, config.dropout),   # Band 1: Coarse\n",
        "            GeometricProcessor(dim, config.dropout),   # Band 2: Medium\n",
        "            HybridProcessor(dim, config.dropout),      # Band 3: Bridge\n",
        "            HybridProcessor(dim, config.dropout),      # Band 4: Fine\n",
        "            ReactiveProcessor(dim, config.dropout),    # Band 5: Texture\n",
        "            ReactiveProcessor(dim, config.dropout),    # Band 6: Edges\n",
        "        ])\n",
        "\n",
        "        # Wormhole attention (temporal + complementary)\n",
        "        self.wormhole = TemporalWormhole(dim, config.max_seq_length)\n",
        "\n",
        "        # Temporal band (Band 7)\n",
        "        self.temporal_band = TemporalBand(dim, config.max_seq_length, num_heads=2, dropout=config.dropout)\n",
        "\n",
        "        # Aggregator: combine spectral bands for temporal processing\n",
        "        self.temporal_agg = nn.Linear(config.embed_dim - dim, dim)  # 7 bands -> 1\n",
        "\n",
        "        # Reconstructor\n",
        "        self.reconstructor = SpectralReconstructor(config.embed_dim, config.num_spectral_bands)\n",
        "\n",
        "        # Final output projection (include temporal)\n",
        "        self.output_proj = nn.Linear(config.embed_dim + dim, config.embed_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Full 7+1 processing.\n",
        "\n",
        "        Args:\n",
        "            x: (batch, seq_len, embed_dim)\n",
        "\n",
        "        Returns:\n",
        "            (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        # 1. Spectral decomposition into 7 bands\n",
        "        bands = self.decomposer(x)\n",
        "\n",
        "        # 2. Per-band attention + processing (parallel)\n",
        "        for i in range(7):\n",
        "            bands[i] = self.band_attns[i](bands[i])\n",
        "            bands[i] = self.band_processors[i](bands[i])\n",
        "\n",
        "        # 3. Wormhole cross-band communication\n",
        "        bands = self.wormhole(bands)\n",
        "\n",
        "        # 4. Prepare temporal input (aggregate spectral bands)\n",
        "        spectral_concat = torch.cat([bands[i] for i in range(7)], dim=-1)\n",
        "        temporal_input = self.temporal_agg(spectral_concat)\n",
        "\n",
        "        # 5. Temporal band processing (causal)\n",
        "        temporal_output = self.temporal_band(temporal_input)\n",
        "\n",
        "        # 6. Reconstruct from spectral bands\n",
        "        spectral_output = self.reconstructor(bands)\n",
        "\n",
        "        # 7. Combine spectral + temporal\n",
        "        combined = torch.cat([spectral_output, temporal_output], dim=-1)\n",
        "        output = self.output_proj(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"SpectralBeliefLayer defined (full 7+1 architecture)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mul5hK938yyJ",
        "outputId": "b42a5364-9205-40be-8d2e-f09df91a427f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectralBeliefLayer defined (full 7+1 architecture)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 11: SPECTRAL BELIEF MACHINE (Full Model)\n",
        "# ==============================================================================\n",
        "\n",
        "class SpectralBeliefMachine(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Spectral Belief Machine.\n",
        "\n",
        "    Implements the full theory:\n",
        "    - Learnable bandpass spectral decomposition\n",
        "    - 7 spectral + 1 temporal bands\n",
        "    - Temporal wormhole with differential windows (Heisenberg)\n",
        "    - Differential processing modes (Geometric/Hybrid/Reactive)\n",
        "    - Complementary cross-band communication\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: SpectralConfig):\n",
        "        super().__init__()\n",
        "        config.validate()\n",
        "        self.config = config\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.position_embedding = nn.Embedding(config.max_seq_length, config.embed_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Spectral Belief Layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            SpectralBeliefLayer(config) for _ in range(config.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output\n",
        "        self.final_norm = nn.LayerNorm(config.embed_dim)\n",
        "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def get_optimizer_param_groups(self) -> List[Dict]:\n",
        "        \"\"\"Get parameter groups with differential learning rates per band.\"\"\"\n",
        "        # Group parameters by band\n",
        "        band_params = {i: [] for i in range(8)}\n",
        "        other_params = []\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            assigned = False\n",
        "            for band_idx in range(7):\n",
        "                if f'band_processors.{band_idx}' in name or f'band_attns.{band_idx}' in name:\n",
        "                    band_params[band_idx].append(param)\n",
        "                    assigned = True\n",
        "                    break\n",
        "            if 'temporal_band' in name:\n",
        "                band_params[7].append(param)\n",
        "                assigned = True\n",
        "            if not assigned:\n",
        "                other_params.append(param)\n",
        "\n",
        "        param_groups = []\n",
        "        for band_idx in range(8):\n",
        "            if band_params[band_idx]:\n",
        "                param_groups.append({\n",
        "                    'params': band_params[band_idx],\n",
        "                    'lr': self.config.band_learning_rates[band_idx],\n",
        "                    'name': f'band_{band_idx}'\n",
        "                })\n",
        "\n",
        "        if other_params:\n",
        "            param_groups.append({\n",
        "                'params': other_params,\n",
        "                'lr': self.config.band_learning_rates[3],  # Bridge rate\n",
        "                'name': 'shared'\n",
        "            })\n",
        "\n",
        "        return param_groups\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        B, T = input_ids.shape\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Process through layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Output\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        output = {'logits': logits}\n",
        "\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, self.config.vocab_size),\n",
        "                shift_labels.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "            output['loss'] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"SpectralBeliefMachine defined (complete model)\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9k7V6kJ8uiT",
        "outputId": "33ad9f72-0413-4803-bc27-6cff7e15fece"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectralBeliefMachine defined (complete model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 12: TEST MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING SPECTRAL BELIEF MACHINE V2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "config = SpectralConfig()\n",
        "model = SpectralBeliefMachine(config)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Embed dim: {config.embed_dim}\")\n",
        "print(f\"Dim per band: {config.dim_per_band}\")\n",
        "print(f\"Bands: 7 spectral + 1 temporal = 8\")\n",
        "\n",
        "# Test forward\n",
        "batch_size, seq_len = 2, 64\n",
        "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "labels = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "\n",
        "output = model(input_ids, labels=labels)\n",
        "print(f\"Output shape: {output['logits'].shape}\")\n",
        "print(f\"Loss: {output['loss'].item():.4f}\")\n",
        "\n",
        "# Show param groups\n",
        "param_groups = model.get_optimizer_param_groups()\n",
        "print(\"\\nParameter groups with differential LRs:\")\n",
        "for pg in param_groups:\n",
        "    print(f\"  {pg['name']}: {sum(p.numel() for p in pg['params']):,} params, lr={pg['lr']}\")\n",
        "\n",
        "print(\"\\nSpectral Belief Machine V2 test complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuBRzyew8q2m",
        "outputId": "b304ca88-b1d1-4102-b882-0287f9521b9d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING SPECTRAL BELIEF MACHINE V2\n",
            "============================================================\n",
            "Model parameters: 81,485,440\n",
            "Embed dim: 512\n",
            "Dim per band: 64\n",
            "Bands: 7 spectral + 1 temporal = 8\n",
            "Output shape: torch.Size([2, 64, 50257])\n",
            "Loss: 10.8848\n",
            "\n",
            "Parameter groups with differential LRs:\n",
            "  band_0: 225,792 params, lr=3e-05\n",
            "  band_1: 225,792 params, lr=5e-05\n",
            "  band_2: 225,792 params, lr=0.0001\n",
            "  band_3: 250,368 params, lr=0.0003\n",
            "  band_4: 250,368 params, lr=0.0005\n",
            "  band_5: 299,904 params, lr=0.001\n",
            "  band_6: 299,904 params, lr=0.003\n",
            "  band_7: 100,608 params, lr=0.0003\n",
            "  shared: 79,606,912 params, lr=0.0003\n",
            "\n",
            "Spectral Belief Machine V2 test complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 13: DATA LOADING\n",
        "# ==============================================================================\n",
        "\n",
        "def load_wikitext103(tokenizer, max_length: int = 256, split: str = \"train\"):\n",
        "    from datasets import load_dataset\n",
        "    print(f\"Loading WikiText-103 {split}...\")\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=split)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "        return tokenized\n",
        "\n",
        "    dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
        "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    return tokenized\n",
        "\n",
        "print(\"Data loading function defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyAP-iSH8oGm",
        "outputId": "2d456286-2a18-470a-bdc5-3fe32516719a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 14: TRAINING\n",
        "# ==============================================================================\n",
        "\n",
        "def train_spectral(model, train_loader, eval_loader, total_steps=5000, eval_every=500, log_every=50):\n",
        "    model = model.to(device)\n",
        "\n",
        "    param_groups = model.get_optimizer_param_groups()\n",
        "    optimizer = AdamW(param_groups, weight_decay=0.01)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-7)\n",
        "    scaler = torch.amp.GradScaler('cuda') if device == \"cuda\" else None\n",
        "\n",
        "    model.train()\n",
        "    train_iter = iter(train_loader)\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for step in tqdm(range(total_steps), desc=\"SBM Training\"):\n",
        "        try:\n",
        "            batch = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(train_loader)\n",
        "            batch = next(train_iter)\n",
        "\n",
        "        ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out = model(ids, labels=labels)\n",
        "                loss = out[\"loss\"]\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            out = model(ids, labels=labels)\n",
        "            loss = out[\"loss\"]\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if step % 500 == 0 and step > 0:\n",
        "            model.eval()\n",
        "            eval_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for i, eb in enumerate(eval_loader):\n",
        "                    if i >= 50: break\n",
        "                    with torch.amp.autocast('cuda') if device == \"cuda\" else torch.no_grad():\n",
        "                        eo = model(eb[\"input_ids\"].to(device), labels=eb[\"labels\"].to(device))\n",
        "                    eval_loss += eo[\"loss\"].item()\n",
        "            eval_loss /= 50\n",
        "            print(f\"\\nStep {step}: eval_loss={eval_loss:.4f}, ppl={math.exp(eval_loss):.2f}\")\n",
        "            if eval_loss < best_loss:\n",
        "                best_loss = eval_loss\n",
        "                print(\"  New best!\")\n",
        "            model.train()\n",
        "\n",
        "    return best_loss\n",
        "\n",
        "print(\"Training function defined\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbEIxRKk8kF9",
        "outputId": "d0f5d9a4-97e4-487a-85e6-0ad67bcaa067"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 15: RUN TRAINING\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "train_dataset = load_wikitext103(tokenizer, max_length=256, split=\"train\")\n",
        "eval_dataset = load_wikitext103(tokenizer, max_length=256, split=\"validation\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "2530ace9958d4799b29e1b22173ae742",
            "328e5c5edd0741cfbf1da6ff1a892cd7",
            "f1e8fda5e2434fd48abb479ecd0f93b5",
            "bad47e841a4b47ce8a4d681f7b23afb1",
            "c7575e82f105450c83e9de5ddea2a9b8",
            "ac21a5f378484840b22518d0be8cca09",
            "a637255537e1414d98f65d9a70b680e1",
            "86329401fa704ba2aec44264170ceba1",
            "e0141ca9c3df4e37b7d03eb965adbb88",
            "e558eb9a685a438dbceb0561eda2ccc6",
            "89f9492fb3264dc6a99b51ee68dbedad",
            "4b524afb918d4cb19ec57a1ca2758c87",
            "4d462dd58ddd4ef8891c78a9fae2804b",
            "ae7c786cd47a4de7beb0ca89ad06d177",
            "03ad5e1e2ffd47a7bb3f738526c8dfc5",
            "8239acdc08784542a05a17bfed76f907",
            "7d12332b6cd645fea549cf38a8409f4d",
            "8607387920914f6f90c64fa18a818934",
            "c7991daf38754832a0f882a78ea2775c",
            "609c139658ef4355858787a254e79928",
            "65bc8d5486c744e4babe4dbd403523b3",
            "d762989a192f47918a47d38b872064b6"
          ]
        },
        "id": "B8GPXXYz8gDW",
        "outputId": "3024a472-945f-460b-dceb-9024012e3d01"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiText-103 train...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1165029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2530ace9958d4799b29e1b22173ae742"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiText-103 validation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b524afb918d4cb19ec57a1ca2758c87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1165029, Eval: 2461\n"
          ]
        }
      ]
    }
  ]
}