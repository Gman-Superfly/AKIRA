{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 038: Three-Level Distinction\n",
    "\n",
    "**Question:** Can we empirically distinguish Measurements (L1), Inferences (L2), and Action Quanta (L3)?\n",
    "\n",
    "**Hypothesis:** L3 is a strict subset of L2, more stable, and uniquely load-bearing.\n",
    "\n",
    "**Key Prediction:** Ablating L3 causes >50% degradation; L2-only <20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch scikit-learn matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "PROBE_LAYER = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Three Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1: Raw activations (Measurements)\n",
    "def extract_L1(text: str, layer: int = PROBE_LAYER) -> torch.Tensor:\n",
    "    \"\"\"Extract raw activations.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    activations = {}\n",
    "    \n",
    "    def hook(m, i, o):\n",
    "        activations['out'] = o[0].detach().cpu()\n",
    "    \n",
    "    h = model.transformer.h[layer].register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "    h.remove()\n",
    "    \n",
    "    return activations['out'].mean(dim=1).squeeze()  # Pool over sequence\n",
    "\n",
    "# Test L1\n",
    "test_L1 = extract_L1(\"This is a test sentence.\")\n",
    "print(f\"L1 shape: {test_L1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train L2 Probes (Inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for sentiment probe\n",
    "POSITIVE_TEXTS = [\n",
    "    \"This movie was wonderful and amazing!\",\n",
    "    \"I loved every minute, fantastic experience.\",\n",
    "    \"Excellent service, delicious food, great atmosphere.\",\n",
    "    \"Beautiful day, everything is perfect!\",\n",
    "    \"Best thing ever, so happy and grateful.\",\n",
    "    \"Brilliant work, truly outstanding performance.\",\n",
    "    \"Exceptional quality, exceeded expectations.\",\n",
    "    \"Absolutely delightful, highly recommend!\",\n",
    "]\n",
    "\n",
    "NEGATIVE_TEXTS = [\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"I hated every minute, awful experience.\",\n",
    "    \"Horrible service, disgusting food, bad atmosphere.\",\n",
    "    \"Terrible day, everything went wrong!\",\n",
    "    \"Worst thing ever, so sad and frustrated.\",\n",
    "    \"Awful work, completely disappointing performance.\",\n",
    "    \"Poor quality, failed to meet expectations.\",\n",
    "    \"Absolutely dreadful, do not recommend!\",\n",
    "]\n",
    "\n",
    "# Extract features\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for text in POSITIVE_TEXTS:\n",
    "    X_train.append(extract_L1(text).numpy())\n",
    "    y_train.append(1)  # Positive\n",
    "\n",
    "for text in NEGATIVE_TEXTS:\n",
    "    X_train.append(extract_L1(text).numpy())\n",
    "    y_train.append(0)  # Negative\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sentiment probe (L2)\n",
    "sentiment_probe = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(sentiment_probe, X_train, y_train, cv=4)\n",
    "print(f\"Probe CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "\n",
    "# Fit on all data\n",
    "sentiment_probe.fit(X_train, y_train)\n",
    "\n",
    "# L2 extraction function\n",
    "def extract_L2(text: str) -> Dict:\n",
    "    \"\"\"Extract L2 inference (sentiment prediction).\"\"\"\n",
    "    L1 = extract_L1(text).numpy().reshape(1, -1)\n",
    "    pred = sentiment_probe.predict(L1)[0]\n",
    "    prob = sentiment_probe.predict_proba(L1)[0]\n",
    "    return {\n",
    "        'prediction': 'positive' if pred == 1 else 'negative',\n",
    "        'confidence': prob.max(),\n",
    "        'prob_positive': prob[1],\n",
    "        'prob_negative': prob[0]\n",
    "    }\n",
    "\n",
    "# Test L2\n",
    "print(\"\\nL2 Test:\")\n",
    "print(f\"  'Great movie!': {extract_L2('Great movie!')}\")\n",
    "print(f\"  'Terrible film!': {extract_L2('Terrible film!')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify L3 Features (Load-Bearing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(text: str, target_token: str) -> torch.Tensor:\n",
    "    \"\"\"Compute feature importance via gradient.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    target_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    target_id = target_ids[0] if target_ids else tokenizer.eos_token_id\n",
    "    \n",
    "    # Get embeddings with gradients\n",
    "    embeddings = model.get_input_embeddings()(inputs['input_ids'])\n",
    "    embeddings = embeddings.clone().requires_grad_(True)\n",
    "    \n",
    "    outputs = model(inputs_embeds=embeddings)\n",
    "    target_logit = outputs.logits[0, -1, target_id]\n",
    "    target_logit.backward()\n",
    "    \n",
    "    # Importance = gradient magnitude per dimension\n",
    "    importance = embeddings.grad.abs().mean(dim=(0, 1)).cpu()\n",
    "    return importance\n",
    "\n",
    "def identify_L3(text: str, target_token: str, top_k: int = 50) -> Set[int]:\n",
    "    \"\"\"Identify L3 features (most important for target prediction).\"\"\"\n",
    "    importance = compute_feature_importance(text, target_token)\n",
    "    top_indices = importance.topk(top_k).indices.tolist()\n",
    "    return set(top_indices), importance\n",
    "\n",
    "# Test L3 identification\n",
    "TEST_INPUT = \"This movie was absolutely wonderful, I loved every minute.\"\n",
    "TARGET = \"positive\"\n",
    "\n",
    "L3_indices, importance = identify_L3(TEST_INPUT, TARGET)\n",
    "print(f\"L3 features (top 50): {len(L3_indices)} dimensions\")\n",
    "print(f\"Top 10 indices: {sorted(list(L3_indices))[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Ablation Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_prob(text: str, target_token: str) -> float:\n",
    "    \"\"\"Get probability of target token.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    target_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    target_id = target_ids[0] if target_ids else tokenizer.eos_token_id\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits[0, -1, :], dim=-1)\n",
    "    return probs[target_id].item()\n",
    "\n",
    "def ablate_features(text: str, target_token: str, feature_indices: Set[int]) -> float:\n",
    "    \"\"\"Get output prob after ablating features.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    target_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    target_id = target_ids[0] if target_ids else tokenizer.eos_token_id\n",
    "    \n",
    "    embeddings = model.get_input_embeddings()(inputs['input_ids'])\n",
    "    \n",
    "    # Ablate: zero out features\n",
    "    for idx in feature_indices:\n",
    "        embeddings[:, :, idx] = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs_embeds=embeddings)\n",
    "        probs = torch.softmax(outputs.logits[0, -1, :], dim=-1)\n",
    "    return probs[target_id].item()\n",
    "\n",
    "# Baseline\n",
    "baseline_prob = get_output_prob(TEST_INPUT, TARGET)\n",
    "print(f\"Baseline probability for '{TARGET}': {baseline_prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare L3 ablation vs L2-only ablation\n",
    "\n",
    "# L3: Top 50 most important features\n",
    "L3_indices, importance = identify_L3(TEST_INPUT, TARGET, top_k=50)\n",
    "\n",
    "# L2-only: Random features NOT in L3 (representing probed but non-load-bearing)\n",
    "all_dims = set(range(model.config.n_embd))\n",
    "L2_only_indices = all_dims - L3_indices\n",
    "L2_only_sample = set(list(L2_only_indices)[:50])  # Sample same size\n",
    "\n",
    "# Ablate L3\n",
    "prob_after_L3_ablation = ablate_features(TEST_INPUT, TARGET, L3_indices)\n",
    "L3_degradation = (baseline_prob - prob_after_L3_ablation) / baseline_prob * 100\n",
    "\n",
    "# Ablate L2-only\n",
    "prob_after_L2_ablation = ablate_features(TEST_INPUT, TARGET, L2_only_sample)\n",
    "L2_only_degradation = (baseline_prob - prob_after_L2_ablation) / baseline_prob * 100\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ABLATION RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nBaseline prob: {baseline_prob:.6f}\")\n",
    "print(f\"\\nL3 Ablation (top 50 important features):\")\n",
    "print(f\"  Prob after: {prob_after_L3_ablation:.6f}\")\n",
    "print(f\"  Degradation: {L3_degradation:.1f}%\")\n",
    "print(f\"\\nL2-only Ablation (50 non-important features):\")\n",
    "print(f\"  Prob after: {prob_after_L2_ablation:.6f}\")\n",
    "print(f\"  Degradation: {L2_only_degradation:.1f}%\")\n",
    "print(f\"\\nRatio: {L3_degradation / max(L2_only_degradation, 0.1):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stability Across Paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAPHRASES = [\n",
    "    \"This movie was absolutely wonderful, I loved every minute.\",\n",
    "    \"This film was totally amazing, I enjoyed every second.\",\n",
    "    \"What a fantastic movie, loved it from start to finish.\",\n",
    "    \"An absolutely brilliant film, thoroughly enjoyable.\",\n",
    "]\n",
    "\n",
    "# Get L2 predictions for all paraphrases\n",
    "L2_predictions = [extract_L2(p) for p in PARAPHRASES]\n",
    "\n",
    "print(\"L2 Stability Across Paraphrases:\")\n",
    "print(\"-\" * 50)\n",
    "for text, pred in zip(PARAPHRASES, L2_predictions):\n",
    "    print(f\"'{text[:40]}...'\")\n",
    "    print(f\"  -> {pred['prediction']} ({pred['confidence']:.3f})\")\n",
    "\n",
    "# Compute variance\n",
    "confidences = [p['confidence'] for p in L2_predictions]\n",
    "variance = np.var(confidences)\n",
    "stability = 1.0 / (1.0 + variance * 10)\n",
    "\n",
    "print(f\"\\nL2 Confidence variance: {variance:.4f}\")\n",
    "print(f\"L2 Stability score: {stability:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare L3 feature stability\n",
    "L3_sets = []\n",
    "for text in PARAPHRASES:\n",
    "    L3, _ = identify_L3(text, TARGET, top_k=50)\n",
    "    L3_sets.append(L3)\n",
    "\n",
    "# Compute Jaccard similarities\n",
    "print(\"L3 Feature Stability (Jaccard Similarity):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "jaccards = []\n",
    "for i in range(len(L3_sets)):\n",
    "    for j in range(i+1, len(L3_sets)):\n",
    "        intersection = len(L3_sets[i] & L3_sets[j])\n",
    "        union = len(L3_sets[i] | L3_sets[j])\n",
    "        jaccard = intersection / union if union > 0 else 0\n",
    "        jaccards.append(jaccard)\n",
    "        print(f\"  Paraphrase {i+1} vs {j+1}: {jaccard:.3f}\")\n",
    "\n",
    "print(f\"\\nMean L3 Jaccard similarity: {np.mean(jaccards):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Three Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# L1: Raw activations histogram\n",
    "ax1 = axes[0]\n",
    "L1_example = extract_L1(TEST_INPUT).numpy()\n",
    "ax1.hist(L1_example, bins=50, alpha=0.7, color='blue')\n",
    "ax1.set_title('L1: Measurements\\n(Raw Activations)')\n",
    "ax1.set_xlabel('Activation Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# L2: Probe prediction distribution\n",
    "ax2 = axes[1]\n",
    "L2_preds = [extract_L2(text)['prob_positive'] for text in POSITIVE_TEXTS + NEGATIVE_TEXTS]\n",
    "labels = ['Positive'] * len(POSITIVE_TEXTS) + ['Negative'] * len(NEGATIVE_TEXTS)\n",
    "colors = ['green' if l == 'Positive' else 'red' for l in labels]\n",
    "ax2.scatter(range(len(L2_preds)), L2_preds, c=colors, alpha=0.7, s=100)\n",
    "ax2.axhline(y=0.5, color='black', linestyle='--', label='Decision boundary')\n",
    "ax2.set_title('L2: Inferences\\n(Probe Predictions)')\n",
    "ax2.set_xlabel('Sample Index')\n",
    "ax2.set_ylabel('P(Positive)')\n",
    "ax2.legend()\n",
    "\n",
    "# L3: Feature importance\n",
    "ax3 = axes[2]\n",
    "_, importance = identify_L3(TEST_INPUT, TARGET)\n",
    "sorted_imp = importance.sort(descending=True).values.numpy()[:100]\n",
    "ax3.bar(range(len(sorted_imp)), sorted_imp, alpha=0.7, color='purple')\n",
    "ax3.axvline(x=50, color='red', linestyle='--', label='L3 threshold (top 50)')\n",
    "ax3.set_title('L3: Action Quanta\\n(Feature Importance)')\n",
    "ax3.set_xlabel('Feature Rank')\n",
    "ax3.set_ylabel('Importance')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 038 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. THREE LEVELS IDENTIFIED:\")\n",
    "print(f\"   L1 (Measurements): {L1_example.shape[0]} dimensional raw activations\")\n",
    "print(f\"   L2 (Inferences): Semantic probes with {cv_scores.mean():.1%} accuracy\")\n",
    "print(f\"   L3 (Action Quanta): Top 50 load-bearing features\")\n",
    "\n",
    "print(\"\\n2. SUBSET RELATIONSHIP:\")\n",
    "print(f\"   L3 size: 50 features\")\n",
    "print(f\"   L1 size: {L1_example.shape[0]} features\")\n",
    "print(f\"   Ratio: {50/L1_example.shape[0]:.1%} (L3 << L1)\")\n",
    "\n",
    "print(\"\\n3. ABLATION ASYMMETRY:\")\n",
    "print(f\"   L3 ablation degradation: {L3_degradation:.1f}%\")\n",
    "print(f\"   L2-only ablation degradation: {L2_only_degradation:.1f}%\")\n",
    "ratio = L3_degradation / max(L2_only_degradation, 0.1)\n",
    "print(f\"   Ratio: {ratio:.1f}x\")\n",
    "\n",
    "print(\"\\n4. STABILITY:\")\n",
    "print(f\"   L2 confidence variance: {variance:.4f}\")\n",
    "print(f\"   L3 mean Jaccard: {np.mean(jaccards):.3f}\")\n",
    "\n",
    "print(\"\\n5. VERDICT:\")\n",
    "if L3_degradation > L2_only_degradation * 2:\n",
    "    print(\"   HYPOTHESIS SUPPORTED\")\n",
    "    print(\"   - L3 (AQ) is a distinct, load-bearing subset\")\n",
    "    print(\"   - Ablating L3 is significantly more damaging\")\n",
    "    print(\"   - Three levels ARE empirically distinguishable\")\n",
    "else:\n",
    "    print(\"   HYPOTHESIS NOT SUPPORTED\")\n",
    "    print(\"   - No clear distinction between levels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
