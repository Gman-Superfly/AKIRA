{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 041: Circuit Complexity Boundary\n",
    "\n",
    "**Question:** Do fixed-width networks have hard performance boundaries determined by SOS width?\n",
    "\n",
    "**Hypothesis:** For B=8, beta=2: max solvable k = (8/2) - 1 = 3. Tasks with k>3 should fail.\n",
    "\n",
    "**Key Prediction:** Sharp accuracy drop at k=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Circuit Complexity Theory\n",
    "\n",
    "```\n",
    "REQUIRED BREADTH = (k + 1) x beta\n",
    "\n",
    "For AKIRA-like architecture:\n",
    "  B = 8 (breadth)\n",
    "  beta = 2 (predicate arity)\n",
    "  \n",
    "Max solvable k = (B/beta) - 1 = (8/2) - 1 = 3\n",
    "\n",
    "Prediction:\n",
    "  k <= 3: Can solve\n",
    "  k > 3: Cannot solve (mathematically impossible)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Tasks with Controlled SOS Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k1_tasks(n=20):\n",
    "    \"\"\"k=1: Track single object property.\"\"\"\n",
    "    colors = ['red', 'blue', 'green', 'yellow']\n",
    "    objects = ['ball', 'box', 'cup']\n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        c = colors[i % len(colors)]\n",
    "        o = objects[i % len(objects)]\n",
    "        tasks.append({\n",
    "            'prompt': f\"There is a {c} {o}. What color is the {o}?\",\n",
    "            'answer': c\n",
    "        })\n",
    "    return tasks\n",
    "\n",
    "def generate_k2_tasks(n=20):\n",
    "    \"\"\"k=2: Track object + one relationship.\"\"\"\n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        tasks.append({\n",
    "            'prompt': f\"A is on the table. B is on A. Where is B?\",\n",
    "            'answer': 'A'\n",
    "        })\n",
    "    return tasks\n",
    "\n",
    "def generate_k3_tasks(n=20):\n",
    "    \"\"\"k=3: Track object + two relationships.\"\"\"\n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        tasks.append({\n",
    "            'prompt': f\"A is on the table. B is on A. C is on B. What is on top?\",\n",
    "            'answer': 'C'\n",
    "        })\n",
    "    return tasks\n",
    "\n",
    "def generate_k4_tasks(n=20):\n",
    "    \"\"\"k=4: Track object + three relationships (beyond boundary).\"\"\"\n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        tasks.append({\n",
    "            'prompt': f\"A is on the table. B is on A. C is on B. D is on C. \"\n",
    "                     f\"If B is removed, what falls?\",\n",
    "            'answer': 'C and D'  # or just 'C'\n",
    "        })\n",
    "    return tasks\n",
    "\n",
    "def generate_k5_tasks(n=20):\n",
    "    \"\"\"k=5: Well beyond boundary.\"\"\"\n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        tasks.append({\n",
    "            'prompt': f\"A is on table. B is on A. C is on B. D is on C. E is on D. \"\n",
    "                     f\"List the order from bottom to top.\",\n",
    "            'answer': 'A, B, C, D, E'\n",
    "        })\n",
    "    return tasks\n",
    "\n",
    "# Generate all tasks\n",
    "N_PER_K = 20\n",
    "all_tasks = {\n",
    "    1: generate_k1_tasks(N_PER_K),\n",
    "    2: generate_k2_tasks(N_PER_K),\n",
    "    3: generate_k3_tasks(N_PER_K),\n",
    "    4: generate_k4_tasks(N_PER_K),\n",
    "    5: generate_k5_tasks(N_PER_K)\n",
    "}\n",
    "\n",
    "print(\"Generated tasks:\")\n",
    "for k, tasks in all_tasks.items():\n",
    "    print(f\"  k={k}: {len(tasks)} tasks\")\n",
    "    print(f\"    Example: {tasks[0]['prompt'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model on Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(prompt, expected):\n",
    "    \"\"\"Evaluate model on a single task.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated = response[len(prompt):].strip().lower()\n",
    "    expected_lower = expected.lower()\n",
    "    \n",
    "    # Check if answer is in response\n",
    "    correct = expected_lower in generated or generated.startswith(expected_lower)\n",
    "    \n",
    "    return {\n",
    "        'generated': generated[:50],\n",
    "        'expected': expected,\n",
    "        'correct': correct\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_result = evaluate_task(\"The sky is blue. What color is the sky?\", \"blue\")\n",
    "print(f\"Test: {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all k levels\n",
    "results = {}\n",
    "\n",
    "for k, tasks in all_tasks.items():\n",
    "    print(f\"\\nEvaluating k={k}...\")\n",
    "    correct = 0\n",
    "    task_results = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        result = evaluate_task(task['prompt'], task['answer'])\n",
    "        task_results.append(result)\n",
    "        if result['correct']:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(tasks)\n",
    "    results[k] = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': len(tasks),\n",
    "        'details': task_results\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.1%} ({correct}/{len(tasks)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract accuracies\n",
    "k_values = sorted(results.keys())\n",
    "accuracies = [results[k]['accuracy'] for k in k_values]\n",
    "\n",
    "# Compute drops\n",
    "drops = []\n",
    "for i in range(len(accuracies) - 1):\n",
    "    drop = accuracies[i] - accuracies[i+1]\n",
    "    drops.append({\n",
    "        'from_k': k_values[i],\n",
    "        'to_k': k_values[i+1],\n",
    "        'drop': drop\n",
    "    })\n",
    "\n",
    "print(\"ACCURACY DROPS:\")\n",
    "for d in drops:\n",
    "    significance = \"SIGNIFICANT\" if d['drop'] > 0.3 else \"minor\"\n",
    "    print(f\"  k={d['from_k']} -> k={d['to_k']}: {d['drop']:.1%} [{significance}]\")\n",
    "\n",
    "# Find boundary\n",
    "max_drop = max(drops, key=lambda x: x['drop'])\n",
    "observed_boundary = max_drop['to_k'] if max_drop['drop'] > 0.2 else None\n",
    "predicted_boundary = 4  # For B=8, beta=2: max k = 3, so boundary at k=4\n",
    "\n",
    "print(f\"\\nObserved boundary: k = {observed_boundary}\")\n",
    "print(f\"Predicted boundary: k = {predicted_boundary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Accuracy by k\n",
    "ax1 = axes[0]\n",
    "colors = ['green' if a > 0.7 else 'orange' if a > 0.4 else 'red' for a in accuracies]\n",
    "bars = ax1.bar(k_values, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add boundary line\n",
    "ax1.axvline(x=predicted_boundary - 0.5, color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Predicted boundary (k={predicted_boundary})')\n",
    "ax1.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('SOS Width (k)', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy by SOS Width', fontsize=14)\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.set_xticks(k_values)\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{acc:.0%}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 2: Drop magnitude\n",
    "ax2 = axes[1]\n",
    "labels = [f\"k={d['from_k']}->k={d['to_k']}\" for d in drops]\n",
    "drop_values = [d['drop'] for d in drops]\n",
    "colors = ['red' if d > 0.3 else 'orange' if d > 0.1 else 'green' for d in drop_values]\n",
    "\n",
    "ax2.bar(range(len(drops)), drop_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=0.3, color='red', linestyle='--', label='Significant threshold')\n",
    "ax2.set_xticks(range(len(drops)))\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.set_xlabel('Transition', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy Drop', fontsize=12)\n",
    "ax2.set_title('Accuracy Drop at Each Transition', fontsize=14)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 041 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. THEORETICAL PREDICTION:\")\n",
    "print(\"   Breadth B = 8\")\n",
    "print(\"   Predicate arity beta = 2\")\n",
    "print(\"   Max solvable k = (B/beta) - 1 = 3\")\n",
    "print(\"   Boundary at k = 4\")\n",
    "\n",
    "print(\"\\n2. OBSERVED RESULTS:\")\n",
    "for k in k_values:\n",
    "    acc = results[k]['accuracy']\n",
    "    status = \"PASS\" if acc > 0.6 else \"FAIL\"\n",
    "    print(f\"   k = {k}: {acc:.0%} [{status}]\")\n",
    "\n",
    "print(f\"\\n3. BOUNDARY ANALYSIS:\")\n",
    "print(f\"   Observed boundary: k = {observed_boundary}\")\n",
    "print(f\"   Predicted boundary: k = {predicted_boundary}\")\n",
    "print(f\"   Max drop: {max_drop['drop']:.1%} at k={max_drop['from_k']}->{max_drop['to_k']}\")\n",
    "\n",
    "print(\"\\n4. VERDICT:\")\n",
    "\n",
    "# Check criteria\n",
    "acc_k3 = results.get(3, {}).get('accuracy', 0)\n",
    "acc_k4 = results.get(4, {}).get('accuracy', 1)\n",
    "sharp_drop = acc_k3 - acc_k4 > 0.25\n",
    "boundary_match = observed_boundary == predicted_boundary\n",
    "\n",
    "if sharp_drop:\n",
    "    print(\"   HYPOTHESIS SUPPORTED\")\n",
    "    print(f\"   - Sharp drop at k=4: {acc_k3:.0%} -> {acc_k4:.0%}\")\n",
    "    print(\"   - Fixed-width networks have complexity limits\")\n",
    "    print(\"   - Circuit complexity theory validated\")\n",
    "else:\n",
    "    print(\"   HYPOTHESIS NOT SUPPORTED\")\n",
    "    print(f\"   - No sharp drop: k=3 ({acc_k3:.0%}) vs k=4 ({acc_k4:.0%})\")\n",
    "    print(\"   - Model may use different strategies\")\n",
    "    print(\"   - Or tasks don't properly isolate complexity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
