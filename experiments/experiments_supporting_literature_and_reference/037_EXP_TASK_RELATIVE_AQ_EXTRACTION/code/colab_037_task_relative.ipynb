{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 037: Task-Relative AQ Extraction\n",
    "\n",
    "**Question:** Does the same input produce different Action Quanta under different task framings?\n",
    "\n",
    "**Hypothesis:** AQ are emergent from signal-task interaction, not intrinsic properties of signals.\n",
    "\n",
    "**Key Prediction:** Activations should diverge across tasks (similarity < 0.9 in late layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install transformers torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Dict, List\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"  # Change to \"gpt2-medium\" for better results\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded {MODEL_NAME} with {model.config.n_layer} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Task Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core test: Same input, different tasks\n",
    "INPUT_TEXT = \"A red ball on a blue table, next to a green cup.\"\n",
    "\n",
    "TASKS = {\n",
    "    'color': \"What color is the ball?\",\n",
    "    'location': \"Where is the ball?\",\n",
    "    'existence': \"Is there a ball?\",\n",
    "    'relation': \"What is the ball next to?\",\n",
    "    'count': \"How many objects are mentioned?\"\n",
    "}\n",
    "\n",
    "print(f\"Input: {INPUT_TEXT}\")\n",
    "print(f\"\\nTasks:\")\n",
    "for name, prompt in TASKS.items():\n",
    "    print(f\"  {name}: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Activations Under Each Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(input_text: str, task_prompt: str, layers: List[int]):\n",
    "    \"\"\"Extract activations for input under task framing.\"\"\"\n",
    "    full_prompt = f\"{task_prompt}\\n\\nContext: {input_text}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def make_hook(layer_idx):\n",
    "        def hook(module, inp, out):\n",
    "            activations[layer_idx] = out[0].detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    for idx in layers:\n",
    "        h = model.transformer.h[idx].register_forward_hook(make_hook(idx))\n",
    "        hooks.append(h)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    return {\n",
    "        'activations': activations,\n",
    "        'attentions': [a.cpu() for a in outputs.attentions],\n",
    "        'logits': outputs.logits.cpu()\n",
    "    }\n",
    "\n",
    "# Extract for all tasks\n",
    "LAYERS = [0, 2, 4, 6, 8, 10, 11]\n",
    "task_extractions = {}\n",
    "\n",
    "for task_name, task_prompt in TASKS.items():\n",
    "    print(f\"Extracting for task: {task_name}\")\n",
    "    task_extractions[task_name] = extract_activations(INPUT_TEXT, task_prompt, LAYERS)\n",
    "\n",
    "print(\"\\nExtraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Activations Across Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine similarity between two tensors.\"\"\"\n",
    "    a_flat = a.flatten().float()\n",
    "    b_flat = b.flatten().float()\n",
    "    min_len = min(len(a_flat), len(b_flat))\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        a_flat[:min_len].unsqueeze(0),\n",
    "        b_flat[:min_len].unsqueeze(0)\n",
    "    ).item()\n",
    "\n",
    "# Compute pairwise similarities\n",
    "task_names = list(TASKS.keys())\n",
    "n_tasks = len(task_names)\n",
    "\n",
    "# Store similarities per layer\n",
    "layer_similarities = {layer: np.zeros((n_tasks, n_tasks)) for layer in LAYERS}\n",
    "\n",
    "for layer in LAYERS:\n",
    "    for i, task_a in enumerate(task_names):\n",
    "        for j, task_b in enumerate(task_names):\n",
    "            act_a = task_extractions[task_a]['activations'][layer]\n",
    "            act_b = task_extractions[task_b]['activations'][layer]\n",
    "            sim = cosine_similarity(act_a, act_b)\n",
    "            layer_similarities[layer][i, j] = sim\n",
    "\n",
    "print(\"Similarity matrices computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity matrices by layer\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, layer in enumerate(LAYERS):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(layer_similarities[layer], cmap='RdYlGn', vmin=0.5, vmax=1.0)\n",
    "    ax.set_xticks(range(n_tasks))\n",
    "    ax.set_yticks(range(n_tasks))\n",
    "    ax.set_xticklabels(task_names, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticklabels(task_names, fontsize=8)\n",
    "    ax.set_title(f'Layer {layer}')\n",
    "\n",
    "# Hide last subplot if odd number\n",
    "if len(LAYERS) < len(axes):\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.colorbar(im, ax=axes, shrink=0.6, label='Cosine Similarity')\n",
    "plt.suptitle('Activation Similarity Across Tasks by Layer', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Divergence by Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean off-diagonal similarity per layer\n",
    "mean_divergences = []\n",
    "\n",
    "for layer in LAYERS:\n",
    "    sim_matrix = layer_similarities[layer]\n",
    "    # Get off-diagonal elements (exclude self-similarity)\n",
    "    off_diag = sim_matrix[np.triu_indices(n_tasks, k=1)]\n",
    "    mean_sim = off_diag.mean()\n",
    "    mean_divergences.append((layer, mean_sim))\n",
    "    print(f\"Layer {layer:2d}: mean cross-task similarity = {mean_sim:.4f}\")\n",
    "\n",
    "# Plot\n",
    "layers_plot = [x[0] for x in mean_divergences]\n",
    "sims_plot = [x[1] for x in mean_divergences]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layers_plot, sims_plot, 'bo-', markersize=10, linewidth=2)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='Threshold (0.9)')\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Mean Cross-Task Similarity', fontsize=12)\n",
    "plt.title('Activation Divergence Across Tasks by Layer', fontsize=14)\n",
    "plt.ylim(0.5, 1.05)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Check hypothesis\n",
    "late_layer_sim = sims_plot[-1]\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"HYPOTHESIS TEST\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Late layer similarity: {late_layer_sim:.4f}\")\n",
    "if late_layer_sim < 0.9:\n",
    "    print(f\"RESULT: SUPPORTED - Tasks produce divergent representations\")\n",
    "else:\n",
    "    print(f\"RESULT: NOT SUPPORTED - Representations remain similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_focus(attentions, top_k=5):\n",
    "    \"\"\"Get top-attended positions from last layer, last token.\"\"\"\n",
    "    # Last layer attention, averaged over heads, last query token\n",
    "    last_attn = attentions[-1][0].mean(dim=0)[-1]  # Shape: [seq_len]\n",
    "    top_positions = last_attn.topk(top_k).indices.tolist()\n",
    "    return set(top_positions)\n",
    "\n",
    "# Compare attention focus across tasks\n",
    "attention_focuses = {}\n",
    "for task_name in task_names:\n",
    "    focus = get_attention_focus(task_extractions[task_name]['attentions'])\n",
    "    attention_focuses[task_name] = focus\n",
    "    print(f\"{task_name}: top-5 attended positions = {sorted(focus)}\")\n",
    "\n",
    "# Compute pairwise attention overlap\n",
    "print(\"\\nAttention Focus Overlap (Jaccard):\")\n",
    "for i, task_a in enumerate(task_names):\n",
    "    for task_b in task_names[i+1:]:\n",
    "        focus_a = attention_focuses[task_a]\n",
    "        focus_b = attention_focuses[task_b]\n",
    "        jaccard = len(focus_a & focus_b) / len(focus_a | focus_b)\n",
    "        print(f\"  {task_a} vs {task_b}: {jaccard:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted next tokens across tasks\n",
    "print(\"Predicted Next Tokens by Task:\\n\")\n",
    "\n",
    "for task_name in task_names:\n",
    "    logits = task_extractions[task_name]['logits'][0, -1, :]\n",
    "    probs = torch.softmax(logits, dim=0)\n",
    "    \n",
    "    # Top 5 predictions\n",
    "    top_probs, top_ids = probs.topk(5)\n",
    "    top_tokens = [tokenizer.decode([id]) for id in top_ids]\n",
    "    \n",
    "    print(f\"{task_name}:\")\n",
    "    for token, prob in zip(top_tokens, top_probs):\n",
    "        print(f\"  '{token.strip()}': {prob:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test with Ambiguous Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the \"Fire!\" example\n",
    "AMBIGUOUS_INPUT = \"Fire!\"\n",
    "\n",
    "CONTEXT_TASKS = {\n",
    "    'emergency': \"You are in a crowded building. Someone shouts:\",\n",
    "    'military': \"You are a soldier with weapon ready. The commander says:\",\n",
    "    'campfire': \"You are camping and someone points at wood saying:\",\n",
    "    'pottery': \"You are in a pottery studio. The instructor says:\"\n",
    "}\n",
    "\n",
    "print(f\"Ambiguous input: '{AMBIGUOUS_INPUT}'\\n\")\n",
    "\n",
    "context_extractions = {}\n",
    "for ctx_name, ctx_prompt in CONTEXT_TASKS.items():\n",
    "    context_extractions[ctx_name] = extract_activations(\n",
    "        AMBIGUOUS_INPUT, ctx_prompt, LAYERS\n",
    "    )\n",
    "\n",
    "# Compare similarities\n",
    "print(\"Cross-context similarities (Layer 11):\")\n",
    "ctx_names = list(CONTEXT_TASKS.keys())\n",
    "for i, ctx_a in enumerate(ctx_names):\n",
    "    for ctx_b in ctx_names[i+1:]:\n",
    "        act_a = context_extractions[ctx_a]['activations'][11]\n",
    "        act_b = context_extractions[ctx_b]['activations'][11]\n",
    "        sim = cosine_similarity(act_a, act_b)\n",
    "        print(f\"  {ctx_a} vs {ctx_b}: {sim:.4f}\")\n",
    "\n",
    "# Show predicted tokens per context\n",
    "print(\"\\nPredicted next tokens by context:\")\n",
    "for ctx_name in ctx_names:\n",
    "    logits = context_extractions[ctx_name]['logits'][0, -1, :]\n",
    "    probs = torch.softmax(logits, dim=0)\n",
    "    top_id = probs.argmax()\n",
    "    top_token = tokenizer.decode([top_id])\n",
    "    print(f\"  {ctx_name}: '{top_token.strip()}' ({probs[top_id]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 037 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. ACTIVATION DIVERGENCE:\")\n",
    "print(f\"   Early layers (0-4): High similarity (structure shared)\")\n",
    "print(f\"   Late layers (8-11): Lower similarity (task-specific)\")\n",
    "print(f\"   Final layer mean similarity: {sims_plot[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n2. ATTENTION PATTERNS:\")\n",
    "print(\"   Different tasks attend to different positions\")\n",
    "\n",
    "print(\"\\n3. OUTPUT DISTRIBUTIONS:\")\n",
    "print(\"   Different tasks produce different predictions\")\n",
    "\n",
    "print(\"\\n4. VERDICT:\")\n",
    "if sims_plot[-1] < 0.9:\n",
    "    print(\"   HYPOTHESIS SUPPORTED\")\n",
    "    print(\"   Action Quanta ARE task-relative\")\n",
    "    print(\"   Same input -> Different representations under different tasks\")\n",
    "else:\n",
    "    print(\"   HYPOTHESIS NOT SUPPORTED\")\n",
    "    print(\"   Representations remain similar across tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
